{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15198afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, glob, random, argparse\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# XAutoDL \n",
    "from xautodl.config_utils import load_config, dict2config, configure2str\n",
    "from xautodl.datasets import get_datasets, get_nas_search_loaders\n",
    "from xautodl.procedures import (\n",
    "    prepare_seed,\n",
    "    prepare_logger,\n",
    "    save_checkpoint,\n",
    "    copy_checkpoint,\n",
    "    get_optim_scheduler,\n",
    ")\n",
    "from xautodl.utils import get_model_infos, obtain_accuracy\n",
    "from xautodl.log_utils import AverageMeter, time_string, convert_secs2time\n",
    "from xautodl.models import get_cell_based_tiny_net, get_search_spaces\n",
    "\n",
    "# NB201\n",
    "from nas_201_api import NASBench201API as API\n",
    "\n",
    "from ntk import get_ntk_n\n",
    "from linear_region_counter import Linear_Region_Collector\n",
    "\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "392dc080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44131\n",
      "Namespace(arch_nas_dataset='../../NAS-Bench-201-v1_1-096897.pth', channel=3, config_path='../../configs/nas-benchmark/algos/RANDOM.config', data_path='../../cifar.python', dataset='cifar10', max_nodes=4, num_cells=5, print_freq=200, rand_seed=44131, save_dir='./results/tmp', search_space_name='nas-bench-201', select_num=100, track_running_stats=0, workers=0)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(\"Random search for NAS.\")\n",
    "parser.add_argument(\"--data_path\", type=str, default='../../cifar.python', help=\"The path to dataset\")\n",
    "parser.add_argument(\"--dataset\", type=str, default='cifar10',choices=[\"cifar10\", \"cifar100\", \"ImageNet16-120\"], help=\"Choose between Cifar10/100 and ImageNet-16.\")\n",
    "\n",
    "# channels and number-of-cells\n",
    "parser.add_argument(\"--search_space_name\", type=str, default='nas-bench-201', help=\"The search space name.\")\n",
    "parser.add_argument(\"--config_path\", type=str, default='../../configs/nas-benchmark/algos/RANDOM.config', help=\"The path to the configuration.\")\n",
    "parser.add_argument(\"--max_nodes\", type=int, default=4, help=\"The maximum number of nodes.\")\n",
    "# parser.add_argument(\"--channel\", type=int, default=16, help=\"The number of channels.\")\n",
    "parser.add_argument(\"--num_cells\", type=int, default=5, help=\"The number of cells in one stage.\")\n",
    "parser.add_argument(\"--channel\", type=int, default=3, help=\"The number of channels.\")\n",
    "# parser.add_argument(\"--num_cells\", type=int, default=1, help=\"The number of cells in one stage.\")\n",
    "parser.add_argument(\"--select_num\", type=int, default=100, help=\"The number of selected architectures to evaluate.\")\n",
    "parser.add_argument(\"--track_running_stats\", type=int, default=0, choices=[0, 1], help=\"Whether use track_running_stats or not in the BN layer.\")\n",
    "# log\n",
    "parser.add_argument(\"--workers\", type=int, default=0, help=\"number of data loading workers\")\n",
    "parser.add_argument(\"--save_dir\", type=str, default='./results/tmp', help=\"Folder to save checkpoints and log.\")\n",
    "parser.add_argument(\"--arch_nas_dataset\", type=str, default='../../NAS-Bench-201-v1_1-096897.pth', help=\"The path to load the architecture dataset (tiny-nas-benchmark).\")\n",
    "parser.add_argument(\"--print_freq\", type=int, default=200, help=\"print frequency (default: 200)\")\n",
    "parser.add_argument(\"--rand_seed\", type=int, default=None, help=\"manual seed\")\n",
    "args = parser.parse_args(args=[])\n",
    "if args.rand_seed is None or args.rand_seed < 0:\n",
    "    args.rand_seed = random.randint(1, 100000)\n",
    "\n",
    "    \n",
    "print(args.rand_seed)\n",
    "print(args)\n",
    "xargs=args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1f83abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Function with logger : Logger(dir=results/tmp, use-tf=False, writer=None)\n",
      "Arguments : -------------------------------\n",
      "arch_nas_dataset : ../../NAS-Bench-201-v1_1-096897.pth\n",
      "channel          : 3\n",
      "config_path      : ../../configs/nas-benchmark/algos/RANDOM.config\n",
      "data_path        : ../../cifar.python\n",
      "dataset          : cifar10\n",
      "max_nodes        : 4\n",
      "num_cells        : 5\n",
      "print_freq       : 200\n",
      "rand_seed        : 44131\n",
      "save_dir         : ./results/tmp\n",
      "search_space_name : nas-bench-201\n",
      "select_num       : 100\n",
      "track_running_stats : 0\n",
      "workers          : 0\n",
      "Python  Version  : 3.8.8 (default, Feb 24 2021, 21:46:12)  [GCC 7.3.0]\n",
      "Pillow  Version  : 8.1.2\n",
      "PyTorch Version  : 1.8.1\n",
      "cuDNN   Version  : 8005\n",
      "CUDA available   : True\n",
      "CUDA GPU numbers : 2\n",
      "CUDA_VISIBLE_DEVICES : None\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available(), \"CUDA is not available.\"\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# torch.set_num_threads(xargs.workers)\n",
    "prepare_seed(xargs.rand_seed)\n",
    "logger = prepare_logger(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "259c78b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_normal_fanin_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        if m.affine:\n",
    "            nn.init.ones_(m.weight.data)\n",
    "            nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "def kaiming_normal_fanout_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        if m.affine:\n",
    "            nn.init.ones_(m.weight.data)\n",
    "            nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "def init_model(model, method='kaiming_norm_fanin'):\n",
    "    if method == 'kaiming_norm_fanin':\n",
    "        model.apply(kaiming_normal_fanin_init)\n",
    "    elif method == 'kaiming_norm_fanout':\n",
    "        model.apply(kaiming_normal_fanout_init)\n",
    "    return model\n",
    "\n",
    "def search_find_best(xloader, network, lrc_model, n_samples):\n",
    "    network.train()\n",
    "    archs, ntk_scores, lr_scores = [], [], []\n",
    "    \n",
    "    for i in tqdm.tqdm(range(n_samples)):\n",
    "        # random sampling\n",
    "#         arch = network.module.random_genotype(True)\n",
    "        arch = network.random_genotype(True)\n",
    "        \n",
    "        ntk_score_tmp = []\n",
    "        for _ in range(3):\n",
    "            \n",
    "            init_model(network)\n",
    "            # ntk\n",
    "            score = get_ntk_n(xloader, [network], recalbn=0, train_mode=True, num_batch=1)[0]\n",
    "            ntk_score_tmp.append(-score)\n",
    "        ntk_score = np.mean(ntk_score_tmp)\n",
    "        \n",
    "        lr_score_tmp = []\n",
    "        for _ in range(3):\n",
    "            init_model(network)\n",
    "            lrc_model.reinit(models=[network], seed=xargs.rand_seed)\n",
    "            score = lrc_model.forward_batch_sample()\n",
    "            lr_score_tmp.append(score)\n",
    "            lrc_model.clear()\n",
    "        lr_score = np.mean(lr_score_tmp)\n",
    "        \n",
    "        archs.append(arch)\n",
    "        ntk_scores.append(ntk_score)\n",
    "        lr_scores.append(lr_score)\n",
    "        \n",
    "    return archs, ntk_scores, lr_scores\n",
    "   \n",
    "#     rank_ntk, rank_lr = stats.rankdata(ntk_scores), stats.rankdata(lr_scores)\n",
    "\n",
    "#     rank_agg = rank_ntk + rank_lr\n",
    "\n",
    "#     best_idx = np.argmax(rank_agg)\n",
    "#     best_arch, best_ntk_score, best_lr_score = archs[best_idx], rank_ntk[best_idx], rank_lr[best_idx]\n",
    "\n",
    "#     return best_arch, best_ntk_score, best_lr_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ad28c7",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bbfe256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "../../configs/nas-benchmark/algos/RANDOM.config\n",
      "Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=250, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, test_batch_size=512, class_num=10, xshape=(1, 3, 32, 32))\n",
      "||||||| cifar10    ||||||| Search-Loader-Num=391, Valid-Loader-Num=49, batch size=64\n",
      "||||||| cifar10    ||||||| Config=Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=250, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, test_batch_size=512, class_num=10, xshape=(1, 3, 32, 32))\n",
      "try to create the NAS-Bench-201 api from ../../NAS-Bench-201-v1_1-096897.pth\n",
      "[2022-12-23 07:14:26] create API = NASBench201API(15625/15625 architectures, file=NAS-Bench-201-v1_1-096897.pth) done\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "=> do not find the last-info file : results/tmp/seed-44131-last-info.pth\n"
     ]
    }
   ],
   "source": [
    "## data\n",
    "train_data, valid_data, xshape, class_num = get_datasets(xargs.dataset, xargs.data_path, -1)\n",
    "config = load_config(xargs.config_path, {\"class_num\": class_num, \"xshape\": xshape}, logger)\n",
    "search_loader, train_loader, valid_loader = get_nas_search_loaders(train_data,\n",
    "                                                        valid_data,\n",
    "                                                        xargs.dataset,\n",
    "                                                        \"../../configs/nas-benchmark/\",\n",
    "                                                        (config.batch_size, config.test_batch_size),\n",
    "                                                        xargs.workers)\n",
    "logger.log(\"||||||| {:10s} ||||||| Search-Loader-Num={:}, Valid-Loader-Num={:}, batch size={:}\".format(\n",
    "            xargs.dataset, len(search_loader), len(valid_loader), config.batch_size))\n",
    "logger.log(\"||||||| {:10s} ||||||| Config={:}\".format(xargs.dataset, config))\n",
    "\n",
    "\n",
    "## model\n",
    "search_space = get_search_spaces(\"cell\", xargs.search_space_name)\n",
    "model_config = dict2config(\n",
    "    {\n",
    "        \"name\": \"RANDOM\",\n",
    "        \"C\": xargs.channel,\n",
    "        \"N\": xargs.num_cells,\n",
    "        \"max_nodes\": xargs.max_nodes,\n",
    "        \"num_classes\": class_num,\n",
    "        \"space\": search_space,\n",
    "        \"affine\": False,\n",
    "        \"track_running_stats\": bool(xargs.track_running_stats),\n",
    "    },\n",
    "    None,\n",
    ")\n",
    "search_model = get_cell_based_tiny_net(model_config)\n",
    "\n",
    "if xargs.arch_nas_dataset is None:\n",
    "    api = None\n",
    "else:\n",
    "    api = API(xargs.arch_nas_dataset)\n",
    "logger.log(\"{:} create API = {:} done\".format(time_string(), api))\n",
    "\n",
    "last_info, model_base_path, model_best_path = (\n",
    "    logger.path(\"info\"),\n",
    "    logger.path(\"model\"),\n",
    "    logger.path(\"best\"),\n",
    ")\n",
    "# network = torch.nn.DataParallel(search_model).cuda()\n",
    "network = search_model.cuda()\n",
    "\n",
    "## LRC\n",
    "lrc_model = Linear_Region_Collector(input_size=(300, 3, 3, 3), sample_batch=3, dataset=xargs.dataset, data_path=xargs.data_path, seed=xargs.rand_seed)\n",
    "\n",
    "\n",
    "## misc\n",
    "if last_info.exists():  # automatically resume from previous checkpoint\n",
    "    logger.log(\n",
    "        \"=> loading checkpoint of the last-info '{:}' start\".format(last_info)\n",
    "    )\n",
    "    last_info = torch.load(last_info)\n",
    "    start_epoch = last_info[\"epoch\"]\n",
    "    checkpoint = torch.load(last_info[\"last_checkpoint\"])\n",
    "    genotypes = checkpoint[\"genotypes\"]\n",
    "    valid_accuracies = checkpoint[\"valid_accuracies\"]\n",
    "    search_model.load_state_dict(checkpoint[\"search_model\"])\n",
    "    logger.log(\n",
    "        \"=> loading checkpoint of the last-info '{:}' start with {:}-th epoch.\".format(\n",
    "            last_info, start_epoch\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    logger.log(\"=> do not find the last-info file : {:}\".format(last_info))\n",
    "    start_epoch, valid_accuracies, genotypes = 0, {\"best\": -1}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca428b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 243/1000 [30:18<2:17:35, 10.91s/it]"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "start_time, search_time, epoch_time, total_epoch = (\n",
    "    time.time(),\n",
    "    AverageMeter(),\n",
    "    AverageMeter(),\n",
    "    config.epochs + config.warmup,\n",
    ")\n",
    "\n",
    "# cur_arch, cur_ntk_score, cur_lr_score = search_find_best(train_loader, network, lrc_model, xargs.select_num)\n",
    "archs, ntk_scores, lr_scores = search_find_best(train_loader, network, lrc_model, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153ca924",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# def get_valid_acc_from_api(api, arch):\n",
    "#     # print(api.query_by_arch(arch, \"200\"))\n",
    "#     index = api.query_index_by_arch(arch)\n",
    "#     results = api.query_by_index(index, 'cifar10-valid', '200') # a dict of all trials for 1st net on cifar100, where the key is the seed\n",
    "#     acc = 0\n",
    "#     for seed, result in results.items():\n",
    "#         acc = acc + result.get_eval('valid')['accuracy']\n",
    "#     acc = acc / len(results)\n",
    "#     return acc\n",
    "\n",
    "def get_results_from_api(api, arch):\n",
    "    # print(api.query_by_arch(arch, \"200\"))\n",
    "    index = api.query_index_by_arch(arch)\n",
    "    results = api.query_by_index(index, 'cifar10-valid', '200') # a dict of all trials for 1st net on cifar100, where the key is the seed\n",
    "    \n",
    "    # valid acc\n",
    "    acc = 0\n",
    "    for seed, result in results.items():\n",
    "        acc = acc + result.get_eval('valid')['accuracy']\n",
    "    acc = acc / len(results)\n",
    "    \n",
    "    result = list(results.values())[0]\n",
    "    \n",
    "    return acc, result.flop, result.params\n",
    "\n",
    "api_valid_accs, api_flops, api_params = [], [], []\n",
    "for a in archs:\n",
    "    valid_acc, flops, params = get_results_from_api(api, a)\n",
    "    api_valid_accs.append(valid_acc)\n",
    "    api_flops.append(flops)\n",
    "    api_params.append(params)\n",
    "\n",
    "# api_valid_accs = [get_valid_acc_from_api(api, a) for a in archs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbab1711",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"archs\":archs,\n",
    "            \"ntk_scores\":ntk_scores,\n",
    "            \"lr_scores\":lr_scores,\n",
    "            \"api_valid_accs\":api_valid_accs,\n",
    "            \"api_flops\":api_flops,\n",
    "            \"api_params\":api_params,\n",
    "           },\"./c3n5_1000samples.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b4c375",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_ntk, rank_lr, rank_flops, rank_params = stats.rankdata(ntk_scores), stats.rankdata(lr_scores), stats.rankdata(api_flops), stats.rankdata(api_params)\n",
    "\n",
    "l = len(ntk_scores)\n",
    "rank_agg = np.log(rank_ntk/l) + np.log(rank_lr/l) + np.log(rank_flops/l)+ np.log(rank_params/l)\n",
    "\n",
    "best_idx = np.argmax(rank_agg)\n",
    "best_arch, best_ntk_score, best_lr_score, flops, params = archs[best_idx], ntk_scores[best_idx], lr_scores[best_idx], api_flops[best_idx], api_params[best_idx]\n",
    "\n",
    "print(\"RANDOM-NAS finds the best one : {:} with ntk_score={:}, lr_score={:}\".format(best_arch, best_ntk_score, best_lr_score))\n",
    "print(flops, params)\n",
    "print(\"\\n\\n\")\n",
    "if api is not None:\n",
    "    print(\"{:}\".format(api.query_by_arch(best_arch, \"200\")))\n",
    "\n",
    "x = stats.rankdata(rank_agg)\n",
    "y = stats.rankdata(api_valid_accs)\n",
    "# y = api_valid_accs\n",
    "plt.scatter(x, y)\n",
    "plt.scatter(x[best_idx], y[best_idx], c=\"r\", linewidths=4)\n",
    "plt.title(\"rank_agg\")\n",
    "plt.show()\n",
    "tau, p_value = stats.kendalltau(x, y)\n",
    "print(tau, p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b332093",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = stats.rankdata(ntk_scores)\n",
    "y = stats.rankdata(api_valid_accs)\n",
    "plt.scatter(x, y)\n",
    "plt.scatter(x[best_idx], y[best_idx], c=\"r\", linewidths=4)\n",
    "plt.title(\"metric_confidences\")\n",
    "plt.show()\n",
    "tau, p_value = stats.kendalltau(x, y)\n",
    "print(tau, p_value)\n",
    "\n",
    "x = stats.rankdata(lr_scores)\n",
    "y = stats.rankdata(api_valid_accs)\n",
    "plt.scatter(x, y)\n",
    "plt.scatter(x[best_idx], y[best_idx], c=\"r\", linewidths=4)\n",
    "plt.title(\"metric_confidences\")\n",
    "plt.show()\n",
    "tau, p_value = stats.kendalltau(x, y)\n",
    "print(tau, p_value)\n",
    "\n",
    "x = stats.rankdata(api_flops)\n",
    "y = stats.rankdata(api_valid_accs)\n",
    "plt.scatter(x, y)\n",
    "plt.scatter(x[best_idx], y[best_idx], c=\"r\", linewidths=4)\n",
    "plt.title(\"metric_confidences\")\n",
    "plt.show()\n",
    "tau, p_value = stats.kendalltau(x, y)\n",
    "print(tau, p_value)\n",
    "\n",
    "x = stats.rankdata(api_params)\n",
    "y = stats.rankdata(api_valid_accs)\n",
    "plt.scatter(x, y)\n",
    "plt.scatter(x[best_idx], y[best_idx], c=\"r\", linewidths=4)\n",
    "plt.title(\"metric_confidences\")\n",
    "plt.show()\n",
    "tau, p_value = stats.kendalltau(x, y)\n",
    "print(tau, p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c62b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = api.query_index_by_arch(archs[0])\n",
    "results = api.query_by_index(index, 'cifar10-valid', '200')\n",
    "tmp = list(results.values())[0]\n",
    "print(tmp)\n",
    "print(tmp.params)\n",
    "print(tmp.flop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
