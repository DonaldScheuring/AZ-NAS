{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "902bb49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 08:42:44.722920: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, time, glob, random, argparse\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# XAutoDL \n",
    "from xautodl.config_utils import load_config, dict2config, configure2str\n",
    "from xautodl.datasets import get_datasets, get_nas_search_loaders\n",
    "from xautodl.procedures import (\n",
    "    prepare_seed,\n",
    "    prepare_logger,\n",
    "    save_checkpoint,\n",
    "    copy_checkpoint,\n",
    "    get_optim_scheduler,\n",
    ")\n",
    "from xautodl.utils import get_model_infos, obtain_accuracy\n",
    "from xautodl.log_utils import AverageMeter, time_string, convert_secs2time\n",
    "from xautodl.models import get_search_spaces\n",
    "\n",
    "from custom_models import get_cell_based_tiny_net\n",
    "from custom_search_cells import NAS201SearchCell as SearchCell\n",
    "from xautodl.models.cell_searchs.genotypes import Structure\n",
    "\n",
    "# NB201\n",
    "from nas_201_api import NASBench201API as API\n",
    "\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "792763c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99432\n",
      "Namespace(arch_nas_dataset=None, channel=16, config_path='./MY.config', data_path='../cifar.python', dataset='cifar10', max_nodes=4, num_cells=5, print_freq=200, rand_seed=99432, save_dir='./cell_level-arch_loop-reset_cell_params-loop2_ep5_sample200', search_space_name='nas-bench-201', select_num=100, track_running_stats=0, workers=4)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(\"Random search for NAS.\")\n",
    "parser.add_argument(\"--data_path\", type=str, default='../cifar.python', help=\"The path to dataset\")\n",
    "parser.add_argument(\"--dataset\", type=str, default='cifar10',choices=[\"cifar10\", \"cifar100\", \"ImageNet16-120\"], help=\"Choose between Cifar10/100 and ImageNet-16.\")\n",
    "\n",
    "# channels and number-of-cells\n",
    "parser.add_argument(\"--search_space_name\", type=str, default='nas-bench-201', help=\"The search space name.\")\n",
    "parser.add_argument(\"--config_path\", type=str, default='./MY.config', help=\"The path to the configuration.\")\n",
    "parser.add_argument(\"--max_nodes\", type=int, default=4, help=\"The maximum number of nodes.\")\n",
    "parser.add_argument(\"--channel\", type=int, default=16, help=\"The number of channels.\")\n",
    "parser.add_argument(\"--num_cells\", type=int, default=5, help=\"The number of cells in one stage.\")\n",
    "parser.add_argument(\"--select_num\", type=int, default=100, help=\"The number of selected architectures to evaluate.\")\n",
    "parser.add_argument(\"--track_running_stats\", type=int, default=0, choices=[0, 1], help=\"Whether use track_running_stats or not in the BN layer.\")\n",
    "# log\n",
    "parser.add_argument(\"--workers\", type=int, default=4, help=\"number of data loading workers\")\n",
    "parser.add_argument(\"--save_dir\", type=str, default='./cell_level-arch_loop-reset_cell_params-loop2_ep5_sample200', help=\"Folder to save checkpoints and log.\")\n",
    "# parser.add_argument(\"--arch_nas_dataset\", type=str, default='../NAS-Bench-201-v1_1-096897.pth', help=\"The path to load the architecture dataset (tiny-nas-benchmark).\")\n",
    "parser.add_argument(\"--arch_nas_dataset\", type=str, default=None, help=\"The path to load the architecture dataset (tiny-nas-benchmark).\")\n",
    "parser.add_argument(\"--print_freq\", type=int, default=200, help=\"print frequency (default: 200)\")\n",
    "parser.add_argument(\"--rand_seed\", type=int, default=None, help=\"manual seed\")\n",
    "args = parser.parse_args(args=[])\n",
    "if args.rand_seed is None or args.rand_seed < 0:\n",
    "    args.rand_seed = random.randint(1, 100000)\n",
    "\n",
    "    \n",
    "print(args.rand_seed)\n",
    "print(args)\n",
    "xargs=args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dcd1b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Function with logger : Logger(dir=cell_level-arch_loop-reset_cell_params-loop2_ep5_sample200, use-tf=False, writer=None)\n",
      "Arguments : -------------------------------\n",
      "arch_nas_dataset : None\n",
      "channel          : 16\n",
      "config_path      : ./MY.config\n",
      "data_path        : ../cifar.python\n",
      "dataset          : cifar10\n",
      "max_nodes        : 4\n",
      "num_cells        : 5\n",
      "print_freq       : 200\n",
      "rand_seed        : 99432\n",
      "save_dir         : ./cell_level-arch_loop-reset_cell_params-loop2_ep5_sample200\n",
      "search_space_name : nas-bench-201\n",
      "select_num       : 100\n",
      "track_running_stats : 0\n",
      "workers          : 4\n",
      "Python  Version  : 3.7.13 (default, Mar 29 2022, 02:18:16)  [GCC 7.5.0]\n",
      "Pillow  Version  : 9.0.1\n",
      "PyTorch Version  : 1.12.0\n",
      "cuDNN   Version  : 8302\n",
      "CUDA available   : True\n",
      "CUDA GPU numbers : 2\n",
      "CUDA_VISIBLE_DEVICES : None\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available(), \"CUDA is not available.\"\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.set_num_threads(xargs.workers)\n",
    "prepare_seed(xargs.rand_seed)\n",
    "logger = prepare_logger(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9daa5275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "./MY.config\n",
      "Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=50, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, test_batch_size=512, class_num=10, xshape=(1, 3, 32, 32))\n",
      "||||||| cifar10    ||||||| Search-Loader-Num=391, Valid-Loader-Num=49, batch size=64\n",
      "||||||| cifar10    ||||||| Config=Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=50, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, test_batch_size=512, class_num=10, xshape=(1, 3, 32, 32))\n",
      "w-optimizer : SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    initial_lr: 0.025\n",
      "    lr: 0.025\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: True\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "w-scheduler : CosineAnnealingLR(warmup=0, max-epoch=50, current::epoch=0, iter=0.00, type=cosine, T-max=50, eta-min=0.001)\n",
      "criterion   : CrossEntropyLoss()\n",
      "[2022-11-03 08:42:47] create API = None done\n",
      "=> do not find the last-info file : cell_level-arch_loop-reset_cell_params-loop2_ep5_sample200/seed-99432-last-info.pth\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, xshape, class_num = get_datasets(xargs.dataset, xargs.data_path, -1)\n",
    "config = load_config(xargs.config_path, {\"class_num\": class_num, \"xshape\": xshape}, logger)\n",
    "search_loader, _, valid_loader = get_nas_search_loaders(train_data,\n",
    "                                                        valid_data,\n",
    "                                                        xargs.dataset,\n",
    "                                                        \"../configs/nas-benchmark/\",\n",
    "                                                        (config.batch_size, config.test_batch_size),\n",
    "                                                        xargs.workers)\n",
    "logger.log(\"||||||| {:10s} ||||||| Search-Loader-Num={:}, Valid-Loader-Num={:}, batch size={:}\".format(\n",
    "            xargs.dataset, len(search_loader), len(valid_loader), config.batch_size))\n",
    "logger.log(\"||||||| {:10s} ||||||| Config={:}\".format(xargs.dataset, config))\n",
    "\n",
    "search_space = get_search_spaces(\"cell\", xargs.search_space_name)\n",
    "model_config = dict2config(\n",
    "    {\n",
    "        \"name\": \"RANDOM\",\n",
    "        \"C\": xargs.channel,\n",
    "        \"N\": xargs.num_cells,\n",
    "        \"max_nodes\": xargs.max_nodes,\n",
    "        \"num_classes\": class_num,\n",
    "        \"space\": search_space,\n",
    "        \"affine\": False,\n",
    "        \"track_running_stats\": bool(xargs.track_running_stats),\n",
    "    },\n",
    "    None,\n",
    ")\n",
    "search_model = get_cell_based_tiny_net(model_config)\n",
    "\n",
    "w_optimizer, w_scheduler, criterion = get_optim_scheduler(search_model.parameters(), config)\n",
    "\n",
    "logger.log(\"w-optimizer : {:}\".format(w_optimizer))\n",
    "logger.log(\"w-scheduler : {:}\".format(w_scheduler))\n",
    "logger.log(\"criterion   : {:}\".format(criterion))\n",
    "# if xargs.arch_nas_dataset is None:\n",
    "api = None\n",
    "# else:\n",
    "#     api = API(xargs.arch_nas_dataset)\n",
    "logger.log(\"{:} create API = {:} done\".format(time_string(), api))\n",
    "\n",
    "last_info, model_base_path, model_best_path = (\n",
    "    logger.path(\"info\"),\n",
    "    logger.path(\"model\"),\n",
    "    logger.path(\"best\"),\n",
    ")\n",
    "network, criterion = torch.nn.DataParallel(search_model).cuda(), criterion.cuda()\n",
    "\n",
    "if last_info.exists():  # automatically resume from previous checkpoint\n",
    "    logger.log(\n",
    "        \"=> loading checkpoint of the last-info '{:}' start\".format(last_info)\n",
    "    )\n",
    "    last_info = torch.load(last_info)\n",
    "    start_epoch = last_info[\"epoch\"]\n",
    "    checkpoint = torch.load(last_info[\"last_checkpoint\"])\n",
    "    genotypes = checkpoint[\"genotypes\"]\n",
    "    valid_accuracies = checkpoint[\"valid_accuracies\"]\n",
    "    search_model.load_state_dict(checkpoint[\"search_model\"])\n",
    "    w_scheduler.load_state_dict(checkpoint[\"w_scheduler\"])\n",
    "    w_optimizer.load_state_dict(checkpoint[\"w_optimizer\"])\n",
    "    logger.log(\n",
    "        \"=> loading checkpoint of the last-info '{:}' start with {:}-th epoch.\".format(\n",
    "            last_info, start_epoch\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    logger.log(\"=> do not find the last-info file : {:}\".format(last_info))\n",
    "    start_epoch, valid_accuracies, genotypes = 0, {\"best\": -1}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94ba0e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_confidence_robustness_metrics(network, inputs, targets):\n",
    "    with torch.no_grad():\n",
    "        # accuracy\n",
    "        network.train()\n",
    "        _, logits = network(inputs)\n",
    "        val_top1, val_top5 = obtain_accuracy(logits.data, targets.data, topk=(1, 5))\n",
    "        acc = val_top1\n",
    "        \n",
    "        # confidence\n",
    "        prob = torch.nn.functional.softmax(logits, dim=1)\n",
    "        one_hot_idx = torch.nn.functional.one_hot(targets)\n",
    "        confidence = (prob[one_hot_idx==1].sum()) / inputs.size(0) * 100 # in percent\n",
    "        \n",
    "        # sensitivity\n",
    "        _, noisy_logits = network(inputs + torch.randn_like(inputs)*0.1)\n",
    "        kl_loss = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        sensitivity = kl_loss(torch.nn.functional.log_softmax(noisy_logits, dim=1), torch.nn.functional.softmax(logits, dim=1))\n",
    "        \n",
    "        # robustness\n",
    "        original_weights = deepcopy(network.state_dict())\n",
    "        for m in network.modules():\n",
    "            if isinstance(m, SearchCell):\n",
    "                for p in m.parameters():\n",
    "                    p.add_(torch.randn_like(p) * p.std()*0.3)\n",
    "            \n",
    "        _, noisy_logits = network(inputs)\n",
    "        kl_loss = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        robustness = -kl_loss(torch.nn.functional.log_softmax(noisy_logits, dim=1), torch.nn.functional.softmax(logits, dim=1))\n",
    "        network.load_state_dict(original_weights)\n",
    "                \n",
    "        return acc.item(), confidence.item(), sensitivity.item(), robustness.item()\n",
    "    \n",
    "def step_sim_metric(network, criterion, inputs, targets):\n",
    "#     inputs, targets = inputs[:64], targets[:64] # smaller batches\n",
    "    original_dict = deepcopy(network.state_dict())\n",
    "    optim_large_step = torch.optim.SGD(network.parameters(), lr=0.025)\n",
    "    \n",
    "    # single large step\n",
    "    network.train()\n",
    "    optim_large_step.zero_grad()\n",
    "    _, logits = network(inputs)\n",
    "    base_loss = criterion(logits, targets)\n",
    "    base_loss.backward()\n",
    "    optim_large_step.step()\n",
    "    large_step_dict = deepcopy(network.state_dict())\n",
    "    \n",
    "    # multiple small steps\n",
    "    network.load_state_dict(original_dict)\n",
    "    optim_small_step = torch.optim.SGD(network.parameters(), lr=0.025/3)\n",
    "    for i in range(3):\n",
    "        optim_small_step.zero_grad()\n",
    "        _, logits = network(inputs)\n",
    "        base_loss = criterion(logits, targets)\n",
    "        base_loss.backward()\n",
    "        optim_small_step.step()\n",
    "    small_step_dict = deepcopy(network.state_dict())\n",
    "    scores = []\n",
    "    for key in large_step_dict.keys():\n",
    "        if ('weight' in key) and (original_dict[key].dim()==4):\n",
    "            if (original_dict[key] != large_step_dict[key]).sum():\n",
    "                large_step = large_step_dict[key] - original_dict[key]\n",
    "                small_step = small_step_dict[key] - original_dict[key]\n",
    "                co, ci, kh, kw = large_step.size()\n",
    "                large_step = large_step.view(co, -1)\n",
    "                small_step = small_step.view(co, -1)\n",
    "                score = torch.nn.functional.cosine_similarity(large_step, small_step, dim=1)\n",
    "                score = score.mean().item() * 100 # in percent\n",
    "                scores.append(score)\n",
    "    if len(scores)==0:\n",
    "        step_sim = 100\n",
    "        raise RuntimeError\n",
    "    else:\n",
    "        step_sim = np.mean(scores)\n",
    "    \n",
    "    # resume\n",
    "    network.load_state_dict(original_dict)\n",
    "            \n",
    "    return step_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cdab22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of nodes:60\n",
      "target_node:1 4\n",
      "target_node:2 24\n",
      "target_node:3 124\n",
      "\n",
      "\n",
      " Searching with a cell #0\n",
      "*Train* [2022-11-03 08:42:54] Ep:0 [000/391] Time 4.51 (4.51) Data 0.23 (0.23) Base [Loss 2.423 (2.423)  Prec@1 7.81 (7.81) Prec@5 37.50 (37.50)]\n",
      "*Train* [2022-11-03 08:43:19] Ep:0 [200/391] Time 0.15 (0.14) Data 0.00 (0.00) Base [Loss 1.612 (1.808)  Prec@1 42.19 (32.42) Prec@5 92.19 (84.72)]\n",
      "*Train* [2022-11-03 08:43:42] Ep:0 [390/391] Time 0.08 (0.13) Data 0.00 (0.00) Base [Loss 1.427 (1.676)  Prec@1 52.50 (37.66) Prec@5 87.50 (87.67)]\n",
      "Ep:0 ends : loss=1.68, accuracy@1=37.66%, accuracy@5=87.67%\n",
      "*Train* [2022-11-03 08:43:42] Ep:1 [000/391] Time 0.43 (0.43) Data 0.34 (0.34) Base [Loss 3.101 (3.101)  Prec@1 7.81 (7.81) Prec@5 51.56 (51.56)]\n",
      "*Train* [2022-11-03 08:44:05] Ep:1 [200/391] Time 0.11 (0.12) Data 0.00 (0.00) Base [Loss 1.420 (1.537)  Prec@1 45.31 (43.77) Prec@5 93.75 (90.46)]\n",
      "*Train* [2022-11-03 08:44:29] Ep:1 [390/391] Time 0.22 (0.12) Data 0.00 (0.00) Base [Loss 1.348 (1.446)  Prec@1 50.00 (47.48) Prec@5 95.00 (92.02)]\n",
      "Ep:1 ends : loss=1.45, accuracy@1=47.48%, accuracy@5=92.02%\n",
      "*Train* [2022-11-03 08:44:29] Ep:2 [000/391] Time 0.42 (0.42) Data 0.32 (0.32) Base [Loss 3.126 (3.126)  Prec@1 7.81 (7.81) Prec@5 54.69 (54.69)]\n",
      "*Train* [2022-11-03 08:44:54] Ep:2 [200/391] Time 0.20 (0.12) Data 0.00 (0.00) Base [Loss 1.360 (1.533)  Prec@1 50.00 (43.56) Prec@5 98.44 (90.30)]\n",
      "*Train* [2022-11-03 08:45:15] Ep:2 [390/391] Time 0.09 (0.12) Data 0.00 (0.00) Base [Loss 1.436 (1.419)  Prec@1 60.00 (48.26) Prec@5 92.50 (92.19)]\n",
      "Ep:2 ends : loss=1.42, accuracy@1=48.26%, accuracy@5=92.19%\n",
      "*Train* [2022-11-03 08:45:15] Ep:3 [000/391] Time 0.41 (0.41) Data 0.30 (0.30) Base [Loss 2.345 (2.345)  Prec@1 28.12 (28.12) Prec@5 70.31 (70.31)]\n",
      "*Train* [2022-11-03 08:45:39] Ep:3 [200/391] Time 0.12 (0.12) Data 0.00 (0.00) Base [Loss 1.149 (1.277)  Prec@1 64.06 (54.73) Prec@5 95.31 (94.25)]\n",
      "*Train* [2022-11-03 08:46:03] Ep:3 [390/391] Time 0.22 (0.12) Data 0.00 (0.00) Base [Loss 1.148 (1.227)  Prec@1 60.00 (56.31) Prec@5 95.00 (94.80)]\n",
      "Ep:3 ends : loss=1.23, accuracy@1=56.31%, accuracy@5=94.80%\n",
      "*Train* [2022-11-03 08:46:04] Ep:4 [000/391] Time 0.45 (0.45) Data 0.36 (0.36) Base [Loss 1.719 (1.719)  Prec@1 31.25 (31.25) Prec@5 90.62 (90.62)]\n",
      "*Train* [2022-11-03 08:46:27] Ep:4 [200/391] Time 0.21 (0.12) Data 0.00 (0.00) Base [Loss 1.079 (1.196)  Prec@1 60.94 (57.17) Prec@5 96.88 (95.37)]\n",
      "*Train* [2022-11-03 08:46:49] Ep:4 [390/391] Time 0.07 (0.12) Data 0.00 (0.00) Base [Loss 1.246 (1.157)  Prec@1 62.50 (58.79) Prec@5 90.00 (95.53)]\n",
      "Ep:4 ends : loss=1.16, accuracy@1=58.79%, accuracy@5=95.53%\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "start_time, search_time, epoch_time, total_epoch = (\n",
    "    time.time(),\n",
    "    AverageMeter(),\n",
    "    AverageMeter(),\n",
    "    config.epochs + config.warmup,\n",
    ")\n",
    "\n",
    "################# initialize\n",
    "cells = []\n",
    "for m in network.modules():\n",
    "    if isinstance(m, SearchCell):\n",
    "        cells.append(m)\n",
    "num_cells = len(cells)\n",
    "print(\"total number of nodes:{}\".format(num_cells*xargs.max_nodes))\n",
    "        \n",
    "op_names = deepcopy(cells[0].op_names)\n",
    "op_names_wo_none = deepcopy(op_names)\n",
    "if \"none\" in op_names_wo_none:\n",
    "    op_names_wo_none.remove(\"none\")\n",
    "\n",
    "genotypes = []\n",
    "for i in range(1, xargs.max_nodes):\n",
    "    xlist = []\n",
    "    for j in range(i):\n",
    "        node_str = \"{:}<-{:}\".format(i, j)\n",
    "        if i-j==1:\n",
    "            op_name = \"skip_connect\"\n",
    "        else:\n",
    "            op_name = \"none\"\n",
    "        xlist.append((op_name, j))\n",
    "    genotypes.append(tuple(xlist))\n",
    "init_arch = Structure(genotypes)\n",
    "\n",
    "for c in cells:\n",
    "    c.arch_cache = init_arch\n",
    "\n",
    "### gen possible connections of a target node\n",
    "possible_connections = {}\n",
    "for target_node_idx in range(1,xargs.max_nodes):\n",
    "    possible_connections[target_node_idx] = list()\n",
    "    xlists = []\n",
    "    for src_node in range(target_node_idx):\n",
    "        node_str = \"{:}<-{:}\".format(target_node_idx, src_node)\n",
    "        # select possible ops\n",
    "#         if target_node_idx - src_node == 1:\n",
    "#             op_names_tmp = op_names_wo_none\n",
    "#         else:\n",
    "#             op_names_tmp = op_names\n",
    "        op_names_tmp = op_names\n",
    "            \n",
    "        if len(xlists) == 0: # initial iteration\n",
    "            for op_name in op_names_tmp:\n",
    "                xlists.append([(op_name, src_node)])\n",
    "        else:\n",
    "            new_xlists = []\n",
    "            for op_name in op_names_tmp:\n",
    "                for xlist in xlists:\n",
    "                    new_xlist = deepcopy(xlist)\n",
    "                    new_xlist.append((op_name, src_node))\n",
    "                    new_xlists.append(new_xlist)\n",
    "            xlists = new_xlists\n",
    "    for xlist in xlists:\n",
    "        selected_ops = []\n",
    "        for l in xlist:\n",
    "            selected_ops.append(l[0])\n",
    "        if sum(np.array(selected_ops) == \"none\") == len(selected_ops):\n",
    "            continue\n",
    "        possible_connections[target_node_idx].append(tuple(xlist))\n",
    "    print(\"target_node:{}\".format(target_node_idx), len(possible_connections[target_node_idx]))\n",
    "        \n",
    "### train while generating random architectures by mutating connections of a target node\n",
    "\n",
    "for arch_loop in range(2):\n",
    "    for target_cell_idx in range(num_cells):\n",
    "        target_cell = cells[target_cell_idx]\n",
    "        print(\"\\n\\n Searching with a cell #{}\".format(target_cell_idx))\n",
    "        ####\n",
    "        for m in target_cell.modules():\n",
    "            if hasattr(m, 'reset_parameters'):\n",
    "                m.reset_parameters()\n",
    "        ####\n",
    "        ## training\n",
    "        for ep in range(5):\n",
    "            ###\n",
    "            genotypes = []\n",
    "            for n in range(1, xargs.max_nodes):\n",
    "                genotypes.append(random.choice(possible_connections[n]))\n",
    "            arch = Structure(genotypes)\n",
    "            target_cell.arch_cache = arch\n",
    "#             arch = target_cell.random_genotype(True)\n",
    "            ###\n",
    "            data_time, batch_time = AverageMeter(), AverageMeter()\n",
    "            base_losses, base_top1, base_top5 = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "            network.train()\n",
    "            end = time.time()\n",
    "            print_freq = 200\n",
    "            for step, (base_inputs, base_targets, arch_inputs, arch_targets) in enumerate(search_loader):\n",
    "                ######### forward/backward/optim\n",
    "                base_targets = base_targets.cuda(non_blocking=True)\n",
    "                arch_targets = arch_targets.cuda(non_blocking=True)\n",
    "                # measure data loading time\n",
    "                data_time.update(time.time() - end)\n",
    "                w_optimizer.zero_grad()\n",
    "                _, logits = network(base_inputs)\n",
    "                base_loss = criterion(logits, base_targets)\n",
    "                base_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(network.parameters(), 5)\n",
    "                w_optimizer.step()\n",
    "\n",
    "                ######### logging\n",
    "                base_prec1, base_prec5 = obtain_accuracy(logits.data, base_targets.data, topk=(1, 5))\n",
    "                base_losses.update(base_loss.item(), base_inputs.size(0))\n",
    "                base_top1.update(base_prec1.item(), base_inputs.size(0))\n",
    "                base_top5.update(base_prec5.item(), base_inputs.size(0))\n",
    "                batch_time.update(time.time() - end)\n",
    "                end = time.time()\n",
    "                if step % print_freq == 0 or step + 1 == len(search_loader):\n",
    "                    Sstr = (\"*Train* \"+ time_string()+\" Ep:{:} [{:03d}/{:03d}]\".format(ep, step, len(search_loader)))\n",
    "                    Tstr = \"Time {batch_time.val:.2f} ({batch_time.avg:.2f}) Data {data_time.val:.2f} ({data_time.avg:.2f})\".format(batch_time=batch_time, data_time=data_time)\n",
    "                    Wstr = \"Base [Loss {loss.val:.3f} ({loss.avg:.3f})  Prec@1 {top1.val:.2f} ({top1.avg:.2f}) Prec@5 {top5.val:.2f} ({top5.avg:.2f})]\".format(loss=base_losses, top1=base_top1, top5=base_top5)\n",
    "                    logger.log(Sstr + \" \" + Tstr + \" \" + Wstr)\n",
    "\n",
    "            logger.log(\"Ep:{:} ends : loss={:.2f}, accuracy@1={:.2f}%, accuracy@5={:.2f}%\".format(ep, base_losses.avg, base_top1.avg, base_top5.avg))\n",
    "        ## evaluation\n",
    "        network.train()\n",
    "        archs, metric_accs, metric_confidences, metric_sensitivities, metric_robustnesses, metric_step_sims = [], [], [], [], [], []\n",
    "        loader_iter = iter(valid_loader)\n",
    "        for search_iter in range(200):\n",
    "            ###### random gen\n",
    "            genotypes = []\n",
    "            for n in range(1, xargs.max_nodes):\n",
    "                genotypes.append(random.choice(possible_connections[n]))\n",
    "            arch = Structure(genotypes)\n",
    "            target_cell.arch_cache = arch\n",
    "#             arch = target_cell.random_genotype(True)\n",
    "            ###### measure metrics\n",
    "            try:\n",
    "                inputs, targets = next(loader_iter)\n",
    "            except:\n",
    "                loader_iter = iter(valid_loader)\n",
    "                inputs, targets = next(loader_iter)\n",
    "            inputs, targets = inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True)\n",
    "            valid_acc, confidence, sensitivity, robustness = acc_confidence_robustness_metrics(network, inputs, targets)\n",
    "            step_sim = step_sim_metric(network, criterion, inputs, targets)\n",
    "            archs.append(arch)\n",
    "            metric_accs.append(valid_acc)\n",
    "            metric_confidences.append(confidence)\n",
    "            metric_sensitivities.append(sensitivity)\n",
    "            metric_robustnesses.append(robustness)\n",
    "            metric_step_sims.append(step_sim)\n",
    "        rank_accs, rank_confidences, rank_sensitivities, rank_robustnesses, rank_step_sims = stats.rankdata(metric_accs), stats.rankdata(metric_confidences), stats.rankdata(metric_sensitivities), stats.rankdata(metric_robustnesses), stats.rankdata(metric_step_sims)\n",
    "        l = len(rank_accs)\n",
    "        rank_agg = np.log(rank_accs/l)+np.log(rank_confidences/l)+np.log(rank_sensitivities/l)+np.log(rank_robustnesses/l)+np.log(rank_step_sims/l)\n",
    "#             rank_agg = np.log(rank_accs/l)+np.log(rank_confidences/l)+np.log(rank_sensitivities/l)+np.log(rank_step_sims/l)\n",
    "        best_idx = np.argmax(rank_agg)\n",
    "        best_arch, best_acc, best_conf, best_sensitivity, best_robust, best_step_sim = archs[best_idx], metric_accs[best_idx], metric_confidences[best_idx], metric_sensitivities[best_idx], metric_robustnesses[best_idx], metric_step_sims[best_idx]\n",
    "        logger.log(\"Found best op for target cell:{}\".format(target_cell_idx))\n",
    "        logger.log(\": {:} with accuracy={:.2f}%, confidence={:.3f}%, sensitivity={:.3f}, robustness={:.3f}, step_sim={:.3f}\".format(best_arch, best_acc, best_conf, best_sensitivity, best_robust, best_step_sim))\n",
    "        target_cell.arch_cache = best_arch\n",
    "            \n",
    "best_archs = []\n",
    "for c in cells:\n",
    "    best_archs.append(c.arch_cache)\n",
    "    \n",
    "torch.save({\"model\":search_model.state_dict(), \"best_archs\":best_archs}, os.path.join(xargs.save_dir, \"output.pth\"))\n",
    "\n",
    "for m in search_model.modules():\n",
    "    if isinstance(m, SearchCell):\n",
    "        logger.log(m.arch_cache)\n",
    "\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bde3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(rank_confidences,rank_accs)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(rank_sensitivities,rank_accs)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(rank_robustnesses,rank_accs)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(rank_step_sims,rank_accs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da32ec",
   "metadata": {},
   "source": [
    "# Train a found model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ae460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_output = torch.load(os.path.join(xargs.save_dir, \"output.pth\"))\n",
    "print(args)\n",
    "args.save_dir = os.path.join(xargs.save_dir, \"train\")\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed27598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c02f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = prepare_logger(args)\n",
    "\n",
    "# cifar_train_config_path = \"./MY.config\"\n",
    "cifar_train_config_path = \"../configs/nas-benchmark/CIFAR.config\"\n",
    "###\n",
    "train_data, test_data, xshape, class_num = get_datasets(xargs.dataset, xargs.data_path, -1)\n",
    "config = load_config(cifar_train_config_path, {\"class_num\": class_num, \"xshape\": xshape}, logger)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "            train_data,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=xargs.workers,\n",
    "            pin_memory=True,)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "            test_data,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=xargs.workers,\n",
    "            pin_memory=True,)\n",
    "\n",
    "# search_loader, _, valid_loader = get_nas_search_loaders(train_data,\n",
    "#                                                         valid_data,\n",
    "#                                                         xargs.dataset,\n",
    "#                                                         \"../configs/nas-benchmark/\",\n",
    "#                                                         (config.batch_size, config.batch_size),\n",
    "#                                                         xargs.workers)\n",
    "logger.log(\"||||||| {:10s} ||||||| Train-Loader-Num={:}, Test-Loader-Num={:}, batch size={:}\".format(\n",
    "            xargs.dataset, len(train_loader), len(test_loader), config.batch_size))\n",
    "logger.log(\"||||||| {:10s} ||||||| Config={:}\".format(xargs.dataset, config))\n",
    "\n",
    "search_space = get_search_spaces(\"cell\", xargs.search_space_name)\n",
    "model_config = dict2config(\n",
    "    {\n",
    "        \"name\": \"RANDOM\",\n",
    "        \"C\": xargs.channel,\n",
    "        \"N\": xargs.num_cells,\n",
    "        \"max_nodes\": xargs.max_nodes,\n",
    "        \"num_classes\": class_num,\n",
    "        \"space\": search_space,\n",
    "        \"affine\": False,\n",
    "        \"track_running_stats\": True, # true for eval\n",
    "    },\n",
    "    None,\n",
    ")\n",
    "search_model = get_cell_based_tiny_net(model_config)\n",
    "\n",
    "### load\n",
    "# trained_output = torch.load(os.path.join(xargs.save_dir, \"output.pth\"))\n",
    "# search_model.load_state_dict(trained_output['model'], strict=False)\n",
    "best_archs = trained_output['best_archs']\n",
    "i=0\n",
    "for m in search_model.modules():\n",
    "    if isinstance(m, SearchCell):\n",
    "        m.arch_cache = best_archs[i]\n",
    "        i += 1\n",
    "for m in network.modules():\n",
    "    if isinstance(m, SearchCell):\n",
    "        print(m.arch_cache)\n",
    "###\n",
    "\n",
    "w_optimizer, w_scheduler, criterion = get_optim_scheduler(search_model.parameters(), config)\n",
    "\n",
    "logger.log(\"w-optimizer : {:}\".format(w_optimizer))\n",
    "logger.log(\"w-scheduler : {:}\".format(w_scheduler))\n",
    "logger.log(\"criterion   : {:}\".format(criterion))\n",
    "\n",
    "network, criterion = torch.nn.DataParallel(search_model).cuda(), criterion.cuda()\n",
    "\n",
    "last_info, model_base_path, model_best_path = (\n",
    "    logger.path(\"info\"),\n",
    "    logger.path(\"model\"),\n",
    "    logger.path(\"best\"),\n",
    ")\n",
    "\n",
    "start_epoch, valid_accuracies, genotypes = 0, {\"best\": -1}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964fcb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def search_func_one_arch(xloader, network, criterion, scheduler, w_optimizer, epoch_str, print_freq, logger):\n",
    "#     data_time, batch_time = AverageMeter(), AverageMeter()\n",
    "#     base_losses, base_top1, base_top5 = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "#     network.train()\n",
    "#     end = time.time()\n",
    "#     for step, (base_inputs, base_targets, arch_inputs, arch_targets) in enumerate(\n",
    "#         xloader\n",
    "#     ):\n",
    "#         scheduler.update(None, 1.0 * step / len(xloader))\n",
    "#         base_targets = base_targets.cuda(non_blocking=True)\n",
    "#         arch_targets = arch_targets.cuda(non_blocking=True)\n",
    "#         # measure data loading time\n",
    "#         data_time.update(time.time() - end)\n",
    "\n",
    "#         w_optimizer.zero_grad()\n",
    "#         _, logits = network(base_inputs)\n",
    "#         base_loss = criterion(logits, base_targets)\n",
    "#         base_loss.backward()\n",
    "#         nn.utils.clip_grad_norm_(network.parameters(), 5)\n",
    "#         w_optimizer.step()\n",
    "#         # record\n",
    "#         base_prec1, base_prec5 = obtain_accuracy(\n",
    "#             logits.data, base_targets.data, topk=(1, 5)\n",
    "#         )\n",
    "#         base_losses.update(base_loss.item(), base_inputs.size(0))\n",
    "#         base_top1.update(base_prec1.item(), base_inputs.size(0))\n",
    "#         base_top5.update(base_prec5.item(), base_inputs.size(0))\n",
    "\n",
    "#         # measure elapsed time\n",
    "#         batch_time.update(time.time() - end)\n",
    "#         end = time.time()\n",
    "\n",
    "#         if step % print_freq == 0 or step + 1 == len(xloader):\n",
    "#             Sstr = (\n",
    "#                 \"*SEARCH* \"\n",
    "#                 + time_string()\n",
    "#                 + \" [{:}][{:03d}/{:03d}]\".format(epoch_str, step, len(xloader))\n",
    "#             )\n",
    "#             Tstr = \"Time {batch_time.val:.2f} ({batch_time.avg:.2f}) Data {data_time.val:.2f} ({data_time.avg:.2f})\".format(\n",
    "#                 batch_time=batch_time, data_time=data_time\n",
    "#             )\n",
    "#             Wstr = \"Base [Loss {loss.val:.3f} ({loss.avg:.3f})  Prec@1 {top1.val:.2f} ({top1.avg:.2f}) Prec@5 {top5.val:.2f} ({top5.avg:.2f})]\".format(\n",
    "#                 loss=base_losses, top1=base_top1, top5=base_top5\n",
    "#             )\n",
    "#             logger.log(Sstr + \" \" + Tstr + \" \" + Wstr)\n",
    "#     return base_losses.avg, base_top1.avg, base_top5.avg\n",
    "\n",
    "def train_func_one_arch(xloader, network, criterion, scheduler, w_optimizer, epoch_str, print_freq, logger):\n",
    "    data_time, batch_time = AverageMeter(), AverageMeter()\n",
    "    base_losses, base_top1, base_top5 = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    network.train()\n",
    "    end = time.time()\n",
    "    for step, (base_inputs, base_targets) in enumerate(\n",
    "        xloader\n",
    "    ):\n",
    "        scheduler.update(None, 1.0 * step / len(xloader))\n",
    "        base_targets = base_targets.cuda(non_blocking=True)\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        w_optimizer.zero_grad()\n",
    "        _, logits = network(base_inputs)\n",
    "        base_loss = criterion(logits, base_targets)\n",
    "        base_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(network.parameters(), 5)\n",
    "        w_optimizer.step()\n",
    "        # record\n",
    "        base_prec1, base_prec5 = obtain_accuracy(\n",
    "            logits.data, base_targets.data, topk=(1, 5)\n",
    "        )\n",
    "        base_losses.update(base_loss.item(), base_inputs.size(0))\n",
    "        base_top1.update(base_prec1.item(), base_inputs.size(0))\n",
    "        base_top5.update(base_prec5.item(), base_inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if step % print_freq == 0 or step + 1 == len(xloader):\n",
    "            Sstr = (\n",
    "                \"*SEARCH* \"\n",
    "                + time_string()\n",
    "                + \" [{:}][{:03d}/{:03d}]\".format(epoch_str, step, len(xloader))\n",
    "            )\n",
    "            Tstr = \"Time {batch_time.val:.2f} ({batch_time.avg:.2f}) Data {data_time.val:.2f} ({data_time.avg:.2f})\".format(\n",
    "                batch_time=batch_time, data_time=data_time\n",
    "            )\n",
    "            Wstr = \"Base [Loss {loss.val:.3f} ({loss.avg:.3f})  Prec@1 {top1.val:.2f} ({top1.avg:.2f}) Prec@5 {top5.val:.2f} ({top5.avg:.2f})]\".format(\n",
    "                loss=base_losses, top1=base_top1, top5=base_top5\n",
    "            )\n",
    "            logger.log(Sstr + \" \" + Tstr + \" \" + Wstr)\n",
    "    return base_losses.avg, base_top1.avg, base_top5.avg\n",
    "\n",
    "def valid_func_one_arch(xloader, network, criterion):\n",
    "    data_time, batch_time = AverageMeter(), AverageMeter()\n",
    "    arch_losses, arch_top1, arch_top5 = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    network.eval()\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for step, (arch_inputs, arch_targets) in enumerate(xloader):\n",
    "            arch_targets = arch_targets.cuda(non_blocking=True)\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "            # prediction\n",
    "\n",
    "#             network.module.random_genotype_per_cell(True)\n",
    "            _, logits = network(arch_inputs)\n",
    "            arch_loss = criterion(logits, arch_targets)\n",
    "            # record\n",
    "            arch_prec1, arch_prec5 = obtain_accuracy(\n",
    "                logits.data, arch_targets.data, topk=(1, 5)\n",
    "            )\n",
    "            arch_losses.update(arch_loss.item(), arch_inputs.size(0))\n",
    "            arch_top1.update(arch_prec1.item(), arch_inputs.size(0))\n",
    "            arch_top5.update(arch_prec5.item(), arch_inputs.size(0))\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "    return arch_losses.avg, arch_top1.avg, arch_top5.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be07a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time, search_time, epoch_time, total_epoch = (\n",
    "    time.time(),\n",
    "    AverageMeter(),\n",
    "    AverageMeter(),\n",
    "    config.epochs + config.warmup,\n",
    ")\n",
    "for epoch in range(0, total_epoch):\n",
    "    w_scheduler.update(epoch, 0.0)\n",
    "    need_time = \"Time Left: {:}\".format(\n",
    "        convert_secs2time(epoch_time.val * (total_epoch - epoch), True)\n",
    "    )\n",
    "    epoch_str = \"{:03d}-{:03d}\".format(epoch, total_epoch)\n",
    "    logger.log(\n",
    "        \"\\n[Search the {:}-th epoch] {:}, LR={:}\".format(\n",
    "            epoch_str, need_time, min(w_scheduler.get_lr())\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # selected_arch = search_find_best(valid_loader, network, criterion, xargs.select_num)\n",
    "    search_w_loss, search_w_top1, search_w_top5 = train_func_one_arch(\n",
    "        train_loader,\n",
    "        network,\n",
    "        criterion,\n",
    "        w_scheduler,\n",
    "        w_optimizer,\n",
    "        epoch_str,\n",
    "        xargs.print_freq,\n",
    "        logger,\n",
    "    )\n",
    "    search_time.update(time.time() - start_time)\n",
    "    logger.log(\n",
    "        \"[{:}] searching : loss={:.2f}, accuracy@1={:.2f}%, accuracy@5={:.2f}%, time-cost={:.1f} s\".format(\n",
    "            epoch_str, search_w_loss, search_w_top1, search_w_top5, search_time.sum\n",
    "        )\n",
    "    )\n",
    "    valid_a_loss, valid_a_top1, valid_a_top5 = valid_func_one_arch(\n",
    "        test_loader, network, criterion\n",
    "    )\n",
    "    logger.log(\n",
    "        \"[{:}] evaluate  : loss={:.2f}, accuracy@1={:.2f}%, accuracy@5={:.2f}%\".format(\n",
    "            epoch_str, valid_a_loss, valid_a_top1, valid_a_top5\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # check the best accuracy\n",
    "    valid_accuracies[epoch] = valid_a_top1\n",
    "    if valid_a_top1 > valid_accuracies[\"best\"]:\n",
    "        valid_accuracies[\"best\"] = valid_a_top1\n",
    "        find_best = True\n",
    "    else:\n",
    "        find_best = False\n",
    "\n",
    "    # save checkpoint\n",
    "    save_path = save_checkpoint(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"args\": deepcopy(xargs),\n",
    "            \"search_model\": search_model.state_dict(),\n",
    "            \"w_optimizer\": w_optimizer.state_dict(),\n",
    "            \"w_scheduler\": w_scheduler.state_dict(),\n",
    "            \"genotypes\": genotypes,\n",
    "            \"valid_accuracies\": valid_accuracies,\n",
    "        },\n",
    "        model_base_path,\n",
    "        logger,\n",
    "    )\n",
    "    last_info = save_checkpoint(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"args\": deepcopy(args),\n",
    "            \"last_checkpoint\": save_path,\n",
    "        },\n",
    "        logger.path(\"info\"),\n",
    "        logger,\n",
    "    )\n",
    "    if find_best:\n",
    "        logger.log(\n",
    "            \"<<<--->>> The {:}-th epoch : find the highest validation accuracy : {:.2f}%.\".format(\n",
    "                epoch_str, valid_a_top1\n",
    "            )\n",
    "        )\n",
    "        copy_checkpoint(model_base_path, model_best_path, logger)\n",
    "    if api is not None:\n",
    "        logger.log(\"{:}\".format(api.query_by_arch(genotypes[epoch], \"200\")))\n",
    "    # measure elapsed time\n",
    "    epoch_time.update(time.time() - start_time)\n",
    "    start_time = time.time()\n",
    "\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d00afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_archs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
