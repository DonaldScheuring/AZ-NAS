{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15198afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 14:30:18.670549: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, time, glob, random, argparse\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# XAutoDL \n",
    "from xautodl.config_utils import load_config, dict2config, configure2str\n",
    "from xautodl.datasets import get_datasets, get_nas_search_loaders\n",
    "from xautodl.procedures import (\n",
    "    prepare_seed,\n",
    "    prepare_logger,\n",
    "    save_checkpoint,\n",
    "    copy_checkpoint,\n",
    "    get_optim_scheduler,\n",
    ")\n",
    "from xautodl.utils import get_model_infos, obtain_accuracy\n",
    "from xautodl.log_utils import AverageMeter, time_string, convert_secs2time\n",
    "from xautodl.models import get_cell_based_tiny_net, get_search_spaces\n",
    "\n",
    "# NB201\n",
    "from nas_201_api import NASBench201API as API\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "392dc080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68228\n",
      "Namespace(arch_nas_dataset='../NAS-Bench-201-v1_1-096897.pth', channel=16, config_path='../configs/nas-benchmark/algos/RANDOM.config', data_path='../cifar.python', dataset='cifar10', max_nodes=4, num_cells=5, print_freq=200, rand_seed=68228, save_dir='../results/three_metrics/test', search_space_name='nas-bench-201', select_num=100, track_running_stats=0, workers=4)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(\"Random search for NAS.\")\n",
    "parser.add_argument(\"--data_path\", type=str, default='../cifar.python', help=\"The path to dataset\")\n",
    "parser.add_argument(\"--dataset\", type=str, default='cifar10',choices=[\"cifar10\", \"cifar100\", \"ImageNet16-120\"], help=\"Choose between Cifar10/100 and ImageNet-16.\")\n",
    "\n",
    "# channels and number-of-cells\n",
    "parser.add_argument(\"--search_space_name\", type=str, default='nas-bench-201', help=\"The search space name.\")\n",
    "parser.add_argument(\"--config_path\", type=str, default='../configs/nas-benchmark/algos/RANDOM.config', help=\"The path to the configuration.\")\n",
    "# parser.add_argument(\"--config_path\", type=str, default='./MY.config', help=\"The path to the configuration.\")\n",
    "parser.add_argument(\"--max_nodes\", type=int, default=4, help=\"The maximum number of nodes.\")\n",
    "parser.add_argument(\"--channel\", type=int, default=16, help=\"The number of channels.\")\n",
    "parser.add_argument(\"--num_cells\", type=int, default=5, help=\"The number of cells in one stage.\")\n",
    "parser.add_argument(\"--select_num\", type=int, default=100, help=\"The number of selected architectures to evaluate.\")\n",
    "parser.add_argument(\"--track_running_stats\", type=int, default=0, choices=[0, 1], help=\"Whether use track_running_stats or not in the BN layer.\")\n",
    "# log\n",
    "parser.add_argument(\"--workers\", type=int, default=4, help=\"number of data loading workers\")\n",
    "parser.add_argument(\"--save_dir\", type=str, default='../results/three_metrics/test', help=\"Folder to save checkpoints and log.\")\n",
    "parser.add_argument(\"--arch_nas_dataset\", type=str, default='../NAS-Bench-201-v1_1-096897.pth', help=\"The path to load the architecture dataset (tiny-nas-benchmark).\")\n",
    "parser.add_argument(\"--print_freq\", type=int, default=200, help=\"print frequency (default: 200)\")\n",
    "parser.add_argument(\"--rand_seed\", type=int, default=None, help=\"manual seed\")\n",
    "args = parser.parse_args(args=[])\n",
    "if args.rand_seed is None or args.rand_seed < 0:\n",
    "    args.rand_seed = random.randint(1, 100000)\n",
    "\n",
    "print(args.rand_seed)\n",
    "print(args)\n",
    "xargs=args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1f83abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Function with logger : Logger(dir=../results/three_metrics/test, use-tf=False, writer=None)\n",
      "Arguments : -------------------------------\n",
      "arch_nas_dataset : ../NAS-Bench-201-v1_1-096897.pth\n",
      "channel          : 16\n",
      "config_path      : ../configs/nas-benchmark/algos/RANDOM.config\n",
      "data_path        : ../cifar.python\n",
      "dataset          : cifar10\n",
      "max_nodes        : 4\n",
      "num_cells        : 5\n",
      "print_freq       : 200\n",
      "rand_seed        : 68228\n",
      "save_dir         : ../results/three_metrics/test\n",
      "search_space_name : nas-bench-201\n",
      "select_num       : 100\n",
      "track_running_stats : 0\n",
      "workers          : 4\n",
      "Python  Version  : 3.7.13 (default, Mar 29 2022, 02:18:16)  [GCC 7.5.0]\n",
      "Pillow  Version  : 9.0.1\n",
      "PyTorch Version  : 1.12.0\n",
      "cuDNN   Version  : 8302\n",
      "CUDA available   : True\n",
      "CUDA GPU numbers : 2\n",
      "CUDA_VISIBLE_DEVICES : None\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available(), \"CUDA is not available.\"\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.set_num_threads(xargs.workers)\n",
    "prepare_seed(xargs.rand_seed)\n",
    "logger = prepare_logger(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d1647d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_func(xloader, network, criterion, scheduler, w_optimizer, epoch_str, print_freq, logger):\n",
    "    data_time, batch_time = AverageMeter(), AverageMeter()\n",
    "    base_losses, base_top1, base_top5 = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    network.train()\n",
    "    end = time.time()\n",
    "    for step, (base_inputs, base_targets, arch_inputs, arch_targets) in enumerate(\n",
    "        xloader\n",
    "    ):\n",
    "        scheduler.update(None, 1.0 * step / len(xloader))\n",
    "        base_targets = base_targets.cuda(non_blocking=True)\n",
    "        arch_targets = arch_targets.cuda(non_blocking=True)\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # update the weights\n",
    "        network.module.random_genotype(True)\n",
    "        w_optimizer.zero_grad()\n",
    "        _, logits = network(base_inputs)\n",
    "        base_loss = criterion(logits, base_targets)\n",
    "        base_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(network.parameters(), 5)\n",
    "        w_optimizer.step()\n",
    "        # record\n",
    "        base_prec1, base_prec5 = obtain_accuracy(\n",
    "            logits.data, base_targets.data, topk=(1, 5)\n",
    "        )\n",
    "        base_losses.update(base_loss.item(), base_inputs.size(0))\n",
    "        base_top1.update(base_prec1.item(), base_inputs.size(0))\n",
    "        base_top5.update(base_prec5.item(), base_inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if step % print_freq == 0 or step + 1 == len(xloader):\n",
    "            Sstr = (\n",
    "                \"*SEARCH* \"\n",
    "                + time_string()\n",
    "                + \" [{:}][{:03d}/{:03d}]\".format(epoch_str, step, len(xloader))\n",
    "            )\n",
    "            Tstr = \"Time {batch_time.val:.2f} ({batch_time.avg:.2f}) Data {data_time.val:.2f} ({data_time.avg:.2f})\".format(\n",
    "                batch_time=batch_time, data_time=data_time\n",
    "            )\n",
    "            Wstr = \"Base [Loss {loss.val:.3f} ({loss.avg:.3f})  Prec@1 {top1.val:.2f} ({top1.avg:.2f}) Prec@5 {top5.val:.2f} ({top5.avg:.2f})]\".format(\n",
    "                loss=base_losses, top1=base_top1, top5=base_top5\n",
    "            )\n",
    "            logger.log(Sstr + \" \" + Tstr + \" \" + Wstr)\n",
    "    return base_losses.avg, base_top1.avg, base_top5.avg\n",
    "\n",
    "\n",
    "def valid_func(xloader, network, criterion):\n",
    "    data_time, batch_time = AverageMeter(), AverageMeter()\n",
    "    arch_losses, arch_top1, arch_top5 = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    network.eval()\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for step, (arch_inputs, arch_targets) in enumerate(xloader):\n",
    "            arch_targets = arch_targets.cuda(non_blocking=True)\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "            # prediction\n",
    "\n",
    "            network.module.random_genotype(True)\n",
    "            _, logits = network(arch_inputs)\n",
    "            arch_loss = criterion(logits, arch_targets)\n",
    "            # record\n",
    "            arch_prec1, arch_prec5 = obtain_accuracy(\n",
    "                logits.data, arch_targets.data, topk=(1, 5)\n",
    "            )\n",
    "            arch_losses.update(arch_loss.item(), arch_inputs.size(0))\n",
    "            arch_top1.update(arch_prec1.item(), arch_inputs.size(0))\n",
    "            arch_top5.update(arch_prec5.item(), arch_inputs.size(0))\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "    return arch_losses.avg, arch_top1.avg, arch_top5.avg\n",
    "\n",
    "\n",
    "def search_find_best(xloader, network, n_samples):\n",
    "    with torch.no_grad():\n",
    "        network.eval()\n",
    "        archs, valid_accs = [], []\n",
    "        # print ('obtain the top-{:} architectures'.format(n_samples))\n",
    "        loader_iter = iter(xloader)\n",
    "        for i in range(n_samples):\n",
    "            arch = network.module.random_genotype(True)\n",
    "            try:\n",
    "                inputs, targets = next(loader_iter)\n",
    "            except:\n",
    "                loader_iter = iter(xloader)\n",
    "                inputs, targets = next(loader_iter)\n",
    "\n",
    "            _, logits = network(inputs)\n",
    "            val_top1, val_top5 = obtain_accuracy(\n",
    "                logits.cpu().data, targets.data, topk=(1, 5)\n",
    "            )\n",
    "\n",
    "            archs.append(arch)\n",
    "            valid_accs.append(val_top1.item())\n",
    "\n",
    "        best_idx = np.argmax(valid_accs)\n",
    "        best_arch, best_valid_acc = archs[best_idx], valid_accs[best_idx]\n",
    "        return best_arch, best_valid_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ad28c7",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca428b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "../configs/nas-benchmark/algos/RANDOM.config\n",
      "Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=250, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, test_batch_size=512, class_num=10, xshape=(1, 3, 32, 32))\n",
      "||||||| cifar10    ||||||| Search-Loader-Num=391, Valid-Loader-Num=49, batch size=64\n",
      "||||||| cifar10    ||||||| Config=Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=250, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, test_batch_size=512, class_num=10, xshape=(1, 3, 32, 32))\n",
      "w-optimizer : SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    initial_lr: 0.025\n",
      "    lr: 0.025\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: True\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "w-scheduler : CosineAnnealingLR(warmup=0, max-epoch=250, current::epoch=0, iter=0.00, type=cosine, T-max=250, eta-min=0.001)\n",
      "criterion   : CrossEntropyLoss()\n",
      "try to create the NAS-Bench-201 api from ../NAS-Bench-201-v1_1-096897.pth\n",
      "[2022-10-12 14:31:17] create API = NASBench201API(15625/15625 architectures, file=NAS-Bench-201-v1_1-096897.pth) done\n",
      "=> loading checkpoint of the last-info '../results/three_metrics/test/seed-68228-last-info.pth' start\n",
      "=> loading checkpoint of the last-info '{'epoch': 241, 'args': Namespace(arch_nas_dataset='../NAS-Bench-201-v1_1-096897.pth', channel=16, config_path='../configs/nas-benchmark/algos/RANDOM.config', data_path='../cifar.python', dataset='cifar10', max_nodes=4, num_cells=5, print_freq=200, rand_seed=68228, save_dir='../results/three_metrics/test', search_space_name='nas-bench-201', select_num=100, track_running_stats=0, workers=4), 'last_checkpoint': PosixPath('../results/three_metrics/test/checkpoint/seed-68228-basic.pth')}' start with 241-th epoch.\n",
      "\n",
      "[Search the 241-250-th epoch] Time Left: [00:00:00], LR=0.0010766642737598993\n",
      "*SEARCH* [2022-10-12 14:31:23] [241-250][000/391] Time 4.95 (4.95) Data 0.71 (0.71) Base [Loss 0.938 (0.938)  Prec@1 65.62 (65.62) Prec@5 98.44 (98.44)]\n",
      "*SEARCH* [2022-10-12 14:31:55] [241-250][200/391] Time 0.13 (0.18) Data 0.00 (0.00) Base [Loss 0.888 (1.054)  Prec@1 64.06 (62.86) Prec@5 98.44 (94.67)]\n",
      "*SEARCH* [2022-10-12 14:32:18] [241-250][390/391] Time 0.07 (0.15) Data 0.00 (0.00) Base [Loss 0.822 (1.043)  Prec@1 70.00 (63.09) Prec@5 95.00 (94.87)]\n",
      "[241-250] searching : loss=1.04, accuracy@1=63.09%, accuracy@5=94.87%, time-cost=59.6 s\n",
      "[241-250] evaluate  : loss=1.63, accuracy@1=44.02%, accuracy@5=87.58%\n",
      "[241-250] find-the-best : Structure(4 nodes with |nor_conv_3x3~0|+|skip_connect~0|nor_conv_3x3~1|+|nor_conv_3x3~0|nor_conv_3x3~1|nor_conv_1x1~2|), accuracy@1=63.09%\n",
      "Find ../results/three_metrics/test/checkpoint/seed-68228-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into ../results/three_metrics/test/checkpoint/seed-68228-basic.pth\n",
      "Find ../results/three_metrics/test/seed-68228-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into ../results/three_metrics/test/seed-68228-last-info.pth\n",
      "Call query_info_str_by_arch with arch=Structure(4 nodes with |nor_conv_3x3~0|+|skip_connect~0|nor_conv_3x3~1|+|nor_conv_3x3~0|nor_conv_3x3~1|nor_conv_1x1~2|) and hp=200\n",
      "Call query_index_by_arch with arch=Structure(4 nodes with |nor_conv_3x3~0|+|skip_connect~0|nor_conv_3x3~1|+|nor_conv_3x3~0|nor_conv_3x3~1|nor_conv_1x1~2|)\n",
      "|nor_conv_3x3~0|+|skip_connect~0|nor_conv_3x3~1|+|nor_conv_3x3~0|nor_conv_3x3~1|nor_conv_1x1~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=571\n",
      "cifar10-valid  FLOP=153.27 M, Params=1.073 MB, latency=20.88 ms.\n",
      "cifar10-valid  train : [loss = 0.002, top1 = 99.99%], valid : [loss = 0.493, top1 = 90.14%]\n",
      "cifar10        FLOP=153.27 M, Params=1.073 MB, latency=20.88 ms.\n",
      "cifar10        train : [loss = 0.003, top1 = 99.97%], test  : [loss = 0.299, top1 = 93.69%]\n",
      "cifar100       FLOP=153.28 M, Params=1.079 MB, latency=19.39 ms.\n",
      "cifar100       train : [loss = 0.042, top1 = 99.54%], valid : [loss = 1.316, top1 = 70.87%], test : [loss = 1.320, top1 = 70.95%]\n",
      "ImageNet16-120 FLOP= 38.33 M, Params=1.081 MB, latency=19.26 ms.\n",
      "ImageNet16-120 train : [loss = 1.547, top1 = 58.21%], valid : [loss = 2.186, top1 = 44.64%], test : [loss = 2.152, top1 = 45.33%]\n",
      "\n",
      "[Search the 242-250-th epoch] Time Left: [00:10:37], LR=0.001060587796224398\n",
      "*SEARCH* [2022-10-12 14:32:40] [242-250][000/391] Time 1.93 (1.93) Data 0.67 (0.67) Base [Loss 1.242 (1.242)  Prec@1 57.81 (57.81) Prec@5 95.31 (95.31)]\n",
      "*SEARCH* [2022-10-12 14:33:03] [242-250][200/391] Time 0.07 (0.12) Data 0.00 (0.00) Base [Loss 1.409 (1.016)  Prec@1 48.44 (64.26) Prec@5 92.19 (95.70)]\n",
      "*SEARCH* [2022-10-12 14:33:22] [242-250][390/391] Time 0.07 (0.11) Data 0.00 (0.00) Base [Loss 1.305 (1.009)  Prec@1 60.00 (64.50) Prec@5 90.00 (95.61)]\n",
      "[242-250] searching : loss=1.01, accuracy@1=64.50%, accuracy@5=95.61%, time-cost=104.0 s\n",
      "[242-250] evaluate  : loss=1.66, accuracy@1=42.34%, accuracy@5=87.29%\n",
      "[242-250] find-the-best : Structure(4 nodes with |avg_pool_3x3~0|+|skip_connect~0|nor_conv_1x1~1|+|nor_conv_3x3~0|skip_connect~1|nor_conv_1x1~2|), accuracy@1=57.81%\n",
      "Find ../results/three_metrics/test/checkpoint/seed-68228-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into ../results/three_metrics/test/checkpoint/seed-68228-basic.pth\n",
      "Find ../results/three_metrics/test/seed-68228-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into ../results/three_metrics/test/seed-68228-last-info.pth\n",
      "Call query_info_str_by_arch with arch=Structure(4 nodes with |avg_pool_3x3~0|+|skip_connect~0|nor_conv_1x1~1|+|nor_conv_3x3~0|skip_connect~1|nor_conv_1x1~2|) and hp=200\n",
      "Call query_index_by_arch with arch=Structure(4 nodes with |avg_pool_3x3~0|+|skip_connect~0|nor_conv_1x1~1|+|nor_conv_3x3~0|skip_connect~1|nor_conv_1x1~2|)\n",
      "|avg_pool_3x3~0|+|skip_connect~0|nor_conv_1x1~1|+|nor_conv_3x3~0|skip_connect~1|nor_conv_1x1~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=1102\n",
      "cifar10-valid  FLOP= 51.04 M, Params=0.372 MB, latency=16.86 ms.\n",
      "cifar10-valid  train : [loss = 0.005, top1 = 99.97%], valid : [loss = 0.537, top1 = 88.22%]\n",
      "cifar10        FLOP= 51.04 M, Params=0.372 MB, latency=16.86 ms.\n",
      "cifar10        train : [loss = 0.008, top1 = 99.86%], test  : [loss = 0.374, top1 = 91.55%]\n",
      "cifar100       FLOP= 51.04 M, Params=0.378 MB, latency=17.26 ms.\n",
      "cifar100       train : [loss = 0.205, top1 = 95.09%], valid : [loss = 1.506, top1 = 65.47%], test : [loss = 1.452, top1 = 66.36%]\n",
      "ImageNet16-120 FLOP= 12.77 M, Params=0.379 MB, latency=16.46 ms.\n",
      "ImageNet16-120 train : [loss = 2.078, top1 = 45.92%], valid : [loss = 2.408, top1 = 38.98%], test : [loss = 2.423, top1 = 39.62%]\n",
      "\n",
      "[Search the 243-250-th epoch] Time Left: [00:07:40], LR=0.00104639669028193\n",
      "*SEARCH* [2022-10-12 14:33:45] [243-250][000/391] Time 0.82 (0.82) Data 0.73 (0.73) Base [Loss 1.235 (1.235)  Prec@1 57.81 (57.81) Prec@5 96.88 (96.88)]\n",
      "*SEARCH* [2022-10-12 14:34:08] [243-250][200/391] Time 0.12 (0.12) Data 0.00 (0.00) Base [Loss 1.097 (0.958)  Prec@1 68.75 (66.49) Prec@5 96.88 (96.23)]\n",
      "*SEARCH* [2022-10-12 14:34:37] [243-250][390/391] Time 0.12 (0.14) Data 0.00 (0.00) Base [Loss 1.339 (0.984)  Prec@1 57.50 (65.64) Prec@5 92.50 (95.94)]\n",
      "[243-250] searching : loss=0.98, accuracy@1=65.64%, accuracy@5=95.94%, time-cost=157.9 s\n",
      "[243-250] evaluate  : loss=1.67, accuracy@1=41.86%, accuracy@5=87.19%\n",
      "[243-250] find-the-best : Structure(4 nodes with |nor_conv_3x3~0|+|skip_connect~0|skip_connect~1|+|avg_pool_3x3~0|nor_conv_1x1~1|nor_conv_3x3~2|), accuracy@1=56.84%\n",
      "Find ../results/three_metrics/test/checkpoint/seed-68228-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into ../results/three_metrics/test/checkpoint/seed-68228-basic.pth\n",
      "Find ../results/three_metrics/test/seed-68228-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into ../results/three_metrics/test/seed-68228-last-info.pth\n",
      "Call query_info_str_by_arch with arch=Structure(4 nodes with |nor_conv_3x3~0|+|skip_connect~0|skip_connect~1|+|avg_pool_3x3~0|nor_conv_1x1~1|nor_conv_3x3~2|) and hp=200\n",
      "Call query_index_by_arch with arch=Structure(4 nodes with |nor_conv_3x3~0|+|skip_connect~0|skip_connect~1|+|avg_pool_3x3~0|nor_conv_1x1~1|nor_conv_3x3~2|)\n",
      "|nor_conv_3x3~0|+|skip_connect~0|skip_connect~1|+|avg_pool_3x3~0|nor_conv_1x1~1|nor_conv_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=4951\n",
      "cifar10-valid  FLOP= 82.49 M, Params=0.587 MB, latency=17.47 ms.\n",
      "cifar10-valid  train : [loss = 0.002, top1 = 99.98%], valid : [loss = 0.528, top1 = 89.32%]\n",
      "cifar10        FLOP= 82.49 M, Params=0.587 MB, latency=17.47 ms.\n",
      "cifar10        train : [loss = 0.004, top1 = 99.95%], test  : [loss = 0.339, top1 = 92.73%]\n",
      "cifar100       FLOP= 82.50 M, Params=0.593 MB, latency=17.61 ms.\n",
      "cifar100       train : [loss = 0.086, top1 = 98.63%], valid : [loss = 1.478, top1 = 67.81%], test : [loss = 1.433, top1 = 68.55%]\n",
      "ImageNet16-120 FLOP= 20.63 M, Params=0.595 MB, latency=19.66 ms.\n",
      "ImageNet16-120 train : [loss = 1.800, top1 = 52.27%], valid : [loss = 2.417, top1 = 41.15%], test : [loss = 2.410, top1 = 41.40%]\n",
      "\n",
      "[Search the 244-250-th epoch] Time Left: [00:07:17], LR=0.0010340931968726328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*SEARCH* [2022-10-12 14:34:57] [244-250][000/391] Time 0.76 (0.76) Data 0.60 (0.60) Base [Loss 0.682 (0.682)  Prec@1 71.88 (71.88) Prec@5 98.44 (98.44)]\n",
      "*SEARCH* [2022-10-12 14:35:29] [244-250][200/391] Time 0.13 (0.16) Data 0.00 (0.00) Base [Loss 0.809 (0.966)  Prec@1 73.44 (66.16) Prec@5 98.44 (96.34)]\n",
      "*SEARCH* [2022-10-12 14:35:52] [244-250][390/391] Time 0.08 (0.14) Data 0.00 (0.00) Base [Loss 0.565 (0.987)  Prec@1 87.50 (65.47) Prec@5 97.50 (96.18)]\n",
      "[244-250] searching : loss=0.99, accuracy@1=65.47%, accuracy@5=96.18%, time-cost=213.7 s\n",
      "[244-250] evaluate  : loss=1.67, accuracy@1=41.16%, accuracy@5=86.72%\n",
      "[244-250] find-the-best : Structure(4 nodes with |nor_conv_3x3~0|+|avg_pool_3x3~0|nor_conv_3x3~1|+|nor_conv_3x3~0|avg_pool_3x3~1|skip_connect~2|), accuracy@1=55.86%\n",
      "Find ../results/three_metrics/test/checkpoint/seed-68228-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into ../results/three_metrics/test/checkpoint/seed-68228-basic.pth\n",
      "Find ../results/three_metrics/test/seed-68228-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into ../results/three_metrics/test/seed-68228-last-info.pth\n",
      "Call query_info_str_by_arch with arch=Structure(4 nodes with |nor_conv_3x3~0|+|avg_pool_3x3~0|nor_conv_3x3~1|+|nor_conv_3x3~0|avg_pool_3x3~1|skip_connect~2|) and hp=200\n",
      "Call query_index_by_arch with arch=Structure(4 nodes with |nor_conv_3x3~0|+|avg_pool_3x3~0|nor_conv_3x3~1|+|nor_conv_3x3~0|avg_pool_3x3~1|skip_connect~2|)\n",
      "|nor_conv_3x3~0|+|avg_pool_3x3~0|nor_conv_3x3~1|+|nor_conv_3x3~0|avg_pool_3x3~1|skip_connect~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=12888\n",
      "cifar10-valid  FLOP=113.95 M, Params=0.802 MB, latency=17.03 ms.\n",
      "cifar10-valid  train : [loss = 0.003, top1 = 99.96%], valid : [loss = 0.560, top1 = 89.01%]\n",
      "cifar10        FLOP=113.95 M, Params=0.802 MB, latency=17.03 ms.\n",
      "cifar10        train : [loss = 0.003, top1 = 99.94%], test  : [loss = 0.395, top1 = 92.23%]\n",
      "cifar100       FLOP=113.96 M, Params=0.808 MB, latency=17.09 ms.\n",
      "cifar100       train : [loss = 0.073, top1 = 98.88%], valid : [loss = 1.453, top1 = 68.65%], test : [loss = 1.460, top1 = 68.74%]\n",
      "ImageNet16-120 FLOP= 28.50 M, Params=0.810 MB, latency=15.58 ms.\n",
      "ImageNet16-120 train : [loss = 1.803, top1 = 52.21%], valid : [loss = 2.380, top1 = 41.00%], test : [loss = 2.396, top1 = 40.48%]\n",
      "\n",
      "[Search the 245-250-th epoch] Time Left: [00:06:15], LR=0.0010236792588607412\n",
      "*SEARCH* [2022-10-12 14:36:13] [245-250][000/391] Time 0.72 (0.72) Data 0.62 (0.62) Base [Loss 0.973 (0.973)  Prec@1 60.94 (60.94) Prec@5 100.00 (100.00)]\n",
      "*SEARCH* [2022-10-12 14:36:33] [245-250][200/391] Time 0.09 (0.11) Data 0.00 (0.00) Base [Loss 0.671 (0.974)  Prec@1 71.88 (65.87) Prec@5 100.00 (96.04)]\n",
      "*SEARCH* [2022-10-12 14:36:52] [245-250][390/391] Time 0.07 (0.10) Data 0.00 (0.00) Base [Loss 1.116 (0.986)  Prec@1 62.50 (65.53) Prec@5 100.00 (95.70)]\n",
      "[245-250] searching : loss=0.99, accuracy@1=65.53%, accuracy@5=95.70%, time-cost=254.5 s\n",
      "[245-250] evaluate  : loss=1.45, accuracy@1=50.08%, accuracy@5=91.12%\n",
      "[245-250] find-the-best : Structure(4 nodes with |nor_conv_3x3~0|+|skip_connect~0|nor_conv_1x1~1|+|skip_connect~0|avg_pool_3x3~1|nor_conv_1x1~2|), accuracy@1=63.48%\n",
      "Find ../results/three_metrics/test/checkpoint/seed-68228-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into ../results/three_metrics/test/checkpoint/seed-68228-basic.pth\n",
      "Find ../results/three_metrics/test/seed-68228-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into ../results/three_metrics/test/seed-68228-last-info.pth\n",
      "<<<--->>> The 245-250-th epoch : find the highest validation accuracy : 50.08%.\n",
      "Find ../results/three_metrics/test/checkpoint/seed-68228-best.pth exist, delete is at first before saving\n",
      "copy the file from ../results/three_metrics/test/checkpoint/seed-68228-basic.pth into ../results/three_metrics/test/checkpoint/seed-68228-best.pth\n",
      "Call query_info_str_by_arch with arch=Structure(4 nodes with |nor_conv_3x3~0|+|skip_connect~0|nor_conv_1x1~1|+|skip_connect~0|avg_pool_3x3~1|nor_conv_1x1~2|) and hp=200\n",
      "Call query_index_by_arch with arch=Structure(4 nodes with |nor_conv_3x3~0|+|skip_connect~0|nor_conv_1x1~1|+|skip_connect~0|avg_pool_3x3~1|nor_conv_1x1~2|)\n",
      "|nor_conv_3x3~0|+|skip_connect~0|nor_conv_1x1~1|+|skip_connect~0|avg_pool_3x3~1|nor_conv_1x1~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=4123\n",
      "cifar10-valid  FLOP= 51.04 M, Params=0.372 MB, latency=17.62 ms.\n",
      "cifar10-valid  train : [loss = 0.004, top1 = 99.97%], valid : [loss = 0.466, top1 = 89.44%]\n",
      "cifar10        FLOP= 51.04 M, Params=0.372 MB, latency=17.62 ms.\n",
      "cifar10        train : [loss = 0.009, top1 = 99.86%], test  : [loss = 0.312, top1 = 92.70%]\n",
      "cifar100       FLOP= 51.04 M, Params=0.378 MB, latency=16.45 ms.\n",
      "cifar100       train : [loss = 0.176, top1 = 95.83%], valid : [loss = 1.309, top1 = 68.88%], test : [loss = 1.303, top1 = 68.88%]\n",
      "ImageNet16-120 FLOP= 12.77 M, Params=0.379 MB, latency=13.86 ms.\n",
      "ImageNet16-120 train : [loss = 1.799, top1 = 52.18%], valid : [loss = 2.228, top1 = 43.37%], test : [loss = 2.279, top1 = 42.07%]\n",
      "\n",
      "[Search the 246-250-th epoch] Time Left: [00:03:57], LR=0.0010151565207277904\n",
      "*SEARCH* [2022-10-12 14:37:12] [246-250][000/391] Time 0.73 (0.73) Data 0.64 (0.64) Base [Loss 0.765 (0.765)  Prec@1 75.00 (75.00) Prec@5 100.00 (100.00)]\n",
      "*SEARCH* [2022-10-12 14:37:33] [246-250][200/391] Time 0.10 (0.11) Data 0.00 (0.00) Base [Loss 1.071 (0.978)  Prec@1 70.31 (65.53) Prec@5 95.31 (96.19)]\n",
      "*SEARCH* [2022-10-12 14:37:55] [246-250][390/391] Time 0.09 (0.11) Data 0.00 (0.00) Base [Loss 0.796 (0.988)  Prec@1 75.00 (65.28) Prec@5 97.50 (95.95)]\n",
      "[246-250] searching : loss=0.99, accuracy@1=65.28%, accuracy@5=95.95%, time-cost=298.2 s\n",
      "[246-250] evaluate  : loss=1.60, accuracy@1=44.74%, accuracy@5=89.35%\n",
      "[246-250] find-the-best : Structure(4 nodes with |nor_conv_3x3~0|+|avg_pool_3x3~0|nor_conv_3x3~1|+|skip_connect~0|nor_conv_1x1~1|nor_conv_1x1~2|), accuracy@1=63.87%\n",
      "Find ../results/three_metrics/test/checkpoint/seed-68228-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into ../results/three_metrics/test/checkpoint/seed-68228-basic.pth\n",
      "Find ../results/three_metrics/test/seed-68228-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into ../results/three_metrics/test/seed-68228-last-info.pth\n",
      "Call query_info_str_by_arch with arch=Structure(4 nodes with |nor_conv_3x3~0|+|avg_pool_3x3~0|nor_conv_3x3~1|+|skip_connect~0|nor_conv_1x1~1|nor_conv_1x1~2|) and hp=200\n",
      "Call query_index_by_arch with arch=Structure(4 nodes with |nor_conv_3x3~0|+|avg_pool_3x3~0|nor_conv_3x3~1|+|skip_connect~0|nor_conv_1x1~1|nor_conv_1x1~2|)\n",
      "|nor_conv_3x3~0|+|avg_pool_3x3~0|nor_conv_3x3~1|+|skip_connect~0|nor_conv_1x1~1|nor_conv_1x1~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=5711\n",
      "cifar10-valid  FLOP= 86.43 M, Params=0.615 MB, latency=20.28 ms.\n",
      "cifar10-valid  train : [loss = 0.002, top1 = 99.98%], valid : [loss = 0.448, top1 = 90.70%]\n",
      "cifar10        FLOP= 86.43 M, Params=0.615 MB, latency=20.28 ms.\n",
      "cifar10        train : [loss = 0.003, top1 = 99.95%], test  : [loss = 0.298, top1 = 93.78%]\n",
      "cifar100       FLOP= 86.43 M, Params=0.621 MB, latency=20.94 ms.\n",
      "cifar100       train : [loss = 0.046, top1 = 99.35%], valid : [loss = 1.383, top1 = 70.65%], test : [loss = 1.357, top1 = 70.97%]\n",
      "ImageNet16-120 FLOP= 21.61 M, Params=0.623 MB, latency=16.79 ms.\n",
      "ImageNet16-120 train : [loss = 1.442, top1 = 60.84%], valid : [loss = 2.212, top1 = 45.10%], test : [loss = 2.221, top1 = 45.63%]\n",
      "\n",
      "[Search the 247-250-th epoch] Time Left: [00:03:08], LR=0.0010085263283129296\n",
      "*SEARCH* [2022-10-12 14:38:15] [247-250][000/391] Time 0.74 (0.74) Data 0.62 (0.62) Base [Loss 0.925 (0.925)  Prec@1 65.62 (65.62) Prec@5 100.00 (100.00)]\n",
      "*SEARCH* [2022-10-12 14:38:38] [247-250][200/391] Time 0.09 (0.12) Data 0.00 (0.00) Base [Loss 0.718 (0.971)  Prec@1 79.69 (66.03) Prec@5 93.75 (96.24)]\n",
      "*SEARCH* [2022-10-12 14:39:01] [247-250][390/391] Time 0.13 (0.12) Data 0.00 (0.00) Base [Loss 1.512 (0.990)  Prec@1 45.00 (65.28) Prec@5 100.00 (95.90)]\n",
      "[247-250] searching : loss=0.99, accuracy@1=65.28%, accuracy@5=95.90%, time-cost=345.6 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[247-250] evaluate  : loss=1.69, accuracy@1=42.42%, accuracy@5=87.42%\n",
      "[247-250] find-the-best : Structure(4 nodes with |nor_conv_3x3~0|+|nor_conv_3x3~0|nor_conv_1x1~1|+|skip_connect~0|avg_pool_3x3~1|avg_pool_3x3~2|), accuracy@1=59.18%\n",
      "Find ../results/three_metrics/test/checkpoint/seed-68228-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into ../results/three_metrics/test/checkpoint/seed-68228-basic.pth\n",
      "Find ../results/three_metrics/test/seed-68228-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into ../results/three_metrics/test/seed-68228-last-info.pth\n",
      "Call query_info_str_by_arch with arch=Structure(4 nodes with |nor_conv_3x3~0|+|nor_conv_3x3~0|nor_conv_1x1~1|+|skip_connect~0|avg_pool_3x3~1|avg_pool_3x3~2|) and hp=200\n",
      "Call query_index_by_arch with arch=Structure(4 nodes with |nor_conv_3x3~0|+|nor_conv_3x3~0|nor_conv_1x1~1|+|skip_connect~0|avg_pool_3x3~1|avg_pool_3x3~2|)\n",
      "|nor_conv_3x3~0|+|nor_conv_3x3~0|nor_conv_1x1~1|+|skip_connect~0|avg_pool_3x3~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=7452\n",
      "cifar10-valid  FLOP= 82.49 M, Params=0.587 MB, latency=17.23 ms.\n",
      "cifar10-valid  train : [loss = 0.003, top1 = 99.98%], valid : [loss = 0.483, top1 = 89.76%]\n",
      "cifar10        FLOP= 82.49 M, Params=0.587 MB, latency=17.23 ms.\n",
      "cifar10        train : [loss = 0.004, top1 = 99.93%], test  : [loss = 0.328, top1 = 92.79%]\n",
      "cifar100       FLOP= 82.50 M, Params=0.593 MB, latency=16.51 ms.\n",
      "cifar100       train : [loss = 0.106, top1 = 97.93%], valid : [loss = 1.428, top1 = 68.39%], test : [loss = 1.419, top1 = 68.47%]\n",
      "ImageNet16-120 FLOP= 20.63 M, Params=0.595 MB, latency=17.26 ms.\n",
      "ImageNet16-120 train : [loss = 1.772, top1 = 52.79%], valid : [loss = 2.366, top1 = 41.06%], test : [loss = 2.403, top1 = 40.70%]\n",
      "\n",
      "[Search the 248-250-th epoch] Time Left: [00:02:17], LR=0.0010037897286004007\n",
      "*SEARCH* [2022-10-12 14:39:24] [248-250][000/391] Time 0.71 (0.71) Data 0.60 (0.60) Base [Loss 0.936 (0.936)  Prec@1 73.44 (73.44) Prec@5 93.75 (93.75)]\n",
      "*SEARCH* [2022-10-12 14:39:47] [248-250][200/391] Time 0.09 (0.12) Data 0.00 (0.00) Base [Loss 1.208 (0.976)  Prec@1 54.69 (66.32) Prec@5 95.31 (96.54)]\n",
      "*SEARCH* [2022-10-12 14:40:09] [248-250][390/391] Time 0.09 (0.12) Data 0.00 (0.00) Base [Loss 1.023 (0.989)  Prec@1 67.50 (65.70) Prec@5 95.00 (96.07)]\n",
      "[248-250] searching : loss=0.99, accuracy@1=65.70%, accuracy@5=96.07%, time-cost=391.7 s\n",
      "[248-250] evaluate  : loss=1.49, accuracy@1=48.18%, accuracy@5=87.87%\n",
      "[248-250] find-the-best : Structure(4 nodes with |nor_conv_1x1~0|+|none~0|none~1|+|skip_connect~0|avg_pool_3x3~1|nor_conv_1x1~2|), accuracy@1=66.99%\n",
      "Find ../results/three_metrics/test/checkpoint/seed-68228-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into ../results/three_metrics/test/checkpoint/seed-68228-basic.pth\n",
      "Find ../results/three_metrics/test/seed-68228-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into ../results/three_metrics/test/seed-68228-last-info.pth\n",
      "Call query_info_str_by_arch with arch=Structure(4 nodes with |nor_conv_1x1~0|+|none~0|none~1|+|skip_connect~0|avg_pool_3x3~1|nor_conv_1x1~2|) and hp=200\n",
      "Call query_index_by_arch with arch=Structure(4 nodes with |nor_conv_1x1~0|+|none~0|none~1|+|skip_connect~0|avg_pool_3x3~1|nor_conv_1x1~2|)\n",
      "|nor_conv_1x1~0|+|none~0|none~1|+|skip_connect~0|avg_pool_3x3~1|nor_conv_1x1~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=10614\n",
      "cifar10-valid  FLOP= 15.65 M, Params=0.129 MB, latency=14.74 ms.\n",
      "cifar10-valid  train : [loss = 0.040, top1 = 98.97%], valid : [loss = 0.543, top1 = 86.79%]\n",
      "cifar10        FLOP= 15.65 M, Params=0.129 MB, latency=14.74 ms.\n",
      "cifar10        train : [loss = 0.075, top1 = 97.64%], test  : [loss = 0.368, top1 = 89.95%]\n",
      "cifar100       FLOP= 15.65 M, Params=0.135 MB, latency=14.00 ms.\n",
      "cifar100       train : [loss = 0.719, top1 = 78.88%], valid : [loss = 1.346, top1 = 64.34%], test : [loss = 1.340, top1 = 64.06%]\n",
      "ImageNet16-120 FLOP=  3.92 M, Params=0.136 MB, latency=12.90 ms.\n",
      "ImageNet16-120 train : [loss = 2.519, top1 = 36.32%], valid : [loss = 2.629, top1 = 33.68%], test : [loss = 2.672, top1 = 33.05%]\n",
      "\n",
      "[Search the 249-250-th epoch] Time Left: [00:01:06], LR=0.0010009474695542066\n",
      "*SEARCH* [2022-10-12 14:40:30] [249-250][000/391] Time 0.72 (0.72) Data 0.61 (0.61) Base [Loss 1.120 (1.120)  Prec@1 64.06 (64.06) Prec@5 96.88 (96.88)]\n",
      "*SEARCH* [2022-10-12 14:40:53] [249-250][200/391] Time 0.09 (0.12) Data 0.00 (0.00) Base [Loss 0.908 (1.001)  Prec@1 71.88 (64.09) Prec@5 92.19 (96.20)]\n",
      "*SEARCH* [2022-10-12 14:41:15] [249-250][390/391] Time 0.10 (0.12) Data 0.00 (0.00) Base [Loss 0.742 (0.997)  Prec@1 67.50 (64.67) Prec@5 100.00 (95.65)]\n",
      "[249-250] searching : loss=1.00, accuracy@1=64.67%, accuracy@5=95.65%, time-cost=437.5 s\n",
      "[249-250] evaluate  : loss=1.63, accuracy@1=43.75%, accuracy@5=88.37%\n",
      "[249-250] find-the-best : Structure(4 nodes with |nor_conv_1x1~0|+|nor_conv_1x1~0|nor_conv_3x3~1|+|skip_connect~0|avg_pool_3x3~1|avg_pool_3x3~2|), accuracy@1=58.59%\n",
      "Find ../results/three_metrics/test/checkpoint/seed-68228-basic.pth exist, delete is at first before saving\n",
      "save checkpoint into ../results/three_metrics/test/checkpoint/seed-68228-basic.pth\n",
      "Find ../results/three_metrics/test/seed-68228-last-info.pth exist, delete is at first before saving\n",
      "save checkpoint into ../results/three_metrics/test/seed-68228-last-info.pth\n",
      "Call query_info_str_by_arch with arch=Structure(4 nodes with |nor_conv_1x1~0|+|nor_conv_1x1~0|nor_conv_3x3~1|+|skip_connect~0|avg_pool_3x3~1|avg_pool_3x3~2|) and hp=200\n",
      "Call query_index_by_arch with arch=Structure(4 nodes with |nor_conv_1x1~0|+|nor_conv_1x1~0|nor_conv_3x3~1|+|skip_connect~0|avg_pool_3x3~1|avg_pool_3x3~2|)\n",
      "|nor_conv_1x1~0|+|nor_conv_1x1~0|nor_conv_3x3~1|+|skip_connect~0|avg_pool_3x3~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=5140\n",
      "cifar10-valid  FLOP= 51.04 M, Params=0.372 MB, latency=17.43 ms.\n",
      "cifar10-valid  train : [loss = 0.006, top1 = 99.94%], valid : [loss = 0.517, top1 = 88.67%]\n",
      "cifar10        FLOP= 51.04 M, Params=0.372 MB, latency=17.43 ms.\n",
      "cifar10        train : [loss = 0.010, top1 = 99.82%], test  : [loss = 0.349, top1 = 91.93%]\n",
      "cifar100       FLOP= 51.04 M, Params=0.378 MB, latency=16.86 ms.\n",
      "cifar100       train : [loss = 0.236, top1 = 94.00%], valid : [loss = 1.375, top1 = 67.30%], test : [loss = 1.369, top1 = 67.43%]\n",
      "ImageNet16-120 FLOP= 12.77 M, Params=0.379 MB, latency=15.47 ms.\n",
      "ImageNet16-120 train : [loss = 2.003, top1 = 47.55%], valid : [loss = 2.385, top1 = 39.91%], test : [loss = 2.412, top1 = 39.83%]\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Pre-searching costs 437.5 s\n",
      "RANDOM-NAS finds the best one : Structure(4 nodes with |nor_conv_3x3~0|+|none~0|avg_pool_3x3~1|+|skip_connect~0|nor_conv_1x1~1|avg_pool_3x3~2|) with accuracy=57.81%, with 452.9 s.\n",
      "Call query_info_str_by_arch with arch=Structure(4 nodes with |nor_conv_3x3~0|+|none~0|avg_pool_3x3~1|+|skip_connect~0|nor_conv_1x1~1|avg_pool_3x3~2|) and hp=200\n",
      "Call query_index_by_arch with arch=Structure(4 nodes with |nor_conv_3x3~0|+|none~0|avg_pool_3x3~1|+|skip_connect~0|nor_conv_1x1~1|avg_pool_3x3~2|)\n",
      "|nor_conv_3x3~0|+|none~0|avg_pool_3x3~1|+|skip_connect~0|nor_conv_1x1~1|avg_pool_3x3~2|\n",
      "datasets : ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120'], extra-info : arch-index=2547\n",
      "cifar10-valid  FLOP= 47.10 M, Params=0.344 MB, latency=15.03 ms.\n",
      "cifar10-valid  train : [loss = 0.004, top1 = 99.95%], valid : [loss = 0.443, top1 = 90.10%]\n",
      "cifar10        FLOP= 47.10 M, Params=0.344 MB, latency=15.03 ms.\n",
      "cifar10        train : [loss = 0.006, top1 = 99.90%], test  : [loss = 0.289, top1 = 93.17%]\n",
      "cifar100       FLOP= 47.11 M, Params=0.350 MB, latency=15.23 ms.\n",
      "cifar100       train : [loss = 0.179, top1 = 95.76%], valid : [loss = 1.305, top1 = 69.11%], test : [loss = 1.298, top1 = 69.05%]\n",
      "ImageNet16-120 FLOP= 11.78 M, Params=0.351 MB, latency=13.77 ms.\n",
      "ImageNet16-120 train : [loss = 1.820, top1 = 51.55%], valid : [loss = 2.264, top1 = 42.30%], test : [loss = 2.285, top1 = 42.12%]\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, xshape, class_num = get_datasets(xargs.dataset, xargs.data_path, -1)\n",
    "config = load_config(xargs.config_path, {\"class_num\": class_num, \"xshape\": xshape}, logger)\n",
    "search_loader, _, valid_loader = get_nas_search_loaders(train_data,\n",
    "                                                        valid_data,\n",
    "                                                        xargs.dataset,\n",
    "                                                        \"../configs/nas-benchmark/\",\n",
    "                                                        (config.batch_size, config.test_batch_size),\n",
    "                                                        xargs.workers)\n",
    "logger.log(\"||||||| {:10s} ||||||| Search-Loader-Num={:}, Valid-Loader-Num={:}, batch size={:}\".format(\n",
    "            xargs.dataset, len(search_loader), len(valid_loader), config.batch_size))\n",
    "logger.log(\"||||||| {:10s} ||||||| Config={:}\".format(xargs.dataset, config))\n",
    "\n",
    "search_space = get_search_spaces(\"cell\", xargs.search_space_name)\n",
    "model_config = dict2config(\n",
    "    {\n",
    "        \"name\": \"RANDOM\",\n",
    "        \"C\": xargs.channel,\n",
    "        \"N\": xargs.num_cells,\n",
    "        \"max_nodes\": xargs.max_nodes,\n",
    "        \"num_classes\": class_num,\n",
    "        \"space\": search_space,\n",
    "        \"affine\": False,\n",
    "        \"track_running_stats\": bool(xargs.track_running_stats),\n",
    "    },\n",
    "    None,\n",
    ")\n",
    "search_model = get_cell_based_tiny_net(model_config)\n",
    "\n",
    "w_optimizer, w_scheduler, criterion = get_optim_scheduler(search_model.parameters(), config)\n",
    "\n",
    "logger.log(\"w-optimizer : {:}\".format(w_optimizer))\n",
    "logger.log(\"w-scheduler : {:}\".format(w_scheduler))\n",
    "logger.log(\"criterion   : {:}\".format(criterion))\n",
    "if xargs.arch_nas_dataset is None:\n",
    "    api = None\n",
    "else:\n",
    "    api = API(xargs.arch_nas_dataset)\n",
    "logger.log(\"{:} create API = {:} done\".format(time_string(), api))\n",
    "\n",
    "last_info, model_base_path, model_best_path = (\n",
    "    logger.path(\"info\"),\n",
    "    logger.path(\"model\"),\n",
    "    logger.path(\"best\"),\n",
    ")\n",
    "network, criterion = torch.nn.DataParallel(search_model).cuda(), criterion.cuda()\n",
    "\n",
    "if last_info.exists():  # automatically resume from previous checkpoint\n",
    "    logger.log(\n",
    "        \"=> loading checkpoint of the last-info '{:}' start\".format(last_info)\n",
    "    )\n",
    "    last_info = torch.load(last_info)\n",
    "    start_epoch = last_info[\"epoch\"]\n",
    "    checkpoint = torch.load(last_info[\"last_checkpoint\"])\n",
    "    genotypes = checkpoint[\"genotypes\"]\n",
    "    valid_accuracies = checkpoint[\"valid_accuracies\"]\n",
    "    search_model.load_state_dict(checkpoint[\"search_model\"])\n",
    "    w_scheduler.load_state_dict(checkpoint[\"w_scheduler\"])\n",
    "    w_optimizer.load_state_dict(checkpoint[\"w_optimizer\"])\n",
    "    logger.log(\n",
    "        \"=> loading checkpoint of the last-info '{:}' start with {:}-th epoch.\".format(\n",
    "            last_info, start_epoch\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    logger.log(\"=> do not find the last-info file : {:}\".format(last_info))\n",
    "    start_epoch, valid_accuracies, genotypes = 0, {\"best\": -1}, {}\n",
    "\n",
    "# start training\n",
    "start_time, search_time, epoch_time, total_epoch = (\n",
    "    time.time(),\n",
    "    AverageMeter(),\n",
    "    AverageMeter(),\n",
    "    config.epochs + config.warmup,\n",
    ")\n",
    "for epoch in range(start_epoch, total_epoch):\n",
    "    w_scheduler.update(epoch, 0.0)\n",
    "    need_time = \"Time Left: {:}\".format(\n",
    "        convert_secs2time(epoch_time.val * (total_epoch - epoch), True)\n",
    "    )\n",
    "    epoch_str = \"{:03d}-{:03d}\".format(epoch, total_epoch)\n",
    "    logger.log(\n",
    "        \"\\n[Search the {:}-th epoch] {:}, LR={:}\".format(\n",
    "            epoch_str, need_time, min(w_scheduler.get_lr())\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # selected_arch = search_find_best(valid_loader, network, criterion, xargs.select_num)\n",
    "    search_w_loss, search_w_top1, search_w_top5 = search_func(\n",
    "        search_loader,\n",
    "        network,\n",
    "        criterion,\n",
    "        w_scheduler,\n",
    "        w_optimizer,\n",
    "        epoch_str,\n",
    "        xargs.print_freq,\n",
    "        logger,\n",
    "    )\n",
    "    search_time.update(time.time() - start_time)\n",
    "    logger.log(\n",
    "        \"[{:}] searching : loss={:.2f}, accuracy@1={:.2f}%, accuracy@5={:.2f}%, time-cost={:.1f} s\".format(\n",
    "            epoch_str, search_w_loss, search_w_top1, search_w_top5, search_time.sum\n",
    "        )\n",
    "    )\n",
    "    valid_a_loss, valid_a_top1, valid_a_top5 = valid_func(\n",
    "        valid_loader, network, criterion\n",
    "    )\n",
    "    logger.log(\n",
    "        \"[{:}] evaluate  : loss={:.2f}, accuracy@1={:.2f}%, accuracy@5={:.2f}%\".format(\n",
    "            epoch_str, valid_a_loss, valid_a_top1, valid_a_top5\n",
    "        )\n",
    "    )\n",
    "    cur_arch, cur_valid_acc = search_find_best(\n",
    "        valid_loader, network, xargs.select_num\n",
    "    )\n",
    "    logger.log(\n",
    "        \"[{:}] find-the-best : {:}, accuracy@1={:.2f}%\".format(\n",
    "            epoch_str, cur_arch, cur_valid_acc\n",
    "        )\n",
    "    )\n",
    "    genotypes[epoch] = cur_arch\n",
    "    # check the best accuracy\n",
    "    valid_accuracies[epoch] = valid_a_top1\n",
    "    if valid_a_top1 > valid_accuracies[\"best\"]:\n",
    "        valid_accuracies[\"best\"] = valid_a_top1\n",
    "        find_best = True\n",
    "    else:\n",
    "        find_best = False\n",
    "\n",
    "    # save checkpoint\n",
    "    save_path = save_checkpoint(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"args\": deepcopy(xargs),\n",
    "            \"search_model\": search_model.state_dict(),\n",
    "            \"w_optimizer\": w_optimizer.state_dict(),\n",
    "            \"w_scheduler\": w_scheduler.state_dict(),\n",
    "            \"genotypes\": genotypes,\n",
    "            \"valid_accuracies\": valid_accuracies,\n",
    "        },\n",
    "        model_base_path,\n",
    "        logger,\n",
    "    )\n",
    "    last_info = save_checkpoint(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"args\": deepcopy(args),\n",
    "            \"last_checkpoint\": save_path,\n",
    "        },\n",
    "        logger.path(\"info\"),\n",
    "        logger,\n",
    "    )\n",
    "    if find_best:\n",
    "        logger.log(\n",
    "            \"<<<--->>> The {:}-th epoch : find the highest validation accuracy : {:.2f}%.\".format(\n",
    "                epoch_str, valid_a_top1\n",
    "            )\n",
    "        )\n",
    "        copy_checkpoint(model_base_path, model_best_path, logger)\n",
    "    if api is not None:\n",
    "        logger.log(\"{:}\".format(api.query_by_arch(genotypes[epoch], \"200\")))\n",
    "    # measure elapsed time\n",
    "    epoch_time.update(time.time() - start_time)\n",
    "    start_time = time.time()\n",
    "\n",
    "logger.log(\"\\n\" + \"-\" * 200)\n",
    "logger.log(\"Pre-searching costs {:.1f} s\".format(search_time.sum))\n",
    "start_time = time.time()\n",
    "best_arch, best_acc = search_find_best(valid_loader, network, xargs.select_num)\n",
    "search_time.update(time.time() - start_time)\n",
    "logger.log(\n",
    "    \"RANDOM-NAS finds the best one : {:} with accuracy={:.2f}%, with {:.1f} s.\".format(\n",
    "        best_arch, best_acc, search_time.sum\n",
    "    )\n",
    ")\n",
    "if api is not None:\n",
    "    logger.log(\"{:}\".format(api.query_by_arch(best_arch, \"200\")))\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c54c509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(network.state_dict(), \"./250ep_supernet_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df07a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_acc_metric(network, inputs, targets):\n",
    "    with torch.no_grad():\n",
    "        network.eval()\n",
    "        _, logits = network(inputs)\n",
    "        val_top1, val_top5 = obtain_accuracy(\n",
    "            logits.data, targets.data, topk=(1, 5)\n",
    "        )\n",
    "    return val_top1.item()\n",
    "\n",
    "def acc_confidence_robustness_metrics(network, inputs, targets):\n",
    "    with torch.no_grad():\n",
    "        network.eval()\n",
    "        # accuracy\n",
    "        _, logits = network(inputs)\n",
    "        val_top1, val_top5 = obtain_accuracy(logits.data, targets.data, topk=(1, 5))\n",
    "        acc = val_top1.item()\n",
    "        # confidence\n",
    "        prob = torch.nn.funcional.softmax(logits, dim=1)\n",
    "        one_hot_idx = torch.nn.functional.one_hot(targets)\n",
    "        confidence = (prob[one_hot_idx].sum()) / inputs.size(0)\n",
    "        # robustness\n",
    "        _, noisy_logits = network(inputs + torch.randn_like(inputs)*0.01)\n",
    "        kl_loss = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        robustness = -kl_loss(F.log_softmax(noisy_logits), F.softmax(logits))\n",
    "        \n",
    "        return acc, confidence, robustness\n",
    "    \n",
    "def step_sim_metric(network, criterion, inputs, targets):\n",
    "    original_dict = deepcopy(network.state_dict())\n",
    "    optim_large_step = torch.optim.SGD(network.parameters(), lr=0.1)\n",
    "    \n",
    "    # single large step\n",
    "    network.train()\n",
    "    optim_large_step.zero_grad()\n",
    "    _, logits = network(inputs)\n",
    "    base_loss = criterion(logits, targets)\n",
    "    base_loss.backward()\n",
    "    optim_large_step.step()\n",
    "    large_step_dict = deepcopy(network.state_dict())\n",
    "    \n",
    "    # multiple small steps\n",
    "    network.load_state_dict(original_dict)\n",
    "    optim_small_step = torch.optim.SGD(network.parameters(), lr=0.01)\n",
    "    for i in range(10):\n",
    "        optim_small_step.zero_grad()\n",
    "        _, logits = network(inputs)\n",
    "        base_loss = criterion(logits, targets)\n",
    "        base_loss.backward()\n",
    "        optim_small_step.step()\n",
    "    small_step_dict = deepcopy(network.state_dict())\n",
    "    \n",
    "    # resume\n",
    "    network.load_state_dict(original_dict)\n",
    "    network.eval()\n",
    "    \n",
    "    for key in large_step_dict.keys():\n",
    "#         if 'classifier' in key:\n",
    "#             large_step = large_step_dict[key] - original_dict[key]\n",
    "#             small_step = small_step_dict[key] - original_dict[key]\n",
    "#             score = torch.nn.functional.cosine_similarity(large_step, small_step, dim=1)\n",
    "#             score = score.mean().item() * 100 # in percent\n",
    "#             break\n",
    "        if 'stem.0.weight' in key:\n",
    "            large_step = large_step_dict[key] - original_dict[key]\n",
    "            small_step = small_step_dict[key] - original_dict[key]\n",
    "            co, ci, kh, kw = large_step.size()\n",
    "            large_step = large_step.view(co, -1)\n",
    "            small_step = small_step.view(co, -1)\n",
    "            score = torch.nn.functional.cosine_similarity(large_step, small_step, dim=1)\n",
    "            score = score.mean().item() * 100 # in percent\n",
    "            break\n",
    "            \n",
    "    return score\n",
    "    \n",
    "\n",
    "def search_find_best_my(xloader, network, criterion, n_samples):\n",
    "    archs, metric_accs, metric_steps = [], [], []\n",
    "    loader_iter = iter(xloader)\n",
    "    for i in tqdm.tqdm(range(n_samples)):\n",
    "        arch = network.module.random_genotype(True)\n",
    "        try:\n",
    "            inputs, targets = next(loader_iter)\n",
    "        except:\n",
    "            loader_iter = iter(xloader)\n",
    "            inputs, targets = next(loader_iter)\n",
    "        inputs, targets = inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True)\n",
    "        valid_acc = valid_acc_metric(network, inputs, targets)\n",
    "        step_sim = step_sim_metric(network, criterion, inputs, targets)\n",
    "#         robustness = \n",
    "#         confidence = \n",
    "\n",
    "        archs.append(arch)\n",
    "        metric_accs.append(valid_acc)\n",
    "        metric_steps.append(step_sim)\n",
    "\n",
    "    return archs, metric_accs, metric_steps\n",
    "#     best_idx = np.argmax(valid_accs)\n",
    "#     best_arch, best_valid_acc = archs[best_idx], valid_accs[best_idx]\n",
    "#     return best_arch, best_valid_acc\n",
    "    \n",
    "#### my\n",
    "start_time = time.time()\n",
    "archs, metric_accs, metric_steps = search_find_best_my(valid_loader, network, criterion, 100)\n",
    "search_time.update(time.time() - start_time)\n",
    "# find best\n",
    "# best_idx = np.argmax(valid_accs)\n",
    "import scipy.stats as stats\n",
    "rank_accs, rank_steps = stats.rankdata(metric_accs), stats.rankdata(metric_steps)\n",
    "rank_agg = rank_accs+rank_steps\n",
    "best_idx = np.argmax(rank_agg)\n",
    "\n",
    "\n",
    "best_arch, best_acc, best_step = archs[best_idx], metric_accs[best_idx], metric_steps[best_idx]\n",
    "# log\n",
    "print(\n",
    "    \"RANDOM-NAS finds the best one : {:} with accuracy={:.2f}% traj_sim={:.2f}%, with {:.1f} s.\".format(\n",
    "        best_arch, best_acc, best_step, search_time.sum\n",
    "    )\n",
    ")\n",
    "if api is not None:\n",
    "    print(\"{:}\".format(api.query_by_arch(best_arch, \"200\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10463444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_acc_from_api(api, arch):\n",
    "    # print(api.query_by_arch(arch, \"200\"))\n",
    "    index = api.query_index_by_arch(arch)\n",
    "    results = api.query_by_index(index, 'cifar10-valid', '200') # a dict of all trials for 1st net on cifar100, where the key is the seed\n",
    "    acc = 0\n",
    "    for seed, result in results.items():\n",
    "        acc = acc + result.get_eval('valid')['accuracy']\n",
    "    acc = acc / len(results)\n",
    "    return acc\n",
    "\n",
    "api_valid_accs = [get_valid_acc_from_api(api, a) for a in archs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90e1f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "x = stats.rankdata(metric_accs)\n",
    "y = stats.rankdata(api_valid_accs)\n",
    "plt.scatter(x, y)\n",
    "plt.show()\n",
    "tau, p_value = stats.kendalltau(x, y)\n",
    "print(tau, p_value)\n",
    "\n",
    "x = stats.rankdata(metric_steps)\n",
    "y = stats.rankdata(api_valid_accs)\n",
    "plt.scatter(x, y)\n",
    "plt.show()\n",
    "tau, p_value = stats.kendalltau(x, y)\n",
    "print(tau, p_value)\n",
    "\n",
    "x = stats.rankdata(rank_agg)\n",
    "y = stats.rankdata(api_valid_accs)\n",
    "plt.scatter(x, y)\n",
    "plt.show()\n",
    "tau, p_value = stats.kendalltau(x, y)\n",
    "print(tau, p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6622d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
