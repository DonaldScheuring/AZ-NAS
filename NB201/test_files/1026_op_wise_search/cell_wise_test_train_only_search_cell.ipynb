{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "902bb49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 17:53:29.429475: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, time, glob, random, argparse\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# XAutoDL \n",
    "from xautodl.config_utils import load_config, dict2config, configure2str\n",
    "from xautodl.datasets import get_datasets, get_nas_search_loaders\n",
    "from xautodl.procedures import (\n",
    "    prepare_seed,\n",
    "    prepare_logger,\n",
    "    save_checkpoint,\n",
    "    copy_checkpoint,\n",
    "    get_optim_scheduler,\n",
    ")\n",
    "from xautodl.utils import get_model_infos, obtain_accuracy\n",
    "from xautodl.log_utils import AverageMeter, time_string, convert_secs2time\n",
    "from xautodl.models import get_search_spaces\n",
    "\n",
    "from custom_models import get_cell_based_tiny_net\n",
    "from custom_search_cells import NAS201SearchCell as SearchCell\n",
    "from cell_operations import ResNetBasicblock\n",
    "from xautodl.models.cell_searchs.genotypes import Structure\n",
    "\n",
    "# NB201\n",
    "from nas_201_api import NASBench201API as API\n",
    "\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "792763c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48398\n",
      "Namespace(arch_nas_dataset=None, channel=16, config_path='../configs/nas-benchmark/algos/RANDOM.config', data_path='../cifar.python', dataset='cifar10', max_nodes=4, num_cells=5, print_freq=100, rand_seed=48398, save_dir='./train_only_search_cell_include_classifier', search_space_name='nas-bench-201', select_num=100, track_running_stats=0, workers=4)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(\"Random search for NAS.\")\n",
    "parser.add_argument(\"--data_path\", type=str, default='../cifar.python', help=\"The path to dataset\")\n",
    "parser.add_argument(\"--dataset\", type=str, default='cifar10',choices=[\"cifar10\", \"cifar100\", \"ImageNet16-120\"], help=\"Choose between Cifar10/100 and ImageNet-16.\")\n",
    "\n",
    "# channels and number-of-cells\n",
    "parser.add_argument(\"--search_space_name\", type=str, default='nas-bench-201', help=\"The search space name.\")\n",
    "parser.add_argument(\"--config_path\", type=str, default='../configs/nas-benchmark/algos/RANDOM.config', help=\"The path to the configuration.\")\n",
    "parser.add_argument(\"--max_nodes\", type=int, default=4, help=\"The maximum number of nodes.\")\n",
    "parser.add_argument(\"--channel\", type=int, default=16, help=\"The number of channels.\")\n",
    "parser.add_argument(\"--num_cells\", type=int, default=5, help=\"The number of cells in one stage.\")\n",
    "parser.add_argument(\"--select_num\", type=int, default=100, help=\"The number of selected architectures to evaluate.\")\n",
    "parser.add_argument(\"--track_running_stats\", type=int, default=0, choices=[0, 1], help=\"Whether use track_running_stats or not in the BN layer.\")\n",
    "# log\n",
    "parser.add_argument(\"--workers\", type=int, default=4, help=\"number of data loading workers\")\n",
    "parser.add_argument(\"--save_dir\", type=str, default='./train_only_search_cell_include_classifier', help=\"Folder to save checkpoints and log.\")\n",
    "# parser.add_argument(\"--arch_nas_dataset\", type=str, default='../NAS-Bench-201-v1_1-096897.pth', help=\"The path to load the architecture dataset (tiny-nas-benchmark).\")\n",
    "parser.add_argument(\"--arch_nas_dataset\", type=str, default=None, help=\"The path to load the architecture dataset (tiny-nas-benchmark).\")\n",
    "parser.add_argument(\"--print_freq\", type=int, default=100, help=\"print frequency (default: 200)\")\n",
    "parser.add_argument(\"--rand_seed\", type=int, default=None, help=\"manual seed\")\n",
    "args = parser.parse_args(args=[])\n",
    "if args.rand_seed is None or args.rand_seed < 0:\n",
    "    args.rand_seed = random.randint(1, 100000)\n",
    "\n",
    "    \n",
    "print(args.rand_seed)\n",
    "print(args)\n",
    "xargs=args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dcd1b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Function with logger : Logger(dir=train_only_search_cell_include_classifier, use-tf=False, writer=None)\n",
      "Arguments : -------------------------------\n",
      "arch_nas_dataset : None\n",
      "channel          : 16\n",
      "config_path      : ../configs/nas-benchmark/algos/RANDOM.config\n",
      "data_path        : ../cifar.python\n",
      "dataset          : cifar10\n",
      "max_nodes        : 4\n",
      "num_cells        : 5\n",
      "print_freq       : 100\n",
      "rand_seed        : 48398\n",
      "save_dir         : ./train_only_search_cell_include_classifier\n",
      "search_space_name : nas-bench-201\n",
      "select_num       : 100\n",
      "track_running_stats : 0\n",
      "workers          : 4\n",
      "Python  Version  : 3.7.13 (default, Mar 29 2022, 02:18:16)  [GCC 7.5.0]\n",
      "Pillow  Version  : 9.0.1\n",
      "PyTorch Version  : 1.12.0\n",
      "cuDNN   Version  : 8302\n",
      "CUDA available   : True\n",
      "CUDA GPU numbers : 2\n",
      "CUDA_VISIBLE_DEVICES : None\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available(), \"CUDA is not available.\"\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.set_num_threads(xargs.workers)\n",
    "prepare_seed(xargs.rand_seed)\n",
    "logger = prepare_logger(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9daa5275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "../configs/nas-benchmark/algos/RANDOM.config\n",
      "Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=250, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, test_batch_size=512, class_num=10, xshape=(1, 3, 32, 32))\n",
      "||||||| cifar10    ||||||| Search-Loader-Num=391, Valid-Loader-Num=49, batch size=64\n",
      "||||||| cifar10    ||||||| Config=Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=250, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, test_batch_size=512, class_num=10, xshape=(1, 3, 32, 32))\n",
      "w-optimizer : SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    initial_lr: 0.025\n",
      "    lr: 0.025\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: True\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "w-scheduler : CosineAnnealingLR(warmup=0, max-epoch=250, current::epoch=0, iter=0.00, type=cosine, T-max=250, eta-min=0.001)\n",
      "criterion   : CrossEntropyLoss()\n",
      "[2022-10-29 17:53:32] create API = None done\n",
      "=> do not find the last-info file : train_only_search_cell_include_classifier/seed-48398-last-info.pth\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, xshape, class_num = get_datasets(xargs.dataset, xargs.data_path, -1)\n",
    "config = load_config(xargs.config_path, {\"class_num\": class_num, \"xshape\": xshape}, logger)\n",
    "search_loader, _, valid_loader = get_nas_search_loaders(train_data,\n",
    "                                                        valid_data,\n",
    "                                                        xargs.dataset,\n",
    "                                                        \"../configs/nas-benchmark/\",\n",
    "                                                        (config.batch_size, config.test_batch_size),\n",
    "                                                        xargs.workers)\n",
    "logger.log(\"||||||| {:10s} ||||||| Search-Loader-Num={:}, Valid-Loader-Num={:}, batch size={:}\".format(\n",
    "            xargs.dataset, len(search_loader), len(valid_loader), config.batch_size))\n",
    "logger.log(\"||||||| {:10s} ||||||| Config={:}\".format(xargs.dataset, config))\n",
    "\n",
    "search_space = get_search_spaces(\"cell\", xargs.search_space_name)\n",
    "model_config = dict2config(\n",
    "    {\n",
    "        \"name\": \"RANDOM\",\n",
    "        \"C\": xargs.channel,\n",
    "        \"N\": xargs.num_cells,\n",
    "        \"max_nodes\": xargs.max_nodes,\n",
    "        \"num_classes\": class_num,\n",
    "        \"space\": search_space,\n",
    "        \"affine\": False,\n",
    "        \"track_running_stats\": bool(xargs.track_running_stats),\n",
    "    },\n",
    "    None,\n",
    ")\n",
    "search_model = get_cell_based_tiny_net(model_config)\n",
    "\n",
    "w_optimizer, w_scheduler, criterion = get_optim_scheduler(search_model.parameters(), config)\n",
    "\n",
    "logger.log(\"w-optimizer : {:}\".format(w_optimizer))\n",
    "logger.log(\"w-scheduler : {:}\".format(w_scheduler))\n",
    "logger.log(\"criterion   : {:}\".format(criterion))\n",
    "# if xargs.arch_nas_dataset is None:\n",
    "api = None\n",
    "# else:\n",
    "#     api = API(xargs.arch_nas_dataset)\n",
    "logger.log(\"{:} create API = {:} done\".format(time_string(), api))\n",
    "\n",
    "last_info, model_base_path, model_best_path = (\n",
    "    logger.path(\"info\"),\n",
    "    logger.path(\"model\"),\n",
    "    logger.path(\"best\"),\n",
    ")\n",
    "network, criterion = torch.nn.DataParallel(search_model).cuda(), criterion.cuda()\n",
    "\n",
    "if last_info.exists():  # automatically resume from previous checkpoint\n",
    "    logger.log(\n",
    "        \"=> loading checkpoint of the last-info '{:}' start\".format(last_info)\n",
    "    )\n",
    "    last_info = torch.load(last_info)\n",
    "    start_epoch = last_info[\"epoch\"]\n",
    "    checkpoint = torch.load(last_info[\"last_checkpoint\"])\n",
    "    genotypes = checkpoint[\"genotypes\"]\n",
    "    valid_accuracies = checkpoint[\"valid_accuracies\"]\n",
    "    search_model.load_state_dict(checkpoint[\"search_model\"])\n",
    "    w_scheduler.load_state_dict(checkpoint[\"w_scheduler\"])\n",
    "    w_optimizer.load_state_dict(checkpoint[\"w_optimizer\"])\n",
    "    logger.log(\n",
    "        \"=> loading checkpoint of the last-info '{:}' start with {:}-th epoch.\".format(\n",
    "            last_info, start_epoch\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    logger.log(\"=> do not find the last-info file : {:}\".format(last_info))\n",
    "    start_epoch, valid_accuracies, genotypes = 0, {\"best\": -1}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94ba0e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_confidence_robustness_metrics(network, inputs, targets):\n",
    "    with torch.no_grad():\n",
    "        # accuracy\n",
    "        network.train()\n",
    "        _, logits = network(inputs)\n",
    "        val_top1, val_top5 = obtain_accuracy(logits.data, targets.data, topk=(1, 5))\n",
    "        acc = val_top1\n",
    "        \n",
    "        # confidence\n",
    "        prob = torch.nn.functional.softmax(logits, dim=1)\n",
    "        one_hot_idx = torch.nn.functional.one_hot(targets)\n",
    "        confidence = (prob[one_hot_idx==1].sum()) / inputs.size(0) * 100 # in percent\n",
    "        \n",
    "        # sensitivity\n",
    "        _, noisy_logits = network(inputs + torch.randn_like(inputs)*0.1)\n",
    "        kl_loss = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        sensitivity = kl_loss(torch.nn.functional.log_softmax(noisy_logits, dim=1), torch.nn.functional.softmax(logits, dim=1))\n",
    "        \n",
    "        # robustness\n",
    "        original_weights = deepcopy(network.state_dict())\n",
    "        for m in network.modules():\n",
    "            if isinstance(m, SearchCell):\n",
    "                for p in m.parameters():\n",
    "                    p.add_(torch.randn_like(p) * p.std()*0.3)\n",
    "            \n",
    "        _, noisy_logits = network(inputs)\n",
    "        kl_loss = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        robustness = -kl_loss(torch.nn.functional.log_softmax(noisy_logits, dim=1), torch.nn.functional.softmax(logits, dim=1))\n",
    "        network.load_state_dict(original_weights)\n",
    "                \n",
    "        return acc.item(), confidence.item(), sensitivity.item(), robustness.item()\n",
    "    \n",
    "def step_sim_metric(network, criterion, inputs, targets):\n",
    "    original_dict = deepcopy(network.state_dict())\n",
    "    optim_large_step = torch.optim.SGD(network.parameters(), lr=0.025)\n",
    "    \n",
    "    # single large step\n",
    "    network.train()\n",
    "    optim_large_step.zero_grad()\n",
    "    _, logits = network(inputs)\n",
    "    base_loss = criterion(logits, targets)\n",
    "    base_loss.backward()\n",
    "    optim_large_step.step()\n",
    "    large_step_dict = deepcopy(network.state_dict())\n",
    "    \n",
    "    # multiple small steps\n",
    "    network.load_state_dict(original_dict)\n",
    "    optim_small_step = torch.optim.SGD(network.parameters(), lr=0.025/3)\n",
    "    for i in range(3):\n",
    "        optim_small_step.zero_grad()\n",
    "        _, logits = network(inputs)\n",
    "        base_loss = criterion(logits, targets)\n",
    "        base_loss.backward()\n",
    "        optim_small_step.step()\n",
    "    small_step_dict = deepcopy(network.state_dict())\n",
    "    scores = []\n",
    "    for key in large_step_dict.keys():\n",
    "        if ('weight' in key) and (original_dict[key].dim()==4):\n",
    "            if (original_dict[key] != large_step_dict[key]).sum():\n",
    "                large_step = large_step_dict[key] - original_dict[key]\n",
    "                small_step = small_step_dict[key] - original_dict[key]\n",
    "                co, ci, kh, kw = large_step.size()\n",
    "                large_step = large_step.view(co, -1)\n",
    "                small_step = small_step.view(co, -1)\n",
    "                score = torch.nn.functional.cosine_similarity(large_step, small_step, dim=1)\n",
    "                score = score.mean().item() * 100 # in percent\n",
    "                scores.append(score)\n",
    "    if len(scores)==0:\n",
    "        step_sim = 100\n",
    "        raise RuntimeError\n",
    "    else:\n",
    "        step_sim = np.mean(scores)\n",
    "    \n",
    "    # resume\n",
    "    network.load_state_dict(original_dict)\n",
    "            \n",
    "    return step_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cdab22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of nodes:60\n",
      "target_node:1 4\n",
      "target_node:2 24\n",
      "target_node:3 124\n",
      "Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=250, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, test_batch_size=512, class_num=10, xshape=(1, 3, 32, 32))\n",
      "0.025\n",
      "\n",
      "\n",
      " Searching with a cell #0\n",
      "\n",
      "Current target cell:0 / current target node:1\n",
      "*Train* [2022-10-29 17:53:36] Ep:0 [000/391] Time 2.44 (2.44) Data 0.16 (0.16) Base [Loss 2.285 (2.285)  Prec@1 9.38 (9.38) Prec@5 59.38 (59.38)]\n",
      "*Train* [2022-10-29 17:53:51] Ep:0 [200/391] Time 0.12 (0.09) Data 0.00 (0.00) Base [Loss 2.057 (2.136)  Prec@1 25.00 (20.48) Prec@5 73.44 (71.93)]\n",
      "*Train* [2022-10-29 17:54:08] Ep:0 [390/391] Time 0.06 (0.09) Data 0.00 (0.00) Base [Loss 2.034 (2.069)  Prec@1 27.50 (22.81) Prec@5 65.00 (75.88)]\n",
      "Ep:0 ends : loss=2.07, accuracy@1=22.81%, accuracy@5=75.88%\n",
      "*Train* [2022-10-29 17:54:08] Ep:1 [000/391] Time 0.27 (0.27) Data 0.16 (0.16) Base [Loss 1.948 (1.948)  Prec@1 31.25 (31.25) Prec@5 79.69 (79.69)]\n",
      "*Train* [2022-10-29 17:54:24] Ep:1 [200/391] Time 0.06 (0.08) Data 0.00 (0.00) Base [Loss 1.882 (1.942)  Prec@1 35.94 (27.22) Prec@5 81.25 (81.70)]\n",
      "*Train* [2022-10-29 17:54:42] Ep:1 [390/391] Time 0.07 (0.09) Data 0.00 (0.00) Base [Loss 2.132 (1.929)  Prec@1 17.50 (27.49) Prec@5 77.50 (82.32)]\n",
      "Ep:1 ends : loss=1.93, accuracy@1=27.49%, accuracy@5=82.32%\n",
      "*Train* [2022-10-29 17:54:42] Ep:2 [000/391] Time 0.26 (0.26) Data 0.16 (0.16) Base [Loss 2.081 (2.081)  Prec@1 29.69 (29.69) Prec@5 75.00 (75.00)]\n",
      "*Train* [2022-10-29 17:55:02] Ep:2 [200/391] Time 0.14 (0.10) Data 0.00 (0.00) Base [Loss 1.802 (1.894)  Prec@1 35.94 (29.64) Prec@5 90.62 (83.54)]\n",
      "*Train* [2022-10-29 17:55:18] Ep:2 [390/391] Time 0.06 (0.09) Data 0.00 (0.00) Base [Loss 1.801 (1.885)  Prec@1 47.50 (29.67) Prec@5 82.50 (83.65)]\n",
      "Ep:2 ends : loss=1.89, accuracy@1=29.67%, accuracy@5=83.65%\n",
      "*Train* [2022-10-29 17:55:18] Ep:3 [000/391] Time 0.23 (0.23) Data 0.16 (0.16) Base [Loss 1.856 (1.856)  Prec@1 26.56 (26.56) Prec@5 81.25 (81.25)]\n",
      "*Train* [2022-10-29 17:55:34] Ep:3 [200/391] Time 0.07 (0.08) Data 0.00 (0.00) Base [Loss 1.752 (1.842)  Prec@1 39.06 (30.84) Prec@5 87.50 (84.75)]\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "start_time, search_time, epoch_time, total_epoch = (\n",
    "    time.time(),\n",
    "    AverageMeter(),\n",
    "    AverageMeter(),\n",
    "    config.epochs + config.warmup,\n",
    ")\n",
    "\n",
    "################# initialize\n",
    "cells = []\n",
    "for m in network.modules():\n",
    "    if isinstance(m, SearchCell):\n",
    "        cells.append(m)\n",
    "num_cells = len(cells)\n",
    "print(\"total number of nodes:{}\".format(num_cells*xargs.max_nodes))\n",
    "        \n",
    "op_names = deepcopy(cells[0].op_names)\n",
    "op_names_wo_none = deepcopy(op_names)\n",
    "if \"none\" in op_names_wo_none:\n",
    "    op_names_wo_none.remove(\"none\")\n",
    "\n",
    "genotypes = []\n",
    "for i in range(1, xargs.max_nodes):\n",
    "    xlist = []\n",
    "    for j in range(i):\n",
    "        node_str = \"{:}<-{:}\".format(i, j)\n",
    "        if i-j==1:\n",
    "            op_name = \"skip_connect\"\n",
    "        else:\n",
    "            op_name = \"none\"\n",
    "        xlist.append((op_name, j))\n",
    "    genotypes.append(tuple(xlist))\n",
    "init_arch = Structure(genotypes)\n",
    "\n",
    "for c in cells:\n",
    "    c.arch_cache = init_arch\n",
    "\n",
    "### gen possible connections of a target node\n",
    "possible_connections = {}\n",
    "for target_node_idx in range(1,xargs.max_nodes):\n",
    "    possible_connections[target_node_idx] = list()\n",
    "    xlists = []\n",
    "    for src_node in range(target_node_idx):\n",
    "        node_str = \"{:}<-{:}\".format(target_node_idx, src_node)\n",
    "        # select possible ops\n",
    "#         if target_node_idx - src_node == 1:\n",
    "#             op_names_tmp = op_names_wo_none\n",
    "#         else:\n",
    "#             op_names_tmp = op_names\n",
    "        op_names_tmp = op_names\n",
    "            \n",
    "        if len(xlists) == 0: # initial iteration\n",
    "            for op_name in op_names_tmp:\n",
    "                xlists.append([(op_name, src_node)])\n",
    "        else:\n",
    "            new_xlists = []\n",
    "            for op_name in op_names_tmp:\n",
    "                for xlist in xlists:\n",
    "                    new_xlist = deepcopy(xlist)\n",
    "                    new_xlist.append((op_name, src_node))\n",
    "                    new_xlists.append(new_xlist)\n",
    "            xlists = new_xlists\n",
    "    for xlist in xlists:\n",
    "        selected_ops = []\n",
    "        for l in xlist:\n",
    "            selected_ops.append(l[0])\n",
    "        if sum(np.array(selected_ops) == \"none\") == len(selected_ops):\n",
    "            continue\n",
    "        possible_connections[target_node_idx].append(tuple(xlist))\n",
    "    print(\"target_node:{}\".format(target_node_idx), len(possible_connections[target_node_idx]))\n",
    "        \n",
    "### train while generating random architectures by mutating connections of a target node\n",
    "print(config)\n",
    "print(config.LR)\n",
    "for target_cell_idx in range(num_cells):\n",
    "    target_cell = cells[target_cell_idx]\n",
    "    ###\n",
    "    target_params = list(network.module.stem.parameters())+list(network.module.classifier.parameters())\n",
    "    i = 0\n",
    "    for m in network.modules():\n",
    "        if isinstance(m, ResNetBasicblock):\n",
    "            target_params += list(m.parameters())\n",
    "        if isinstance(m, SearchCell):\n",
    "            target_params += list(m.parameters())\n",
    "            if i == target_cell_idx:\n",
    "                break\n",
    "            i += 1\n",
    "    w_optimizer = torch.optim.SGD(target_params, lr=config.LR, momentum=config.momentum, weight_decay=config.decay, nesterov=config.nesterov)\n",
    "    ###\n",
    "    print(\"\\n\\n Searching with a cell #{}\".format(target_cell_idx))\n",
    "    for cell_loop in range(1):\n",
    "        for target_node_idx in range(1,xargs.max_nodes):\n",
    "            current_genotypes,_ = target_cell.arch_cache.tolist(None)\n",
    "            print(\"\\nCurrent target cell:{} / current target node:{}\".format(target_cell_idx, target_node_idx))\n",
    "            ## training\n",
    "            for ep in range(5):\n",
    "                data_time, batch_time = AverageMeter(), AverageMeter()\n",
    "                base_losses, base_top1, base_top5 = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "                network.train()\n",
    "                end = time.time()\n",
    "                print_freq = 200\n",
    "                for step, (base_inputs, base_targets, arch_inputs, arch_targets) in enumerate(search_loader):\n",
    "                    ######### random generation\n",
    "                    genotypes = deepcopy(current_genotypes)\n",
    "                    connection = random.choice(possible_connections[target_node_idx])\n",
    "                    genotypes[target_node_idx-1] = connection\n",
    "                    arch = Structure(genotypes)\n",
    "                    target_cell.arch_cache = arch\n",
    "\n",
    "                    ######### forward/backward/optim\n",
    "                    base_targets = base_targets.cuda(non_blocking=True)\n",
    "                    arch_targets = arch_targets.cuda(non_blocking=True)\n",
    "                    # measure data loading time\n",
    "                    data_time.update(time.time() - end)\n",
    "                    w_optimizer.zero_grad()\n",
    "                    _, logits = network(base_inputs)\n",
    "                    base_loss = criterion(logits, base_targets)\n",
    "                    base_loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(network.parameters(), 5)\n",
    "                    w_optimizer.step()\n",
    "\n",
    "                    ######### logging\n",
    "                    base_prec1, base_prec5 = obtain_accuracy(logits.data, base_targets.data, topk=(1, 5))\n",
    "                    base_losses.update(base_loss.item(), base_inputs.size(0))\n",
    "                    base_top1.update(base_prec1.item(), base_inputs.size(0))\n",
    "                    base_top5.update(base_prec5.item(), base_inputs.size(0))\n",
    "                    batch_time.update(time.time() - end)\n",
    "                    end = time.time()\n",
    "                    if step % print_freq == 0 or step + 1 == len(search_loader):\n",
    "                        Sstr = (\"*Train* \"+ time_string()+\" Ep:{:} [{:03d}/{:03d}]\".format(ep, step, len(search_loader)))\n",
    "                        Tstr = \"Time {batch_time.val:.2f} ({batch_time.avg:.2f}) Data {data_time.val:.2f} ({data_time.avg:.2f})\".format(batch_time=batch_time, data_time=data_time)\n",
    "                        Wstr = \"Base [Loss {loss.val:.3f} ({loss.avg:.3f})  Prec@1 {top1.val:.2f} ({top1.avg:.2f}) Prec@5 {top5.val:.2f} ({top5.avg:.2f})]\".format(loss=base_losses, top1=base_top1, top5=base_top5)\n",
    "                        logger.log(Sstr + \" \" + Tstr + \" \" + Wstr)\n",
    "\n",
    "                logger.log(\"Ep:{:} ends : loss={:.2f}, accuracy@1={:.2f}%, accuracy@5={:.2f}%\".format(ep, base_losses.avg, base_top1.avg, base_top5.avg))\n",
    "            ## evaluation\n",
    "            network.train()\n",
    "            archs, metric_accs, metric_confidences, metric_sensitivities, metric_robustnesses, metric_step_sims = [], [], [], [], [], []\n",
    "            loader_iter = iter(valid_loader)\n",
    "            for connection in possible_connections[target_node_idx]:\n",
    "                ###### traverse over possible archs\n",
    "                genotypes = deepcopy(current_genotypes)\n",
    "                genotypes[target_node_idx-1] = connection\n",
    "                arch = Structure(genotypes)\n",
    "                target_cell.arch_cache = arch\n",
    "                ###### measure metrics\n",
    "                try:\n",
    "                    inputs, targets = next(loader_iter)\n",
    "                except:\n",
    "                    loader_iter = iter(valid_loader)\n",
    "                    inputs, targets = next(loader_iter)\n",
    "                inputs, targets = inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True)\n",
    "                valid_acc, confidence, sensitivity, robustness = acc_confidence_robustness_metrics(network, inputs, targets)\n",
    "                step_sim = step_sim_metric(network, criterion, inputs, targets)\n",
    "                archs.append(arch)\n",
    "                metric_accs.append(valid_acc)\n",
    "                metric_confidences.append(confidence)\n",
    "                metric_sensitivities.append(sensitivity)\n",
    "                metric_robustnesses.append(robustness)\n",
    "                metric_step_sims.append(step_sim)\n",
    "            rank_accs, rank_confidences, rank_sensitivities, rank_robustnesses, rank_step_sims = stats.rankdata(metric_accs), stats.rankdata(metric_confidences), stats.rankdata(metric_sensitivities), stats.rankdata(metric_robustnesses), stats.rankdata(metric_step_sims)\n",
    "            l = len(rank_accs)\n",
    "            rank_agg = np.log(rank_accs/l)+np.log(rank_confidences/l)+np.log(rank_sensitivities/l)+np.log(rank_robustnesses/l)+np.log(rank_step_sims/l)\n",
    "#             rank_agg = np.log(rank_accs/l)+np.log(rank_confidences/l)+np.log(rank_sensitivities/l)+np.log(rank_step_sims/l)\n",
    "            best_idx = np.argmax(rank_agg)\n",
    "            best_arch, best_acc, best_conf, best_sensitivity, best_robust, best_step_sim = archs[best_idx], metric_accs[best_idx], metric_confidences[best_idx], metric_sensitivities[best_idx], metric_robustnesses[best_idx], metric_step_sims[best_idx]\n",
    "            logger.log(\"Found best op for target cell:{} / target node:{}\".format(target_cell_idx, target_node_idx))\n",
    "            logger.log(\": {:} with accuracy={:.2f}%, confidence={:.3f}%, sensitivity={:.3f}, robustness={:.3f}, step_sim={:.3f}\".format(best_arch, best_acc, best_conf, best_sensitivity, best_robust, best_step_sim))\n",
    "            target_cell.arch_cache = best_arch\n",
    "            \n",
    "best_archs = []\n",
    "for c in cells:\n",
    "    best_archs.append(c.arch_cache)\n",
    "    \n",
    "torch.save({\"model\":search_model.state_dict(), \"best_archs\":best_archs}, os.path.join(xargs.save_dir, \"output.pth\"))\n",
    "\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bc84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(rank_confidences,rank_accs)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(rank_sensitivities,rank_accs)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(rank_robustnesses,rank_accs)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(rank_step_sims,rank_accs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec008be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in cells:\n",
    "#     print(c.arch_cache)\n",
    "\n",
    "for m in search_model.modules():\n",
    "    if isinstance(m, SearchCell):\n",
    "        print(m.arch_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da32ec",
   "metadata": {},
   "source": [
    "# Train a found model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ae460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(args)\n",
    "trained_output = torch.load(os.path.join(xargs.save_dir, \"output.pth\"))\n",
    "args.save_dir = os.path.join(args.save_dir, 'train')\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed27598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c02f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = prepare_logger(args)\n",
    "\n",
    "cifar_train_config_path = \"./MY.config\"\n",
    "###\n",
    "train_data, valid_data, xshape, class_num = get_datasets(xargs.dataset, xargs.data_path, -1)\n",
    "config = load_config(cifar_train_config_path, {\"class_num\": class_num, \"xshape\": xshape}, logger)\n",
    "search_loader, _, valid_loader = get_nas_search_loaders(train_data,\n",
    "                                                        valid_data,\n",
    "                                                        xargs.dataset,\n",
    "                                                        \"../configs/nas-benchmark/\",\n",
    "                                                        (config.batch_size, config.batch_size),\n",
    "                                                        xargs.workers)\n",
    "logger.log(\"||||||| {:10s} ||||||| Search-Loader-Num={:}, Valid-Loader-Num={:}, batch size={:}\".format(\n",
    "            xargs.dataset, len(search_loader), len(valid_loader), config.batch_size))\n",
    "logger.log(\"||||||| {:10s} ||||||| Config={:}\".format(xargs.dataset, config))\n",
    "\n",
    "search_space = get_search_spaces(\"cell\", xargs.search_space_name)\n",
    "model_config = dict2config(\n",
    "    {\n",
    "        \"name\": \"RANDOM\",\n",
    "        \"C\": xargs.channel,\n",
    "        \"N\": xargs.num_cells,\n",
    "        \"max_nodes\": xargs.max_nodes,\n",
    "        \"num_classes\": class_num,\n",
    "        \"space\": search_space,\n",
    "        \"affine\": False,\n",
    "        \"track_running_stats\": True, # true for eval\n",
    "    },\n",
    "    None,\n",
    ")\n",
    "search_model = get_cell_based_tiny_net(model_config)\n",
    "\n",
    "### load\n",
    "# trained_output = torch.load(os.path.join(xargs.save_dir, \"./output.pth\"))\n",
    "search_model.load_state_dict(trained_output['model'], strict=False)\n",
    "best_archs = trained_output['best_archs']\n",
    "i=0\n",
    "for m in search_model.modules():\n",
    "    if isinstance(m, SearchCell):\n",
    "        m.arch_cache = best_archs[i]\n",
    "        i += 1\n",
    "for m in network.modules():\n",
    "    if isinstance(m, SearchCell):\n",
    "        print(m.arch_cache)\n",
    "###\n",
    "\n",
    "w_optimizer, w_scheduler, criterion = get_optim_scheduler(search_model.parameters(), config)\n",
    "\n",
    "logger.log(\"w-optimizer : {:}\".format(w_optimizer))\n",
    "logger.log(\"w-scheduler : {:}\".format(w_scheduler))\n",
    "logger.log(\"criterion   : {:}\".format(criterion))\n",
    "\n",
    "network, criterion = torch.nn.DataParallel(search_model).cuda(), criterion.cuda()\n",
    "\n",
    "last_info, model_base_path, model_best_path = (\n",
    "    logger.path(\"info\"),\n",
    "    logger.path(\"model\"),\n",
    "    logger.path(\"best\"),\n",
    ")\n",
    "\n",
    "start_epoch, valid_accuracies, genotypes = 0, {\"best\": -1}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964fcb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_func_one_arch(xloader, network, criterion, scheduler, w_optimizer, epoch_str, print_freq, logger):\n",
    "    data_time, batch_time = AverageMeter(), AverageMeter()\n",
    "    base_losses, base_top1, base_top5 = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    network.train()\n",
    "    end = time.time()\n",
    "    for step, (base_inputs, base_targets, arch_inputs, arch_targets) in enumerate(\n",
    "        xloader\n",
    "    ):\n",
    "        scheduler.update(None, 1.0 * step / len(xloader))\n",
    "        base_targets = base_targets.cuda(non_blocking=True)\n",
    "        arch_targets = arch_targets.cuda(non_blocking=True)\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        w_optimizer.zero_grad()\n",
    "        _, logits = network(base_inputs)\n",
    "        base_loss = criterion(logits, base_targets)\n",
    "        base_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(network.parameters(), 5)\n",
    "        w_optimizer.step()\n",
    "        # record\n",
    "        base_prec1, base_prec5 = obtain_accuracy(\n",
    "            logits.data, base_targets.data, topk=(1, 5)\n",
    "        )\n",
    "        base_losses.update(base_loss.item(), base_inputs.size(0))\n",
    "        base_top1.update(base_prec1.item(), base_inputs.size(0))\n",
    "        base_top5.update(base_prec5.item(), base_inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if step % print_freq == 0 or step + 1 == len(xloader):\n",
    "            Sstr = (\n",
    "                \"*SEARCH* \"\n",
    "                + time_string()\n",
    "                + \" [{:}][{:03d}/{:03d}]\".format(epoch_str, step, len(xloader))\n",
    "            )\n",
    "            Tstr = \"Time {batch_time.val:.2f} ({batch_time.avg:.2f}) Data {data_time.val:.2f} ({data_time.avg:.2f})\".format(\n",
    "                batch_time=batch_time, data_time=data_time\n",
    "            )\n",
    "            Wstr = \"Base [Loss {loss.val:.3f} ({loss.avg:.3f})  Prec@1 {top1.val:.2f} ({top1.avg:.2f}) Prec@5 {top5.val:.2f} ({top5.avg:.2f})]\".format(\n",
    "                loss=base_losses, top1=base_top1, top5=base_top5\n",
    "            )\n",
    "            logger.log(Sstr + \" \" + Tstr + \" \" + Wstr)\n",
    "    return base_losses.avg, base_top1.avg, base_top5.avg\n",
    "\n",
    "def valid_func_one_arch(xloader, network, criterion):\n",
    "    data_time, batch_time = AverageMeter(), AverageMeter()\n",
    "    arch_losses, arch_top1, arch_top5 = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    network.eval()\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for step, (arch_inputs, arch_targets) in enumerate(xloader):\n",
    "            arch_targets = arch_targets.cuda(non_blocking=True)\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "            # prediction\n",
    "\n",
    "#             network.module.random_genotype_per_cell(True)\n",
    "            _, logits = network(arch_inputs)\n",
    "            arch_loss = criterion(logits, arch_targets)\n",
    "            # record\n",
    "            arch_prec1, arch_prec5 = obtain_accuracy(\n",
    "                logits.data, arch_targets.data, topk=(1, 5)\n",
    "            )\n",
    "            arch_losses.update(arch_loss.item(), arch_inputs.size(0))\n",
    "            arch_top1.update(arch_prec1.item(), arch_inputs.size(0))\n",
    "            arch_top5.update(arch_prec5.item(), arch_inputs.size(0))\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "    return arch_losses.avg, arch_top1.avg, arch_top5.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be07a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time, search_time, epoch_time, total_epoch = (\n",
    "    time.time(),\n",
    "    AverageMeter(),\n",
    "    AverageMeter(),\n",
    "    config.epochs + config.warmup,\n",
    ")\n",
    "for epoch in range(0, total_epoch):\n",
    "    w_scheduler.update(epoch, 0.0)\n",
    "    need_time = \"Time Left: {:}\".format(\n",
    "        convert_secs2time(epoch_time.val * (total_epoch - epoch), True)\n",
    "    )\n",
    "    epoch_str = \"{:03d}-{:03d}\".format(epoch, total_epoch)\n",
    "    logger.log(\n",
    "        \"\\n[Search the {:}-th epoch] {:}, LR={:}\".format(\n",
    "            epoch_str, need_time, min(w_scheduler.get_lr())\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # selected_arch = search_find_best(valid_loader, network, criterion, xargs.select_num)\n",
    "    search_w_loss, search_w_top1, search_w_top5 = search_func_one_arch(\n",
    "        search_loader,\n",
    "        network,\n",
    "        criterion,\n",
    "        w_scheduler,\n",
    "        w_optimizer,\n",
    "        epoch_str,\n",
    "        xargs.print_freq,\n",
    "        logger,\n",
    "    )\n",
    "    search_time.update(time.time() - start_time)\n",
    "    logger.log(\n",
    "        \"[{:}] searching : loss={:.2f}, accuracy@1={:.2f}%, accuracy@5={:.2f}%, time-cost={:.1f} s\".format(\n",
    "            epoch_str, search_w_loss, search_w_top1, search_w_top5, search_time.sum\n",
    "        )\n",
    "    )\n",
    "    valid_a_loss, valid_a_top1, valid_a_top5 = valid_func_one_arch(\n",
    "        valid_loader, network, criterion\n",
    "    )\n",
    "    logger.log(\n",
    "        \"[{:}] evaluate  : loss={:.2f}, accuracy@1={:.2f}%, accuracy@5={:.2f}%\".format(\n",
    "            epoch_str, valid_a_loss, valid_a_top1, valid_a_top5\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # check the best accuracy\n",
    "    valid_accuracies[epoch] = valid_a_top1\n",
    "    if valid_a_top1 > valid_accuracies[\"best\"]:\n",
    "        valid_accuracies[\"best\"] = valid_a_top1\n",
    "        find_best = True\n",
    "    else:\n",
    "        find_best = False\n",
    "\n",
    "    # save checkpoint\n",
    "    save_path = save_checkpoint(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"args\": deepcopy(xargs),\n",
    "            \"search_model\": search_model.state_dict(),\n",
    "            \"w_optimizer\": w_optimizer.state_dict(),\n",
    "            \"w_scheduler\": w_scheduler.state_dict(),\n",
    "            \"genotypes\": genotypes,\n",
    "            \"valid_accuracies\": valid_accuracies,\n",
    "        },\n",
    "        model_base_path,\n",
    "        logger,\n",
    "    )\n",
    "    last_info = save_checkpoint(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"args\": deepcopy(args),\n",
    "            \"last_checkpoint\": save_path,\n",
    "        },\n",
    "        logger.path(\"info\"),\n",
    "        logger,\n",
    "    )\n",
    "    if find_best:\n",
    "        logger.log(\n",
    "            \"<<<--->>> The {:}-th epoch : find the highest validation accuracy : {:.2f}%.\".format(\n",
    "                epoch_str, valid_a_top1\n",
    "            )\n",
    "        )\n",
    "        copy_checkpoint(model_base_path, model_best_path, logger)\n",
    "    if api is not None:\n",
    "        logger.log(\"{:}\".format(api.query_by_arch(genotypes[epoch], \"200\")))\n",
    "    # measure elapsed time\n",
    "    epoch_time.update(time.time() - start_time)\n",
    "    start_time = time.time()\n",
    "\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d00afb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
