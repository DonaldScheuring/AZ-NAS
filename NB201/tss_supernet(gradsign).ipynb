{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cc63dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, glob, random, argparse\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import tqdm\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# XAutoDL \n",
    "from xautodl.config_utils import load_config, dict2config, configure2str\n",
    "from xautodl.datasets import get_datasets, get_nas_search_loaders\n",
    "from xautodl.procedures import (\n",
    "    prepare_seed,\n",
    "    prepare_logger,\n",
    "    save_checkpoint,\n",
    "    copy_checkpoint,\n",
    "    get_optim_scheduler,\n",
    ")\n",
    "from xautodl.utils import get_model_infos, obtain_accuracy\n",
    "from xautodl.log_utils import AverageMeter, time_string, convert_secs2time\n",
    "from xautodl.models import get_search_spaces\n",
    "\n",
    "# API\n",
    "from nats_bench import create\n",
    "\n",
    "# custom modules\n",
    "from custom.tss_model_supernet import get_cell_based_tiny_net\n",
    "from ZeroShotProxy import *\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6018e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"Training-free NAS on NATSBench (TSS)\")\n",
    "parser.add_argument(\"--data_path\", type=str, default='./cifar.python', help=\"The path to dataset\")\n",
    "parser.add_argument(\"--dataset\", type=str, default='cifar10',choices=[\"cifar10\", \"cifar100\", \"ImageNet16-120\"], help=\"Choose between Cifar10/100 and ImageNet-16.\")\n",
    "\n",
    "# channels and number-of-cells\n",
    "parser.add_argument(\"--search_space\", type=str, default='tss', help=\"The search space name.\")\n",
    "parser.add_argument(\"--config_path\", type=str, default='./configs/nas-benchmark/algos/weight-sharing.config', help=\"The path to the configuration.\")\n",
    "parser.add_argument(\"--max_nodes\", type=int, default=4, help=\"The maximum number of nodes.\")\n",
    "parser.add_argument(\"--channel\", type=int, default=16, help=\"The number of channels.\")\n",
    "parser.add_argument(\"--num_cells\", type=int, default=5, help=\"The number of cells in one stage.\")\n",
    "parser.add_argument(\"--affine\", type=int, default=0, choices=[0, 1], help=\"Whether use affine=True or False in the BN layer.\")\n",
    "parser.add_argument(\"--track_running_stats\", type=int, default=0, choices=[0, 1], help=\"Whether use track_running_stats or not in the BN layer.\")\n",
    "\n",
    "# log\n",
    "parser.add_argument(\"--print_freq\", type=int, default=200, help=\"print frequency (default: 200)\")\n",
    "\n",
    "# custom\n",
    "parser.add_argument(\"--gpu\", type=int, default=0, help=\"\")\n",
    "parser.add_argument(\"--workers\", type=int, default=2, help=\"number of data loading workers\")\n",
    "parser.add_argument(\"--api_data_path\", type=str, default=\"./api_data/NATS-tss-v1_0-3ffb9-simple/\", help=\"\")\n",
    "parser.add_argument(\"--save_dir\", type=str, default='./results/tmp', help=\"Folder to save checkpoints and log.\")\n",
    "parser.add_argument('--zero_shot_score', type=str, default='gradsign', choices=['az_nas','zico','zen','gradnorm','naswot','synflow','snip','grasp','te_nas','gradsign'])\n",
    "parser.add_argument(\"--rand_seed\", type=int, default=1, help=\"manual seed\")\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "if args.rand_seed is None or args.rand_seed < 0:\n",
    "    args.rand_seed = random.randint(1, 100000)\n",
    "\n",
    "print(args.rand_seed)\n",
    "print(args)\n",
    "xargs=args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a05319",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available(), \"CUDA is not available.\"\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.set_num_threads(xargs.workers)\n",
    "prepare_seed(xargs.rand_seed)\n",
    "logger = prepare_logger(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78557cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## API\n",
    "api = create(xargs.api_data_path, xargs.search_space, fast_mode=True, verbose=False)\n",
    "logger.log(\"Create API = {:} done\".format(api))\n",
    "\n",
    "## data\n",
    "train_data, valid_data, xshape, class_num = get_datasets(xargs.dataset, xargs.data_path, -1)\n",
    "config = load_config(xargs.config_path, {\"class_num\": class_num, \"xshape\": xshape}, logger)\n",
    "search_loader, train_loader, valid_loader = get_nas_search_loaders(train_data,\n",
    "                                                                   valid_data,\n",
    "                                                                   xargs.dataset,\n",
    "                                                                   \"./configs/nas-benchmark/\",\n",
    "                                                                   (config.batch_size, config.test_batch_size),\n",
    "                                                                   xargs.workers,)\n",
    "logger.log(\"||||||| {:10s} ||||||| Search-Loader-Num={:}, Valid-Loader-Num={:}, batch size={:}\".format(xargs.dataset, len(search_loader), len(valid_loader), config.batch_size))\n",
    "logger.log(\"||||||| {:10s} ||||||| Config={:}\".format(xargs.dataset, config))\n",
    "\n",
    "## model\n",
    "search_space = get_search_spaces(xargs.search_space, \"nats-bench\")\n",
    "model_config = dict2config(\n",
    "    dict(\n",
    "        name=\"generic\",\n",
    "        C=xargs.channel,\n",
    "        N=xargs.num_cells,\n",
    "        max_nodes=xargs.max_nodes,\n",
    "        num_classes=class_num,\n",
    "        space=search_space,\n",
    "        affine=bool(xargs.affine),\n",
    "        track_running_stats=bool(xargs.track_running_stats),\n",
    "    ),\n",
    "    None,\n",
    ")\n",
    "logger.log(\"search space : {:}\".format(search_space))\n",
    "logger.log(\"model config : {:}\".format(model_config))\n",
    "search_model = get_cell_based_tiny_net(model_config)\n",
    "\n",
    "device = torch.device('cuda:{}'.format(xargs.gpu))\n",
    "network = search_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c557c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_input_metrics = ['zico', 'snip', 'grasp', 'te_nas', 'gradsign']\n",
    "\n",
    "def search_find_best(xargs, xloader, network, n_samples = None, archs = None):\n",
    "    logger.log(\"Searching with {}\".format(xargs.zero_shot_score.lower()))\n",
    "    score_fn_name = \"compute_{}_score\".format(xargs.zero_shot_score.lower())\n",
    "    score_fn = globals().get(score_fn_name)\n",
    "    input_, target_ = next(iter(xloader))\n",
    "    resolution = input_.size(2)\n",
    "    batch_size = input_.size(0)\n",
    "    zero_shot_score_dict = None\n",
    "    arch_list = []\n",
    "    trainloader = train_loader if xargs.zero_shot_score.lower() in real_input_metrics else None\n",
    "    \n",
    "    if archs is None and n_samples is not None:\n",
    "        network.train()\n",
    "        all_time = []\n",
    "        all_mem = []\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        for i in tqdm.tqdm(range(n_samples)):\n",
    "            # random sampling\n",
    "            arch = network.random_genotype(True)\n",
    "\n",
    "            start.record()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "            info_dict = score_fn.compute_nas_score(network, gpu=xargs.gpu, trainloader=trainloader, resolution=resolution, batch_size=batch_size)\n",
    "\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            all_time.append(start.elapsed_time(end))\n",
    "#             all_mem.append(torch.cuda.max_memory_reserved())\n",
    "            all_mem.append(torch.cuda.max_memory_allocated())\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            arch_list.append(arch)\n",
    "            if zero_shot_score_dict is None: # initialize dict\n",
    "                zero_shot_score_dict = dict()\n",
    "                for k in info_dict.keys():\n",
    "                    zero_shot_score_dict[k] = []\n",
    "            for k, v in info_dict.items():\n",
    "                zero_shot_score_dict[k].append(v)\n",
    "\n",
    "        logger.log(\"------Runtime------\")\n",
    "        logger.log(\"All: {:.5f} ms\".format(np.mean(all_time)))\n",
    "        logger.log(\"------Avg Mem------\")\n",
    "        logger.log(\"All: {:.5f} GB\".format(np.mean(all_mem)/1e9))\n",
    "        logger.log(\"------Max Mem------\")\n",
    "        logger.log(\"All: {:.5f} GB\".format(np.max(all_mem)/1e9))\n",
    "        \n",
    "    elif archs is not None and n_samples is None:\n",
    "        network.train()\n",
    "        all_time = []\n",
    "        all_mem = []\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        for arch in tqdm.tqdm(archs):\n",
    "            network.arch_cache = arch\n",
    "\n",
    "            start.record()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "            info_dict = score_fn.compute_nas_score(network, gpu=xargs.gpu, trainloader=trainloader, resolution=resolution, batch_size=batch_size)\n",
    "\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            all_time.append(start.elapsed_time(end))\n",
    "#             all_mem.append(torch.cuda.max_memory_reserved())\n",
    "            all_mem.append(torch.cuda.max_memory_allocated())\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            arch_list.append(arch)\n",
    "            if zero_shot_score_dict is None: # initialize dict\n",
    "                zero_shot_score_dict = dict()\n",
    "                for k in info_dict.keys():\n",
    "                    zero_shot_score_dict[k] = []\n",
    "            for k, v in info_dict.items():\n",
    "                zero_shot_score_dict[k].append(v)\n",
    "\n",
    "        logger.log(\"------Runtime------\")\n",
    "        logger.log(\"All: {:.5f} ms\".format(np.mean(all_time)))\n",
    "        logger.log(\"------Avg Mem------\")\n",
    "        logger.log(\"All: {:.5f} GB\".format(np.mean(all_mem)/1e9))\n",
    "        logger.log(\"------Max Mem------\")\n",
    "        logger.log(\"All: {:.5f} GB\".format(np.max(all_mem)/1e9))\n",
    "        \n",
    "    return arch_list, zero_shot_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562c71e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### search across random N archs #########\n",
    "archs, results = search_find_best(xargs, train_loader, network, n_samples=3000)\n",
    "\n",
    "# ######### search across N archs uniformly sampled according to test acc. #########\n",
    "# def uniform_sample_archs(search_space, xargs, api, n_samples=1000, dataset='ImageNet16-120'):\n",
    "#     arch = random_genotype(xargs.max_nodes, search_space)\n",
    "#     search_space = get_search_spaces(xargs.search_space, \"nats-bench\")\n",
    "#     archs = arch.gen_all(search_space, xargs.max_nodes, False)\n",
    "    \n",
    "#     def get_results_from_api(api, arch, dataset='cifar10'):\n",
    "#         dataset_candidates = ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120']\n",
    "#         assert dataset in dataset_candidates\n",
    "#         index = api.query_index_by_arch(arch)\n",
    "#         api._prepare_info(index)\n",
    "#         archresult = api.arch2infos_dict[index]['200']\n",
    "#         if dataset == 'cifar10-valid':\n",
    "#             acc = archresult.get_metrics(dataset, 'x-valid', iepoch=None, is_random=False)['accuracy']\n",
    "#         elif dataset == 'cifar10':\n",
    "#             acc = archresult.get_metrics(dataset, 'ori-test', iepoch=None, is_random=False)['accuracy']\n",
    "#         else:\n",
    "#             acc = archresult.get_metrics(dataset, 'x-test', iepoch=None, is_random=False)['accuracy']\n",
    "#         return acc\n",
    "\n",
    "#     accs = []\n",
    "#     for a in archs:\n",
    "#         accs.append(get_results_from_api(api, a, dataset))\n",
    "#     interval = len(archs) // n_samples\n",
    "#     sorted_indices = np.argsort(accs)\n",
    "#     new_archs = []\n",
    "#     for i, idx in enumerate(sorted_indices):\n",
    "#         if i % interval == 0:\n",
    "#             new_archs.append(archs[idx])\n",
    "#     archs = new_archs\n",
    "    \n",
    "#     return archs\n",
    "\n",
    "# if os.path.exists(\"./tss_uniform_arch.pickle\"):\n",
    "#     with open(\"./tss_uniform_arch.pickle\", \"rb\") as fp:\n",
    "#         uniform_archs = pickle.load(fp)\n",
    "# else:\n",
    "#     uniform_archs = uniform_sample_archs(network, xargs, api, 1000, 'ImageNet16-120')\n",
    "#     with open(\"./tss_uniform_arch.pickle\", \"wb\") as fp:\n",
    "#         pickle.dump(uniform_archs, fp)\n",
    "\n",
    "# result_path = \"./{}_uniform_arch.pickle\".format(args.zero_shot_score)\n",
    "# if os.path.exists(result_path):\n",
    "#     print(\"results already exists\")\n",
    "#     with open(result_path, \"rb\") as fp:\n",
    "#         results = pickle.load(fp)\n",
    "#     archs = uniform_archs\n",
    "# else:\n",
    "#     archs, results = search_find_best(xargs, train_loader, network, archs=uniform_archs)\n",
    "#     with open(result_path, \"wb\") as fp:\n",
    "#         pickle.dump(results, fp)\n",
    "\n",
    "\n",
    "# ######### search across all archs #########\n",
    "# def generate_all_archs(network, xargs):\n",
    "#     arch = network.random_genotype(True)\n",
    "#     search_space = get_search_spaces(xargs.search_space, \"nats-bench\")\n",
    "#     archs = arch.gen_all(search_space, xargs.max_nodes, False)\n",
    "#     return archs\n",
    "\n",
    "# if os.path.exists(\"./tss_all_arch.pickle\"):\n",
    "#     with open(\"./tss_all_arch.pickle\", \"rb\") as fp:\n",
    "#         all_archs = pickle.load(fp)\n",
    "# else:\n",
    "#     all_archs = generate_all_archs(search_space, xargs)\n",
    "#     with open(\"./tss_all_arch.pickle\", \"wb\") as fp:\n",
    "#         pickle.dump(all_archs, fp)\n",
    "\n",
    "# # archs, results = search_find_best(xargs, train_loader, network, archs=all_archs)\n",
    "\n",
    "# ####\n",
    "# result_path = \"./{}_all_arch.pickle\".format(args.zero_shot_score)\n",
    "# if os.path.exists(result_path):\n",
    "#     print(\"results already exists\")\n",
    "#     with open(result_path, \"rb\") as fp:\n",
    "#         results = pickle.load(fp)\n",
    "#     archs = all_archs\n",
    "# else:\n",
    "#     archs, results = search_find_best(xargs, train_loader, network, archs=all_archs)\n",
    "#     with open(result_path, \"wb\") as fp:\n",
    "#         pickle.dump(results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4147a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_from_api(api, arch, dataset='cifar10'):\n",
    "    dataset_candidates = ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120']\n",
    "    assert dataset in dataset_candidates\n",
    "    index = api.query_index_by_arch(arch)\n",
    "    api._prepare_info(index)\n",
    "    archresult = api.arch2infos_dict[index]['200']\n",
    "    \n",
    "    if dataset == 'cifar10-valid':\n",
    "        acc = archresult.get_metrics(dataset, 'x-valid', iepoch=None, is_random=False)['accuracy']\n",
    "    elif dataset == 'cifar10':\n",
    "        acc = archresult.get_metrics(dataset, 'ori-test', iepoch=None, is_random=False)['accuracy']\n",
    "    else:\n",
    "        acc = archresult.get_metrics(dataset, 'x-test', iepoch=None, is_random=False)['accuracy']\n",
    "    flops = archresult.get_compute_costs(dataset)['flops']\n",
    "    params = archresult.get_compute_costs(dataset)['params']\n",
    "    \n",
    "    return acc, flops, params\n",
    "\n",
    "api_valid_accs, api_flops, api_params = [], [], []\n",
    "for a in archs:\n",
    "#     valid_acc, flops, params = get_results_from_api(api, a, 'cifar10')\n",
    "#     valid_acc, flops, params = get_results_from_api(api, a, 'cifar100')\n",
    "    valid_acc, flops, params = get_results_from_api(api, a, 'ImageNet16-120')\n",
    "    api_valid_accs.append(valid_acc)\n",
    "    api_flops.append(flops)\n",
    "    api_params.append(params)\n",
    "    \n",
    "print(\"Maximum acc: {}% \\n Info\".format(np.max(api_valid_accs)))\n",
    "best_idx = np.argmax(api_valid_accs)\n",
    "best_arch = archs[best_idx]\n",
    "if api is not None:\n",
    "    print(\"{:}\".format(api.query_by_arch(best_arch, \"200\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e525a88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig_scale = 1.1\n",
    "\n",
    "if xargs.zero_shot_score.lower() == 'az_nas':\n",
    "    rank_agg = None\n",
    "    l = len(api_flops)\n",
    "    rank_agg = np.log(stats.rankdata(api_flops) / l)\n",
    "    for k in results.keys():\n",
    "        print(k)\n",
    "        if rank_agg is None:\n",
    "            rank_agg = np.log( stats.rankdata(results[k]) / l)\n",
    "        else:\n",
    "            rank_agg = rank_agg + np.log( stats.rankdata(results[k]) / l)\n",
    "\n",
    "\n",
    "    best_idx = np.argmax(rank_agg)\n",
    "\n",
    "    best_arch, acc = archs[best_idx], api_valid_accs[best_idx]\n",
    "    if api is not None:\n",
    "        print(\"{:}\".format(api.query_by_arch(best_arch, \"200\")))\n",
    "\n",
    "\n",
    "    x = stats.rankdata(rank_agg)\n",
    "    y = stats.rankdata(api_valid_accs)\n",
    "    kendalltau = stats.kendalltau(x, y)\n",
    "    spearmanr = stats.spearmanr(x, y)\n",
    "    pearsonr = stats.pearsonr(x, y)\n",
    "    print(\"aggregated: {}\\t{}\\t{}\\t\".format(kendalltau[0], pearsonr[0], spearmanr[0]))\n",
    "    plt.figure(figsize=(4*fig_scale,3*fig_scale))\n",
    "    plt.scatter(x, y, linewidths=0.1)\n",
    "    best_idx = np.argmax(rank_agg)\n",
    "    plt.scatter(x[best_idx], y[best_idx], c=\"r\", linewidths=0.1)\n",
    "    plt.title(\"AZ-NAS\")\n",
    "    plt.show()\n",
    "    \n",
    "elif xargs.zero_shot_score.lower() == 'te_nas':\n",
    "    rank_agg = None\n",
    "    for k in results.keys():\n",
    "        print(k)\n",
    "        if rank_agg is None:\n",
    "            rank_agg = stats.rankdata(results[k])\n",
    "        else:\n",
    "            rank_agg = rank_agg + stats.rankdata(results[k])\n",
    "\n",
    "\n",
    "    best_idx = np.argmax(rank_agg)\n",
    "\n",
    "    best_arch, acc = archs[best_idx], api_valid_accs[best_idx]\n",
    "    if api is not None:\n",
    "        print(\"{:}\".format(api.query_by_arch(best_arch, \"200\")))\n",
    "\n",
    "\n",
    "    x = stats.rankdata(rank_agg)\n",
    "    y = stats.rankdata(api_valid_accs)\n",
    "    kendalltau = stats.kendalltau(x, y)\n",
    "    spearmanr = stats.spearmanr(x, y)\n",
    "    pearsonr = stats.pearsonr(x, y)\n",
    "    print(\"aggregated: {}\\t{}\\t{}\\t\".format(kendalltau[0], pearsonr[0], spearmanr[0]))\n",
    "    plt.figure(figsize=(4*fig_scale,3*fig_scale))\n",
    "    plt.scatter(x, y, linewidths=0.1)\n",
    "    best_idx = np.argmax(rank_agg)\n",
    "    plt.scatter(x[best_idx], y[best_idx], c=\"r\", linewidths=0.1)\n",
    "    plt.title(\"Rank_agg_te_nas\")\n",
    "    plt.show()\n",
    "\n",
    "# for each\n",
    "# metrics = {'FLOPs':api_flops, 'Params':api_params}\n",
    "metrics = {}\n",
    "for k, v in results.items():\n",
    "    metrics[k] = v\n",
    "\n",
    "    print(k)\n",
    "    best_idx = np.argmax(v)\n",
    "\n",
    "    best_arch, acc = archs[best_idx], api_valid_accs[best_idx]\n",
    "    if api is not None:\n",
    "        print(\"{:}\".format(api.query_by_arch(best_arch, \"200\")))\n",
    "\n",
    "\n",
    "for k in metrics.keys():\n",
    "    x = stats.rankdata(metrics[k])\n",
    "    y = stats.rankdata(api_valid_accs)\n",
    "    kendalltau = stats.kendalltau(x, y)\n",
    "    spearmanr = stats.spearmanr(x, y)\n",
    "    pearsonr = stats.pearsonr(x, y)\n",
    "    print(\"{}: {}\\t{}\\t{}\\t\".format(k, kendalltau[0], pearsonr[0], spearmanr[0]))\n",
    "    plt.figure(figsize=(4*fig_scale,3*fig_scale))\n",
    "    plt.scatter(x, y, linewidths=0.1)\n",
    "    best_idx = np.argmax(metrics[k])\n",
    "    plt.scatter(x[best_idx], y[best_idx], c=\"r\", linewidths=0.1)\n",
    "    plt.title(\"Rank_{}\".format(k))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6319bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
