{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf03b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import json\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from timm.data import Mixup\n",
    "from timm.models import create_model\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "from timm.scheduler import create_scheduler\n",
    "from timm.optim import create_optimizer\n",
    "from timm.utils import NativeScaler\n",
    "from lib.datasets import build_dataset\n",
    "from lib.samplers import RASampler\n",
    "from lib import utils\n",
    "from lib.config import cfg, update_config_from_file\n",
    "from model.autoformer_space import Vision_TransformerSuper\n",
    "\n",
    "import random, os\n",
    "from timm.utils.model import unwrap_model\n",
    "\n",
    "# from lib.training_free.indicators.etf import compute_nas_score\n",
    "from lib.training_free import *\n",
    "\n",
    "import tqdm\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6afcf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The seed number is set to 0\n",
      "Namespace(batch_size=64, relative_position=True, gp=True, change_qkv=True, max_relative_position=14, mode='retrain', input_size=224, patch_size=16, drop=0.0, drop_path=0.1, no_abs_pos=False, data_path='../../../../dataset/ILSVRC2012/', data_set='IMNET', num_workers=16, pin_mem=True, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', reprob=0.25, remode='pixel', recount=1, resplit=False, cfg='./experiments/search_space/space-T.yaml', param_limits=7, min_param_limits=5, num_archs=1000, seed=0, repeated_aug=True, amp=True)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser('Sampling archs from pretrained AutoFormer supernets', add_help=False)\n",
    "parser.add_argument('--batch-size', default=64, type=int)\n",
    "\n",
    "# custom parameters\n",
    "parser.add_argument('--relative_position', type=bool, default=True)\n",
    "parser.add_argument('--gp', type=bool, default=True)\n",
    "parser.add_argument('--change_qkv', type=bool, default=True)\n",
    "parser.add_argument('--max_relative_position', type=int, default=14, help='max distance in relative position embedding')\n",
    "\n",
    "# AutoFormer config\n",
    "parser.add_argument('--mode', type=str, default='retrain', choices=['super', 'retrain'], help='mode of AutoFormer')\n",
    "parser.add_argument('--input-size', default=224, type=int)\n",
    "parser.add_argument('--patch_size', default=16, type=int)\n",
    "\n",
    "parser.add_argument('--drop', type=float, default=0.0, metavar='PCT', help='Dropout rate (default: 0.)')\n",
    "parser.add_argument('--drop-path', type=float, default=0.1, metavar='PCT', help='Drop path rate (default: 0.1)')\n",
    "# parser.add_argument('--drop-path', type=float, default=0, metavar='PCT', help='Drop path rate (default: 0.1)')\n",
    "parser.add_argument('--no_abs_pos', type=bool, default=False)\n",
    "\n",
    "# Dataset parameters\n",
    "parser.add_argument('--data-path', default='../../../../dataset/ILSVRC2012/', type=str, help='dataset path')\n",
    "parser.add_argument('--data-set', default='IMNET', choices=['CIFAR', 'IMNET', 'INAT', 'INAT19'],\n",
    "                        type=str, help='Image Net dataset path')\n",
    "parser.add_argument('--num_workers', default=16, type=int)\n",
    "parser.add_argument('--pin-mem', type=bool, default=True, help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "\n",
    "parser.add_argument('--color-jitter', type=float, default=0.4, metavar='PCT', help='Color jitter factor (default: 0.4)')\n",
    "# parser.add_argument('--color-jitter', type=float, default=0, metavar='PCT', help='Color jitter factor (default: 0.4)')\n",
    "parser.add_argument('--aa', type=str, default='rand-m9-mstd0.5-inc1', metavar='NAME', help='Use AutoAugment policy. \"v0\" or \"original\". \" + \"(default: rand-m9-mstd0.5-inc1)'),\n",
    "# parser.add_argument('--aa', type=str, default='original', metavar='NAME', help='Use AutoAugment policy. \"v0\" or \"original\". \" + \"(default: rand-m9-mstd0.5-inc1)'),\n",
    "parser.add_argument('--smoothing', type=float, default=0.1, help='Label smoothing (default: 0.1)')\n",
    "parser.add_argument('--train-interpolation', type=str, default='bicubic', help='Training interpolation (random, bilinear, bicubic default: \"bicubic\")')\n",
    "parser.set_defaults(repeated_aug=True)\n",
    "\n",
    "parser.add_argument('--reprob', type=float, default=0.25, metavar='PCT', help='Random erase prob (default: 0.25)')\n",
    "# parser.add_argument('--reprob', type=float, default=0, metavar='PCT', help='Random erase prob (default: 0.25)')\n",
    "parser.add_argument('--remode', type=str, default='pixel', help='Random erase mode (default: \"pixel\")')\n",
    "parser.add_argument('--recount', type=int, default=1, help='Random erase count (default: 1)')\n",
    "parser.add_argument('--resplit', action='store_true', default=False, help='Do not random erase first (clean) augmentation split')\n",
    "\n",
    "# config file\n",
    "parser.add_argument('--cfg', default='./experiments/search_space/space-T.yaml' ,type=str)\n",
    "parser.add_argument('--param-limits', type=float, default=7)\n",
    "parser.add_argument('--min-param-limits', type=float, default=5)\n",
    "parser.add_argument('--num-archs', type=int, default=1000)\n",
    "parser.add_argument('--seed', type=int, default=0)\n",
    "\n",
    "parser.set_defaults(amp=True)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "update_config_from_file(args.cfg)\n",
    "\n",
    "if args.seed is not None:\n",
    "    print(\"The seed number is set to {}\".format(args.seed))\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(args.seed)\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9a21eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vision_TransformerSuper(\n",
       "  (patch_embed_super): PatchembedSuper(\n",
       "    (proj): Conv2d(3, 256, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0): TransformerEncoderLayer(\n",
       "      (drop_path): Identity()\n",
       "      (attn): AttentionSuper(\n",
       "        (qkv): qkv_super(in_features=256, out_features=768, bias=True)\n",
       "        (rel_pos_embed_k): RelativePosition2D_super()\n",
       "        (rel_pos_embed_v): RelativePosition2D_super()\n",
       "        (proj): LinearSuper(in_features=256, out_features=256, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (attn_layer_norm): LayerNormSuper((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn_layer_norm): LayerNormSuper((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (activation_fn): GELU(approximate='none')\n",
       "      (fc1): LinearSuper(in_features=256, out_features=1024, bias=True)\n",
       "      (fc2): LinearSuper(in_features=1024, out_features=256, bias=True)\n",
       "    )\n",
       "    (1-13): 13 x TransformerEncoderLayer(\n",
       "      (drop_path): DropPath()\n",
       "      (attn): AttentionSuper(\n",
       "        (qkv): qkv_super(in_features=256, out_features=768, bias=True)\n",
       "        (rel_pos_embed_k): RelativePosition2D_super()\n",
       "        (rel_pos_embed_v): RelativePosition2D_super()\n",
       "        (proj): LinearSuper(in_features=256, out_features=256, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (attn_layer_norm): LayerNormSuper((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn_layer_norm): LayerNormSuper((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (activation_fn): GELU(approximate='none')\n",
       "      (fc1): LinearSuper(in_features=256, out_features=1024, bias=True)\n",
       "      (fc2): LinearSuper(in_features=1024, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNormSuper((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): LinearSuper(in_features=256, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataloader\n",
    "dataset_train, args.nb_classes = build_dataset(is_train=True, args=args)\n",
    "sampler_train = torch.utils.data.SequentialSampler(dataset_train)\n",
    "\n",
    "data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=int(args.batch_size),\n",
    "                                                sampler=sampler_train, num_workers=args.num_workers,\n",
    "                                                pin_memory=args.pin_mem, drop_last=True)\n",
    "train_samples = []\n",
    "max_batches = 1\n",
    "for i, sample in enumerate(data_loader_train):\n",
    "    if i == max_batches:\n",
    "        break\n",
    "    train_samples.append(sample)\n",
    "\n",
    "\n",
    "# model\n",
    "model = Vision_TransformerSuper(img_size=args.input_size,\n",
    "                                patch_size=args.patch_size,\n",
    "                                embed_dim=cfg.SUPERNET.EMBED_DIM, depth=cfg.SUPERNET.DEPTH,\n",
    "                                num_heads=cfg.SUPERNET.NUM_HEADS,mlp_ratio=cfg.SUPERNET.MLP_RATIO,\n",
    "                                qkv_bias=True, drop_rate=args.drop,\n",
    "                                drop_path_rate=args.drop_path,\n",
    "                                gp=args.gp,\n",
    "                                num_classes=args.nb_classes,\n",
    "                                max_relative_position=args.max_relative_position,\n",
    "                                relative_position=args.relative_position,\n",
    "                                change_qkv=args.change_qkv, abs_pos=not args.no_abs_pos)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7a7f8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['archs', 'params', 'flops', 'acc'])\n"
     ]
    }
   ],
   "source": [
    "with open('./Extract_SuperNet_Proxy/Tiny_5M-7M_1000samples.pkl', 'rb') as f:\n",
    "    proxies = pickle.load(f)\n",
    "    \n",
    "print(proxies.keys())\n",
    "\n",
    "# for k in proxies.keys():\n",
    "#     proxies[k] = proxies[k][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d090c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/home/junghyup/nas/nasbench201-random/random_my/ImageNet_AutoFormer/lib/training_free/indicators/dss.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343967769/work/build/aten/src/ATen/core/TensorBody.h:486.)\n",
      "  if layer.samples['weight'].grad is not None:\n",
      "/home/junghyup/nas/nasbench201-random/random_my/ImageNet_AutoFormer/lib/training_free/indicators/dss.py:49: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343967769/work/build/aten/src/ATen/core/TensorBody.h:486.)\n",
      "  if layer.samples['weight'].grad is not None:\n",
      " 77%|███████▋  | 770/1000 [31:20<09:34,  2.50s/it]"
     ]
    }
   ],
   "source": [
    "num_archs = len(proxies['archs'])\n",
    "\n",
    "results=None\n",
    "\n",
    "for i in tqdm.tqdm(range(num_archs)):\n",
    "    config = proxies['archs'][i]\n",
    "\n",
    "    model_module = unwrap_model(model)\n",
    "    model_module.set_sample_config(config=config)\n",
    "    params = model.get_sampled_params_numel(config)\n",
    "    p = params/10**6\n",
    "    if p > args.param_limits or p < args.min_param_limits:\n",
    "        raise RuntimeError\n",
    "\n",
    "    indicators = compute_indicators.find_indicators(model, data_loader_train, ('random', 1, 1000), device)\n",
    "    \n",
    "    if results is None:\n",
    "        results = {}\n",
    "        for k in indicators.keys():\n",
    "            results[k] = []\n",
    "    for k, v in indicators.items():\n",
    "        results[k].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124b8c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.keys())\n",
    "rank_agg = results['dss']\n",
    "\n",
    "proxy_acc = proxies['acc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f128ecd9",
   "metadata": {},
   "source": [
    "- Real training image + Attention temperature 0.001 + Omit progressivity + Normalize PCA score by upper bound = 0.543\n",
    "- Real training image + Attention temperature 0.001 + Omit progressivity + PCA score w/o normalize = 0.543\n",
    "- DSS = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8730beb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig_scale = 1.1\n",
    "\n",
    "best_idx = np.argmax(rank_agg)\n",
    "\n",
    "best_arch, acc = proxies['archs'][best_idx], proxies['acc'][best_idx]\n",
    "print(acc)\n",
    "print(max(proxies['acc']))\n",
    "\n",
    "# x = stats.rankdata(rank_agg)\n",
    "# y = stats.rankdata(proxies['acc'])\n",
    "x = rank_agg\n",
    "y = proxies['acc']\n",
    "kendalltau = stats.kendalltau(x, y)\n",
    "spearmanr = stats.spearmanr(x, y)\n",
    "pearsonr = stats.pearsonr(x, y)\n",
    "print(\"aggregated: {}\\t{}\\t{}\\t\".format(kendalltau[0], pearsonr[0], spearmanr[0]))\n",
    "plt.figure(figsize=(4*fig_scale,3*fig_scale))\n",
    "plt.scatter(x, y, linewidths=0.1)\n",
    "plt.scatter(x[best_idx], y[best_idx], c=\"r\", linewidths=0.1)\n",
    "plt.title(\"Rank_agg\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# for each\n",
    "metrics = {'FLOPs':proxies['flops'], 'Params':proxies['params']}\n",
    "for k, v in results.items():\n",
    "    metrics[k] = v\n",
    "for k in metrics.keys():\n",
    "    x = stats.rankdata(metrics[k])\n",
    "    y = stats.rankdata(proxies['acc'])\n",
    "#     x = metrics[k]\n",
    "#     y = proxies['acc']\n",
    "    kendalltau = stats.kendalltau(x, y)\n",
    "    spearmanr = stats.spearmanr(x, y)\n",
    "    pearsonr = stats.pearsonr(x, y)\n",
    "    print(\"{}: {}\\t{}\\t{}\\t\".format(k, kendalltau[0], pearsonr[0], spearmanr[0]))\n",
    "    plt.figure(figsize=(4*fig_scale,3*fig_scale))\n",
    "    plt.scatter(x, y, linewidths=0.1)\n",
    "    plt.scatter(x[best_idx], y[best_idx], c=\"r\", linewidths=0.1)\n",
    "    plt.title(\"Rank_{}\".format(k))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0939c38f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
