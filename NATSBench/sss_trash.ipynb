{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4cc63dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 04:11:40.520746: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-10 04:11:40.565893: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-10 04:11:41.315883: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os, sys, time, glob, random, argparse\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import tqdm\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# XAutoDL \n",
    "from xautodl.config_utils import load_config, dict2config, configure2str\n",
    "from xautodl.datasets import get_datasets, get_nas_search_loaders\n",
    "from xautodl.procedures import (\n",
    "    prepare_seed,\n",
    "    prepare_logger,\n",
    "    save_checkpoint,\n",
    "    copy_checkpoint,\n",
    "    get_optim_scheduler,\n",
    ")\n",
    "from xautodl.utils import get_model_infos, obtain_accuracy\n",
    "from xautodl.log_utils import AverageMeter, time_string, convert_secs2time\n",
    "from xautodl.models import get_search_spaces\n",
    "\n",
    "# API\n",
    "from nats_bench import create\n",
    "\n",
    "# custom modules\n",
    "from custom.sss_model import get_cell_based_tiny_net\n",
    "from ZeroShotProxy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6018e93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18875\n",
      "Namespace(data_path='./cifar.python', dataset='cifar10', search_space='sss', config_path='./configs/nas-benchmark/algos/weight-sharing.config', genotype='|nor_conv_3x3~0|+|nor_conv_3x3~0|nor_conv_3x3~1|+|skip_connect~0|nor_conv_3x3~1|nor_conv_3x3~2|', affine=0, track_running_stats=0, algo='training-free', print_freq=200, gpu=0, workers=2, api_data_path='./api_data/NATS-sss-v1_0-50262-simple/', save_dir='./results/tmp_sss', zero_shot_score='ETF62', rand_seed=18875)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(\"Training-free NAS on NATSBench (SSS)\")\n",
    "parser.add_argument(\"--data_path\", type=str, default='./cifar.python', help=\"The path to dataset\")\n",
    "parser.add_argument(\"--dataset\", type=str, default='cifar10',choices=[\"cifar10\", \"cifar100\", \"ImageNet16-120\"], help=\"Choose between Cifar10/100 and ImageNet-16.\")\n",
    "\n",
    "# channels and number-of-cells\n",
    "parser.add_argument(\"--search_space\", type=str, default='sss', help=\"The search space name.\")\n",
    "parser.add_argument(\"--config_path\", type=str, default='./configs/nas-benchmark/algos/weight-sharing.config', help=\"The path to the configuration.\")\n",
    "parser.add_argument(\"--genotype\", type=str, default=\"|nor_conv_3x3~0|+|nor_conv_3x3~0|nor_conv_3x3~1|+|skip_connect~0|nor_conv_3x3~1|nor_conv_3x3~2|\", help=\"The genotype.\")\n",
    "parser.add_argument(\"--affine\", type=int, default=0, choices=[0, 1], help=\"Whether use affine=True or False in the BN layer.\")\n",
    "parser.add_argument(\"--track_running_stats\", type=int, default=0, choices=[0, 1], help=\"Whether use track_running_stats or not in the BN layer.\")\n",
    "parser.add_argument(\"--algo\", type=str, default=\"training-free\", help=\"\")\n",
    "\n",
    "\n",
    "# log\n",
    "parser.add_argument(\"--print_freq\", type=int, default=200, help=\"print frequency (default: 200)\")\n",
    "\n",
    "# custom\n",
    "parser.add_argument(\"--gpu\", type=int, default=0, help=\"\")\n",
    "parser.add_argument(\"--workers\", type=int, default=2, help=\"number of data loading workers\")\n",
    "parser.add_argument(\"--api_data_path\", type=str, default=\"./api_data/NATS-sss-v1_0-50262-simple/\", help=\"\")\n",
    "parser.add_argument(\"--save_dir\", type=str, default='./results/tmp_sss', help=\"Folder to save checkpoints and log.\")\n",
    "parser.add_argument('--zero_shot_score', type=str, default='ETF62', help='could be: ETF; ZiCo (form ZiCo); params, (for \\#Params); Zen (for Zen-NAS); TE (for TE-NAS)')\n",
    "parser.add_argument(\"--rand_seed\", type=int, default=None, help=\"manual seed\")\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "if args.rand_seed is None or args.rand_seed < 0:\n",
    "    args.rand_seed = random.randint(1, 100000)\n",
    "\n",
    "print(args.rand_seed)\n",
    "print(args)\n",
    "xargs=args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8a05319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Function with logger : Logger(dir=results/tmp_sss, use-tf=False, writer=None)\n",
      "Arguments : -------------------------------\n",
      "data_path        : ./cifar.python\n",
      "dataset          : cifar10\n",
      "search_space     : sss\n",
      "config_path      : ./configs/nas-benchmark/algos/weight-sharing.config\n",
      "genotype         : |nor_conv_3x3~0|+|nor_conv_3x3~0|nor_conv_3x3~1|+|skip_connect~0|nor_conv_3x3~1|nor_conv_3x3~2|\n",
      "affine           : 0\n",
      "track_running_stats : 0\n",
      "algo             : training-free\n",
      "print_freq       : 200\n",
      "gpu              : 0\n",
      "workers          : 2\n",
      "api_data_path    : ./api_data/NATS-sss-v1_0-50262-simple/\n",
      "save_dir         : ./results/tmp_sss\n",
      "zero_shot_score  : ETF62\n",
      "rand_seed        : 18875\n",
      "Python  Version  : 3.10.9 (main, Mar  8 2023, 10:47:38) [GCC 11.2.0]\n",
      "Pillow  Version  : 9.4.0\n",
      "PyTorch Version  : 2.0.0\n",
      "cuDNN   Version  : 8500\n",
      "CUDA available   : True\n",
      "CUDA GPU numbers : 2\n",
      "CUDA_VISIBLE_DEVICES : None\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available(), \"CUDA is not available.\"\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.set_num_threads(xargs.workers)\n",
    "prepare_seed(xargs.rand_seed)\n",
    "logger = prepare_logger(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78557cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create API = NATSsize(0/32768 architectures, fast_mode=True, file=) done\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "./configs/nas-benchmark/algos/weight-sharing.config\n",
      "Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=100, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, test_batch_size=512, class_num=10, xshape=(1, 3, 32, 32))\n",
      "||||||| cifar10    ||||||| Search-Loader-Num=391, Valid-Loader-Num=49, batch size=64\n",
      "||||||| cifar10    ||||||| Config=Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=100, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, test_batch_size=512, class_num=10, xshape=(1, 3, 32, 32))\n",
      "search space : {'candidates': [8, 16, 24, 32, 40, 48, 56, 64], 'numbers': 5}\n",
      "model config : Configure(name='generic', super_type='search-shape', candidate_Cs=[8, 16, 24, 32, 40, 48, 56, 64], max_num_Cs=5, num_classes=10, genotype='|nor_conv_3x3~0|+|nor_conv_3x3~0|nor_conv_3x3~1|+|skip_connect~0|nor_conv_3x3~1|nor_conv_3x3~2|', affine=False, track_running_stats=False)\n",
      "GenericNAS301Model(\n",
      "  GenericNAS301Model(candidates=[8, 16, 24, 32, 40, 48, 56, 64], num=5, N=1, L=6)\n",
      "  (_cells): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (1): InferCell(\n",
      "      info :: nodes=4, inC=64, outC=64, [1<-(I0-L0) | 2<-(I0-L1,I1-L2) | 3<-(I0-L3,I1-L4,I2-L5)], |nor_conv_3x3~0|+|nor_conv_3x3~0|nor_conv_3x3~1|+|skip_connect~0|nor_conv_3x3~1|nor_conv_3x3~2|\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x ReLUConvBN(\n",
      "          (op): Sequential(\n",
      "            (0): ReLU()\n",
      "            (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          )\n",
      "        )\n",
      "        (3): Identity()\n",
      "        (4-5): 2 x ReLUConvBN(\n",
      "          (op): Sequential(\n",
      "            (0): ReLU()\n",
      "            (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): ResNetBasicblock(\n",
      "      ResNetBasicblock(inC=64, outC=64, stride=2)\n",
      "      (conv_a): ReLUConvBN(\n",
      "        (op): Sequential(\n",
      "          (0): ReLU()\n",
      "          (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (conv_b): ReLUConvBN(\n",
      "        (op): Sequential(\n",
      "          (0): ReLU()\n",
      "          (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "        (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (3): InferCell(\n",
      "      info :: nodes=4, inC=64, outC=64, [1<-(I0-L0) | 2<-(I0-L1,I1-L2) | 3<-(I0-L3,I1-L4,I2-L5)], |nor_conv_3x3~0|+|nor_conv_3x3~0|nor_conv_3x3~1|+|skip_connect~0|nor_conv_3x3~1|nor_conv_3x3~2|\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x ReLUConvBN(\n",
      "          (op): Sequential(\n",
      "            (0): ReLU()\n",
      "            (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          )\n",
      "        )\n",
      "        (3): Identity()\n",
      "        (4-5): 2 x ReLUConvBN(\n",
      "          (op): Sequential(\n",
      "            (0): ReLU()\n",
      "            (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): ResNetBasicblock(\n",
      "      ResNetBasicblock(inC=64, outC=64, stride=2)\n",
      "      (conv_a): ReLUConvBN(\n",
      "        (op): Sequential(\n",
      "          (0): ReLU()\n",
      "          (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (conv_b): ReLUConvBN(\n",
      "        (op): Sequential(\n",
      "          (0): ReLU()\n",
      "          (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "        (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (5): InferCell(\n",
      "      info :: nodes=4, inC=64, outC=64, [1<-(I0-L0) | 2<-(I0-L1,I1-L2) | 3<-(I0-L3,I1-L4,I2-L5)], |nor_conv_3x3~0|+|nor_conv_3x3~0|nor_conv_3x3~1|+|skip_connect~0|nor_conv_3x3~1|nor_conv_3x3~2|\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x ReLUConvBN(\n",
      "          (op): Sequential(\n",
      "            (0): ReLU()\n",
      "            (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          )\n",
      "        )\n",
      "        (3): Identity()\n",
      "        (4-5): 2 x ReLUConvBN(\n",
      "          (op): Sequential(\n",
      "            (0): ReLU()\n",
      "            (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lastact): Sequential(\n",
      "    (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (global_pooling): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## API\n",
    "api = create(xargs.api_data_path, xargs.search_space, fast_mode=True, verbose=False)\n",
    "logger.log(\"Create API = {:} done\".format(api))\n",
    "\n",
    "## data\n",
    "train_data, valid_data, xshape, class_num = get_datasets(xargs.dataset, xargs.data_path, -1)\n",
    "config = load_config(xargs.config_path, {\"class_num\": class_num, \"xshape\": xshape}, logger)\n",
    "search_loader, train_loader, valid_loader = get_nas_search_loaders(train_data,\n",
    "                                                                   valid_data,\n",
    "                                                                   xargs.dataset,\n",
    "                                                                   \"./configs/nas-benchmark/\",\n",
    "                                                                   (config.batch_size, config.test_batch_size),\n",
    "                                                                   xargs.workers,)\n",
    "logger.log(\"||||||| {:10s} ||||||| Search-Loader-Num={:}, Valid-Loader-Num={:}, batch size={:}\".format(xargs.dataset, len(search_loader), len(valid_loader), config.batch_size))\n",
    "logger.log(\"||||||| {:10s} ||||||| Config={:}\".format(xargs.dataset, config))\n",
    "\n",
    "## model\n",
    "search_space = get_search_spaces(xargs.search_space, \"nats-bench\")\n",
    "model_config = dict2config(\n",
    "        dict(\n",
    "            name=\"generic\",\n",
    "            super_type=\"search-shape\",\n",
    "            candidate_Cs=search_space[\"candidates\"],\n",
    "            max_num_Cs=search_space[\"numbers\"],\n",
    "            num_classes=class_num,\n",
    "            genotype=args.genotype,\n",
    "            affine=bool(xargs.affine),\n",
    "            track_running_stats=bool(xargs.track_running_stats),\n",
    "        ),\n",
    "        None,\n",
    "    )\n",
    "logger.log(\"search space : {:}\".format(search_space))\n",
    "logger.log(\"model config : {:}\".format(model_config))\n",
    "search_model = get_cell_based_tiny_net(model_config)\n",
    "search_model.set_algo(xargs.algo)\n",
    "logger.log(\"{:}\".format(search_model))\n",
    "\n",
    "device = torch.device('cuda:{}'.format(xargs.gpu))\n",
    "network = search_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c557c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_find_best(xargs, xloader, network, n_samples = None, archs = None):\n",
    "    if 'etf' in xargs.zero_shot_score.lower():\n",
    "        score_fn_name = \"compute_{}_score\".format(args.zero_shot_score.lower())\n",
    "        score_fn = globals().get(score_fn_name)\n",
    "        input_, target_ = next(iter(xloader))\n",
    "        resolution = input_.size(2)\n",
    "        batch_size = input_.size(0)\n",
    "        zero_shot_score_dict = None\n",
    "        arch_list = []\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    if archs is None and n_samples is not None:\n",
    "        network.train()\n",
    "        all_time = []\n",
    "        all_mem = []\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        for i in tqdm.tqdm(range(n_samples)):\n",
    "            # random sampling\n",
    "            arch = network.random_genotype(True)\n",
    "\n",
    "            start.record()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "            info_dict = score_fn.compute_nas_score(model=network, gpu=xargs.gpu, trainloader=None, resolution=resolution, batch_size=batch_size, search_space=xargs.search_space)\n",
    "\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            all_time.append(start.elapsed_time(end))\n",
    "            all_mem.append(torch.cuda.max_memory_reserved())\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            arch_list.append(arch)\n",
    "            if zero_shot_score_dict is None: # initialize dict\n",
    "                zero_shot_score_dict = dict()\n",
    "                for k in info_dict.keys():\n",
    "                    zero_shot_score_dict[k] = []\n",
    "            for k, v in info_dict.items():\n",
    "                zero_shot_score_dict[k].append(v)\n",
    "\n",
    "        print(\"------Runtime------\")\n",
    "        print(\"All: {:.5f} ms\".format(np.mean(all_time)))\n",
    "        print(\"------Avg Mem------\")\n",
    "        print(\"All: {:.5f} GB\".format(np.mean(all_mem)/1e9))\n",
    "        print(\"------Max Mem------\")\n",
    "        print(\"All: {:.5f} GB\".format(np.max(all_mem)/1e9))\n",
    "        \n",
    "    elif archs is not None and n_samples is None:\n",
    "        network.train()\n",
    "        all_time = []\n",
    "        all_mem = []\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        for arch in tqdm.tqdm(archs):\n",
    "            network.arch_cache = arch\n",
    "\n",
    "            start.record()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "            info_dict = score_fn.compute_nas_score(model=network, gpu=xargs.gpu, trainloader=None, resolution=resolution, batch_size=batch_size, search_space=xargs.search_space)\n",
    "\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            all_time.append(start.elapsed_time(end))\n",
    "            all_mem.append(torch.cuda.max_memory_reserved())\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            arch_list.append(arch)\n",
    "            if zero_shot_score_dict is None: # initialize dict\n",
    "                zero_shot_score_dict = dict()\n",
    "                for k in info_dict.keys():\n",
    "                    zero_shot_score_dict[k] = []\n",
    "            for k, v in info_dict.items():\n",
    "                zero_shot_score_dict[k].append(v)\n",
    "\n",
    "        print(\"------Runtime------\")\n",
    "        print(\"All: {:.5f} ms\".format(np.mean(all_time)))\n",
    "        print(\"------Avg Mem------\")\n",
    "        print(\"All: {:.5f} GB\".format(np.mean(all_mem)/1e9))\n",
    "        print(\"------Max Mem------\")\n",
    "        print(\"All: {:.5f} GB\".format(np.max(all_mem)/1e9))\n",
    "        \n",
    "    return arch_list, zero_shot_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "562c71e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64, 8, 8])\n",
      "torch.Size([64, 64, 8, 8])\n",
      "torch.Size([64, 8, 8, 8])\n",
      "torch.Size([64, 64, 8, 8])\n",
      "tensor([-1., -1., -1.,  1., -1.,  1., -1., -1.], device='cuda:0')\n",
      "tensor([-1.3958, -1.0000, -2.2439,  1.0000, -0.4246,  1.0000, -1.2408, -1.0000,\n",
      "        -0.4609,  0.0000, -0.9131, -0.6275,  0.0000,  0.0000, -0.3700,  0.7735,\n",
      "         0.0000,  0.0000, -0.0931,  0.4326,  0.0000,  0.3458,  0.0000,  0.0829,\n",
      "         0.0986,  0.0000,  0.0000,  0.0000,  0.6672,  0.0000, -1.3828,  0.1540,\n",
      "         0.0000, -0.5559,  1.4028,  0.0000,  0.0000, -0.7136, -0.4147, -0.4712,\n",
      "         0.0000,  0.0000,  0.7122,  0.0000,  0.3337,  0.0000,  0.0000,  0.0000,\n",
      "         0.6368,  0.0000,  0.5347,  0.2823,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0420, -0.2295],\n",
      "       device='cuda:0')\n",
      "\n",
      "torch.Size([64, 64, 8, 8])\n",
      "torch.Size([64, 64, 16, 16])\n",
      "torch.Size([64, 64, 8, 8])\n",
      "torch.Size([64, 40, 16, 16])\n",
      "tensor([ 1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,\n",
      "         1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1.,  1., -1.,\n",
      "        -1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1.,\n",
      "         1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,\n",
      "        -1.,  1., -1., -1.,  1., -1.,  1., -1.], device='cuda:0')\n",
      "tensor([-7.8771e-01, -3.9571e-01,  2.5602e+00, -3.2766e-01, -1.9969e+00,\n",
      "         1.9256e+00, -9.7044e-01,  1.4858e+00,  7.7480e-01,  1.7985e+00,\n",
      "         3.3824e-01,  8.1909e-01,  9.0801e-01,  2.3541e-01, -6.8354e-02,\n",
      "         1.8651e+00, -1.6283e-01, -1.6895e-01, -5.9851e-01, -1.0573e-01,\n",
      "        -2.6258e-01,  1.5015e+00,  1.3599e-02,  1.4528e-03, -1.1982e+00,\n",
      "        -5.8275e-01,  2.7002e+00, -2.7816e-01, -2.3110e-01, -2.1415e-01,\n",
      "        -4.9142e-01,  1.6501e+00, -2.0690e-01, -1.6184e-01, -1.1730e+00,\n",
      "        -1.1786e+00, -2.3798e-01, -5.4043e-01,  1.3236e-01, -2.6955e-01],\n",
      "       device='cuda:0')\n",
      "\n",
      "torch.Size([64, 64, 16, 16])\n",
      "torch.Size([64, 64, 16, 16])\n",
      "torch.Size([64, 40, 16, 16])\n",
      "torch.Size([64, 56, 16, 16])\n",
      "tensor([-1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1.,\n",
      "        -1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,\n",
      "         1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1.],\n",
      "       device='cuda:0')\n",
      "tensor([-1.0000,  1.0000, -1.0000,  1.4785, -2.5282, -1.0000, -1.0000,  0.2328,\n",
      "         0.3547,  1.0000, -1.0000, -0.7003, -4.1309,  1.0000, -1.6530, -0.0218,\n",
      "         1.0000, -1.0000,  1.9237,  3.1470,  1.0000, -1.9989,  1.0000, -1.0000,\n",
      "        -0.6533, -1.2637, -0.9780, -1.0000,  1.0000,  1.0000, -0.3253, -2.8956,\n",
      "         1.0000,  1.0000,  1.7563,  1.2220,  1.0000,  1.0000, -3.8987, -1.0000,\n",
      "        -0.6096,  0.0000,  0.0000, -0.9532,  0.0000,  0.0000,  0.0000, -3.0269,\n",
      "         1.1522,  0.0000,  0.0000, -2.2904,  0.0000,  1.0481,  2.7859, -0.6376],\n",
      "       device='cuda:0')\n",
      "\n",
      "torch.Size([64, 64, 16, 16])\n",
      "torch.Size([64, 64, 32, 32])\n",
      "torch.Size([64, 56, 16, 16])\n",
      "torch.Size([64, 40, 32, 32])\n",
      "tensor([ 1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,\n",
      "        -1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1.,  1., -1., -1., -1.,\n",
      "         1., -1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,\n",
      "        -1., -1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1., -1.],\n",
      "       device='cuda:0')\n",
      "tensor([-0.5700, -0.1945, -0.1178, -0.4073, -0.0227, -0.0097,  1.1949, -1.1242,\n",
      "        -0.8048,  0.2845, -0.1771, -0.3447,  0.1356,  0.0332, -0.3554, -0.0123,\n",
      "         0.2363, -0.6057, -0.1308, -0.3029, -0.4689, -0.6520, -0.3165, -0.3800,\n",
      "        -1.4854, -0.1148, -0.2136, -0.6526, -0.2819, -0.4333,  0.8125, -0.4541,\n",
      "        -0.2075, -0.2458,  0.8812,  0.0520, -1.8036,  0.4946,  0.3636,  0.3618],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:38<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m######### search across random N archs #########\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m archs, results \u001b[38;5;241m=\u001b[39m \u001b[43msearch_find_best\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m, in \u001b[0;36msearch_find_best\u001b[0;34m(xargs, xloader, network, n_samples, archs)\u001b[0m\n\u001b[1;32m     23\u001b[0m start\u001b[38;5;241m.\u001b[39mrecord()\n\u001b[1;32m     24\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mreset_peak_memory_stats()\n\u001b[0;32m---> 26\u001b[0m info_dict \u001b[38;5;241m=\u001b[39m \u001b[43mscore_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_nas_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m end\u001b[38;5;241m.\u001b[39mrecord()\n\u001b[1;32m     29\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "File \u001b[0;32m/home/junghyup/nas/nasbench201-random/random_my/NATSBench/ZeroShotProxy/compute_etf62_score.py:148\u001b[0m, in \u001b[0;36mcompute_nas_score\u001b[0;34m(model, gpu, trainloader, resolution, batch_size, search_space, fp16)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(g_out[\u001b[38;5;241m0\u001b[39m,:,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mprint\u001b[39m(g_in[\u001b[38;5;241m0\u001b[39m,:,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m g_out\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;241m==\u001b[39mg_in\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(g_in \u001b[38;5;241m==\u001b[39m g_out):\n\u001b[1;32m    151\u001b[0m     scores\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1191\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1234\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "######### search across random N archs #########\n",
    "archs, results = search_find_best(xargs, train_loader, network, n_samples=1)\n",
    "\n",
    "# # search across N archs uniformly sampled according to test acc.\n",
    "# def uniform_sample_archs(network, xargs, api, n_samples=1000, dataset='cifar10'):\n",
    "#     arch = network.random_genotype(True)\n",
    "#     search_space = get_search_spaces(xargs.search_space, \"nats-bench\")\n",
    "#     archs = arch.gen_all(search_space, xargs.max_nodes, False)\n",
    "    \n",
    "#     def get_results_from_api(api, arch, dataset='cifar10'):\n",
    "#         dataset_candidates = ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120']\n",
    "#         assert dataset in dataset_candidates\n",
    "#         index = api.query_index_by_arch(arch)\n",
    "#         api._prepare_info(index)\n",
    "#         archresult = api.arch2infos_dict[index]['200']\n",
    "#         if dataset == 'cifar10-valid':\n",
    "#             acc = archresult.get_metrics(dataset, 'x-valid', iepoch=199, is_random=False)['accuracy']\n",
    "#         else:\n",
    "#             acc = archresult.get_metrics(dataset, 'ori-test', iepoch=199, is_random=False)['accuracy']\n",
    "#         return acc\n",
    "\n",
    "#     accs = []\n",
    "#     for a in archs:\n",
    "#         accs.append(get_results_from_api(api, a, dataset))\n",
    "#     interval = len(archs) // n_samples\n",
    "#     sorted_indices = np.argsort(accs)\n",
    "#     new_archs = []\n",
    "#     for i, idx in enumerate(sorted_indices):\n",
    "#         if i % interval == 0:\n",
    "#             new_archs.append(archs[idx])\n",
    "#     archs = new_archs\n",
    "    \n",
    "#     return archs\n",
    "\n",
    "# uniform_archs = uniform_sample_archs(network, xargs, api, 1000, 'cifar10')\n",
    "# archs, results = search_find_best(xargs, train_loader, network, archs=uniform_archs)\n",
    "\n",
    "\n",
    "# ######### search across all archs #########\n",
    "# def generate_all_archs(network, xargs):\n",
    "#     arch = network.random_genotype(True)\n",
    "#     search_space = get_search_spaces(xargs.search_space, \"nats-bench\")\n",
    "#     archs = arch.gen_all(search_space, xargs.max_nodes, False)\n",
    "#     return archs\n",
    "\n",
    "# all_archs = generate_all_archs(network, xargs)\n",
    "# archs, results = search_find_best(xargs, train_loader, network, archs=all_archs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4147a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api.get_more_info(1,'cifar10')\n",
    "# api.query_by_index(1)\n",
    "# archresult = api.arch2infos_dict[1]['90']\n",
    "# archresult.get_metrics('cifar10', 'ori-test', iepoch=89, is_random=False)['accuracy']\n",
    "# archresult.get_metrics('cifar100', 'ori-test', iepoch=89, is_random=False)['accuracy']\n",
    "# archresult.get_metrics('ImageNet16-120', 'ori-test', iepoch=89, is_random=False)['accuracy']\n",
    "\n",
    "def get_results_from_api(api, arch, dataset='cifar10'):\n",
    "    dataset_candidates = ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120']\n",
    "    assert dataset in dataset_candidates\n",
    "    index = api.query_index_by_arch(arch)\n",
    "    api._prepare_info(index)\n",
    "    archresult = api.arch2infos_dict[index]['90']\n",
    "    \n",
    "    if dataset == 'cifar10-valid':\n",
    "        acc = archresult.get_metrics(dataset, 'x-valid', iepoch=89, is_random=False)['accuracy']\n",
    "    else:\n",
    "        acc = archresult.get_metrics(dataset, 'ori-test', iepoch=89, is_random=False)['accuracy']\n",
    "    flops = archresult.get_compute_costs(dataset)['flops']\n",
    "    params = archresult.get_compute_costs(dataset)['params']\n",
    "    \n",
    "    return acc, flops, params\n",
    "\n",
    "api_valid_accs, api_flops, api_params = [], [], []\n",
    "for a in archs:\n",
    "#     valid_acc, flops, params = get_results_from_api(api, a, 'cifar10')\n",
    "#     valid_acc, flops, params = get_results_from_api(api, a, 'cifar100')\n",
    "    valid_acc, flops, params = get_results_from_api(api, a, 'ImageNet16-120')\n",
    "    api_valid_accs.append(valid_acc)\n",
    "    api_flops.append(flops)\n",
    "    api_params.append(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e525a88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig_scale = 1.1\n",
    "\n",
    "l = len(api_flops)\n",
    "rank_agg = np.log(stats.rankdata(api_flops) / l)\n",
    "for k in results.keys():\n",
    "    rank_agg = rank_agg + np.log( stats.rankdata(results[k]) / l)\n",
    "    \n",
    "# rank_agg = None\n",
    "# for k in results.keys():\n",
    "#     if rank_agg is None:\n",
    "#         rank_agg = np.log( stats.rankdata(results[k]) / l)\n",
    "#     else:\n",
    "#         rank_agg = rank_agg + np.log( stats.rankdata(results[k]) / l)\n",
    "    \n",
    "best_idx = np.argmax(rank_agg)\n",
    "\n",
    "best_arch, acc = archs[best_idx], api_valid_accs[best_idx]\n",
    "if api is not None:\n",
    "    print(\"{:}\".format(api.query_by_arch(best_arch, \"90\")))\n",
    "    \n",
    "\n",
    "x = stats.rankdata(rank_agg)\n",
    "y = stats.rankdata(api_valid_accs)\n",
    "kendalltau = stats.kendalltau(x, y)\n",
    "spearmanr = stats.spearmanr(x, y)\n",
    "pearsonr = stats.pearsonr(x, y)\n",
    "print(\"aggregated: {}\\t{}\\t{}\\t\".format(kendalltau[0], pearsonr[0], spearmanr[0]))\n",
    "plt.figure(figsize=(4*fig_scale,3*fig_scale))\n",
    "plt.scatter(x, y, linewidths=0.1)\n",
    "plt.scatter(x[best_idx], y[best_idx], c=\"r\", linewidths=0.1)\n",
    "plt.title(\"rank_agg\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# for each\n",
    "tau, p_value = stats.kendalltau(stats.rankdata(api_flops), stats.rankdata(api_valid_accs))\n",
    "x = stats.rankdata(api_flops)\n",
    "y = stats.rankdata(api_valid_accs)\n",
    "kendalltau = stats.kendalltau(x, y)\n",
    "spearmanr = stats.spearmanr(x, y)\n",
    "pearsonr = stats.pearsonr(x, y)\n",
    "print(\"FLOPs: {}\\t{}\\t{}\\t\".format(kendalltau[0], pearsonr[0], spearmanr[0]))\n",
    "plt.figure(figsize=(4*fig_scale,3*fig_scale))\n",
    "plt.scatter(x, y, linewidths=0.1)\n",
    "plt.scatter(x[best_idx], y[best_idx], c=\"r\", linewidths=0.1)\n",
    "plt.title(\"rank_FLOPs\")\n",
    "plt.show()\n",
    "for k in results.keys():\n",
    "    x = stats.rankdata(results[k])\n",
    "    y = stats.rankdata(api_valid_accs)\n",
    "    kendalltau = stats.kendalltau(x, y)\n",
    "    spearmanr = stats.spearmanr(x, y)\n",
    "    pearsonr = stats.pearsonr(x, y)\n",
    "    print(\"{}: {}\\t{}\\t{}\\t\".format(k, kendalltau[0], pearsonr[0], spearmanr[0]))\n",
    "    plt.figure(figsize=(4*fig_scale,3*fig_scale))\n",
    "    plt.scatter(x, y, linewidths=0.1)\n",
    "    plt.scatter(x[best_idx], y[best_idx], c=\"r\", linewidths=0.1)\n",
    "    plt.title(\"rank_{}\".format(k))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d904cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
