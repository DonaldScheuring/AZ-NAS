{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4cc63dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 22:17:11.468586: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-29 22:17:11.514848: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-29 22:17:11.973074: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os, sys, time, glob, random, argparse\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import tqdm\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# XAutoDL \n",
    "from xautodl.config_utils import load_config, dict2config, configure2str\n",
    "from xautodl.datasets import get_datasets, get_nas_search_loaders\n",
    "from xautodl.procedures import (\n",
    "    prepare_seed,\n",
    "    prepare_logger,\n",
    "    save_checkpoint,\n",
    "    copy_checkpoint,\n",
    "    get_optim_scheduler,\n",
    ")\n",
    "from xautodl.utils import get_model_infos, obtain_accuracy\n",
    "from xautodl.log_utils import AverageMeter, time_string, convert_secs2time\n",
    "from xautodl.models import get_search_spaces\n",
    "\n",
    "# API\n",
    "from nats_bench import create\n",
    "\n",
    "# custom modules\n",
    "from custom.tss_model_supernet import get_cell_based_tiny_net\n",
    "from ZeroShotProxy import *\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6018e93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Namespace(data_path='./cifar.python', dataset='cifar10', search_space='tss', config_path='./configs/nas-benchmark/algos/weight-sharing.config', max_nodes=4, channel=16, num_cells=5, affine=0, track_running_stats=0, print_freq=200, gpu=0, workers=2, api_data_path='./api_data/NATS-tss-v1_0-3ffb9-simple/', save_dir='./results/tmp', zero_shot_score='gradsign', rand_seed=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(\"Training-free NAS on NATSBench (TSS)\")\n",
    "parser.add_argument(\"--data_path\", type=str, default='./cifar.python', help=\"The path to dataset\")\n",
    "parser.add_argument(\"--dataset\", type=str, default='cifar10',choices=[\"cifar10\", \"cifar100\", \"ImageNet16-120\"], help=\"Choose between Cifar10/100 and ImageNet-16.\")\n",
    "\n",
    "# channels and number-of-cells\n",
    "parser.add_argument(\"--search_space\", type=str, default='tss', help=\"The search space name.\")\n",
    "parser.add_argument(\"--config_path\", type=str, default='./configs/nas-benchmark/algos/weight-sharing.config', help=\"The path to the configuration.\")\n",
    "parser.add_argument(\"--max_nodes\", type=int, default=4, help=\"The maximum number of nodes.\")\n",
    "parser.add_argument(\"--channel\", type=int, default=16, help=\"The number of channels.\")\n",
    "parser.add_argument(\"--num_cells\", type=int, default=5, help=\"The number of cells in one stage.\")\n",
    "# parser.add_argument(\"--channel\", type=int, default=3, help=\"The number of channels.\")\n",
    "# parser.add_argument(\"--num_cells\", type=int, default=1, help=\"The number of cells in one stage.\")\n",
    "# parser.add_argument(\"--select_num\", type=int, default=100, help=\"The number of selected architectures to evaluate.\")\n",
    "parser.add_argument(\"--affine\", type=int, default=0, choices=[0, 1], help=\"Whether use affine=True or False in the BN layer.\")\n",
    "parser.add_argument(\"--track_running_stats\", type=int, default=0, choices=[0, 1], help=\"Whether use track_running_stats or not in the BN layer.\")\n",
    "\n",
    "# log\n",
    "parser.add_argument(\"--print_freq\", type=int, default=200, help=\"print frequency (default: 200)\")\n",
    "\n",
    "# custom\n",
    "parser.add_argument(\"--gpu\", type=int, default=0, help=\"\")\n",
    "parser.add_argument(\"--workers\", type=int, default=2, help=\"number of data loading workers\")\n",
    "parser.add_argument(\"--api_data_path\", type=str, default=\"./api_data/NATS-tss-v1_0-3ffb9-simple/\", help=\"\")\n",
    "parser.add_argument(\"--save_dir\", type=str, default='./results/tmp', help=\"Folder to save checkpoints and log.\")\n",
    "parser.add_argument('--zero_shot_score', type=str, default='gradsign', help='could be: ETF; ZiCo (form ZiCo); params, (for \\#Params); Zen (for Zen-NAS); TE (for TE-NAS)')\n",
    "parser.add_argument(\"--rand_seed\", type=int, default=1, help=\"manual seed\")\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "if args.rand_seed is None or args.rand_seed < 0:\n",
    "    args.rand_seed = random.randint(1, 100000)\n",
    "\n",
    "print(args.rand_seed)\n",
    "print(args)\n",
    "xargs=args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8a05319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Function with logger : Logger(dir=results/tmp, use-tf=False, writer=None)\n",
      "Arguments : -------------------------------\n",
      "data_path        : ./cifar.python\n",
      "dataset          : cifar10\n",
      "search_space     : tss\n",
      "config_path      : ./configs/nas-benchmark/algos/weight-sharing.config\n",
      "max_nodes        : 4\n",
      "channel          : 16\n",
      "num_cells        : 5\n",
      "affine           : 0\n",
      "track_running_stats : 0\n",
      "print_freq       : 200\n",
      "gpu              : 0\n",
      "workers          : 2\n",
      "api_data_path    : ./api_data/NATS-tss-v1_0-3ffb9-simple/\n",
      "save_dir         : ./results/tmp\n",
      "zero_shot_score  : gradsign\n",
      "rand_seed        : 1\n",
      "Python  Version  : 3.10.11 (main, Apr 20 2023, 19:02:41) [GCC 11.2.0]\n",
      "Pillow  Version  : 9.4.0\n",
      "PyTorch Version  : 2.0.1\n",
      "cuDNN   Version  : 8500\n",
      "CUDA available   : True\n",
      "CUDA GPU numbers : 1\n",
      "CUDA_VISIBLE_DEVICES : 0\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available(), \"CUDA is not available.\"\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.set_num_threads(xargs.workers)\n",
    "prepare_seed(xargs.rand_seed)\n",
    "logger = prepare_logger(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78557cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create API = NATStopology(0/15625 architectures, fast_mode=True, file=) done\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "./configs/nas-benchmark/algos/weight-sharing.config\n",
      "Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=100, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, test_batch_size=512, class_num=10, xshape=(1, 3, 32, 32))\n",
      "||||||| cifar10    ||||||| Search-Loader-Num=391, Valid-Loader-Num=49, batch size=64\n",
      "||||||| cifar10    ||||||| Config=Configure(scheduler='cos', LR=0.025, eta_min=0.001, epochs=100, warmup=0, optim='SGD', decay=0.0005, momentum=0.9, nesterov=True, criterion='Softmax', batch_size=64, test_batch_size=512, class_num=10, xshape=(1, 3, 32, 32))\n",
      "search space : ['none', 'skip_connect', 'nor_conv_1x1', 'nor_conv_3x3', 'avg_pool_3x3']\n",
      "model config : Configure(name='generic', C=16, N=5, max_nodes=4, num_classes=10, space=['none', 'skip_connect', 'nor_conv_1x1', 'nor_conv_3x3', 'avg_pool_3x3'], affine=False, track_running_stats=False)\n"
     ]
    }
   ],
   "source": [
    "## API\n",
    "api = create(xargs.api_data_path, xargs.search_space, fast_mode=True, verbose=False)\n",
    "logger.log(\"Create API = {:} done\".format(api))\n",
    "\n",
    "## data\n",
    "train_data, valid_data, xshape, class_num = get_datasets(xargs.dataset, xargs.data_path, -1)\n",
    "config = load_config(xargs.config_path, {\"class_num\": class_num, \"xshape\": xshape}, logger)\n",
    "search_loader, train_loader, valid_loader = get_nas_search_loaders(train_data,\n",
    "                                                                   valid_data,\n",
    "                                                                   xargs.dataset,\n",
    "                                                                   \"./configs/nas-benchmark/\",\n",
    "                                                                   (config.batch_size, config.test_batch_size),\n",
    "                                                                   xargs.workers,)\n",
    "logger.log(\"||||||| {:10s} ||||||| Search-Loader-Num={:}, Valid-Loader-Num={:}, batch size={:}\".format(xargs.dataset, len(search_loader), len(valid_loader), config.batch_size))\n",
    "logger.log(\"||||||| {:10s} ||||||| Config={:}\".format(xargs.dataset, config))\n",
    "\n",
    "## model\n",
    "search_space = get_search_spaces(xargs.search_space, \"nats-bench\")\n",
    "model_config = dict2config(\n",
    "    dict(\n",
    "        name=\"generic\",\n",
    "        C=xargs.channel,\n",
    "        N=xargs.num_cells,\n",
    "        max_nodes=xargs.max_nodes,\n",
    "        num_classes=class_num,\n",
    "        space=search_space,\n",
    "        affine=bool(xargs.affine),\n",
    "        track_running_stats=bool(xargs.track_running_stats),\n",
    "    ),\n",
    "    None,\n",
    ")\n",
    "logger.log(\"search space : {:}\".format(search_space))\n",
    "logger.log(\"model config : {:}\".format(model_config))\n",
    "search_model = get_cell_based_tiny_net(model_config)\n",
    "\n",
    "device = torch.device('cuda:{}'.format(xargs.gpu))\n",
    "network = search_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c557c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_input_metrics = ['zico', 'snip', 'grasp', 'te_nas', 'gradsign']\n",
    "\n",
    "def search_find_best(xargs, xloader, network, n_samples = None, archs = None):\n",
    "    logger.log(\"Searching with {}\".format(xargs.zero_shot_score.lower()))\n",
    "    score_fn_name = \"compute_{}_score\".format(xargs.zero_shot_score.lower())\n",
    "    score_fn = globals().get(score_fn_name)\n",
    "    input_, target_ = next(iter(xloader))\n",
    "    resolution = input_.size(2)\n",
    "    batch_size = input_.size(0)\n",
    "    zero_shot_score_dict = None\n",
    "    arch_list = []\n",
    "    trainloader = train_loader if xargs.zero_shot_score.lower() in real_input_metrics else None\n",
    "    \n",
    "    if archs is None and n_samples is not None:\n",
    "        network.train()\n",
    "        all_time = []\n",
    "        all_mem = []\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        for i in tqdm.tqdm(range(n_samples)):\n",
    "            # random sampling\n",
    "            arch = network.random_genotype(True)\n",
    "\n",
    "            start.record()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "            info_dict = score_fn.compute_nas_score(network, gpu=xargs.gpu, trainloader=trainloader, resolution=resolution, batch_size=batch_size)\n",
    "\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            all_time.append(start.elapsed_time(end))\n",
    "#             all_mem.append(torch.cuda.max_memory_reserved())\n",
    "            all_mem.append(torch.cuda.max_memory_allocated())\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            arch_list.append(arch)\n",
    "            if zero_shot_score_dict is None: # initialize dict\n",
    "                zero_shot_score_dict = dict()\n",
    "                for k in info_dict.keys():\n",
    "                    zero_shot_score_dict[k] = []\n",
    "            for k, v in info_dict.items():\n",
    "                zero_shot_score_dict[k].append(v)\n",
    "\n",
    "        logger.log(\"------Runtime------\")\n",
    "        logger.log(\"All: {:.5f} ms\".format(np.mean(all_time)))\n",
    "        logger.log(\"------Avg Mem------\")\n",
    "        logger.log(\"All: {:.5f} GB\".format(np.mean(all_mem)/1e9))\n",
    "        logger.log(\"------Max Mem------\")\n",
    "        logger.log(\"All: {:.5f} GB\".format(np.max(all_mem)/1e9))\n",
    "        \n",
    "    elif archs is not None and n_samples is None:\n",
    "        network.train()\n",
    "        all_time = []\n",
    "        all_mem = []\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        for arch in tqdm.tqdm(archs):\n",
    "            network.arch_cache = arch\n",
    "\n",
    "            start.record()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "            info_dict = score_fn.compute_nas_score(network, gpu=xargs.gpu, trainloader=trainloader, resolution=resolution, batch_size=batch_size)\n",
    "\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            all_time.append(start.elapsed_time(end))\n",
    "#             all_mem.append(torch.cuda.max_memory_reserved())\n",
    "            all_mem.append(torch.cuda.max_memory_allocated())\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            arch_list.append(arch)\n",
    "            if zero_shot_score_dict is None: # initialize dict\n",
    "                zero_shot_score_dict = dict()\n",
    "                for k in info_dict.keys():\n",
    "                    zero_shot_score_dict[k] = []\n",
    "            for k, v in info_dict.items():\n",
    "                zero_shot_score_dict[k].append(v)\n",
    "\n",
    "        logger.log(\"------Runtime------\")\n",
    "        logger.log(\"All: {:.5f} ms\".format(np.mean(all_time)))\n",
    "        logger.log(\"------Avg Mem------\")\n",
    "        logger.log(\"All: {:.5f} GB\".format(np.mean(all_mem)/1e9))\n",
    "        logger.log(\"------Max Mem------\")\n",
    "        logger.log(\"All: {:.5f} GB\".format(np.max(all_mem)/1e9))\n",
    "        \n",
    "    return arch_list, zero_shot_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562c71e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with gradsign\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 652/15625 [19:53<6:46:37,  1.63s/it] "
     ]
    }
   ],
   "source": [
    "# ######### search across random N archs #########\n",
    "# archs, results = search_find_best(xargs, train_loader, network, n_samples=3000)\n",
    "\n",
    "# ######### search across N archs uniformly sampled according to test acc. #########\n",
    "# def uniform_sample_archs(network, xargs, api, n_samples=1000, dataset='cifar10'):\n",
    "#     arch = network.random_genotype(True)\n",
    "#     search_space = get_search_spaces(xargs.search_space, \"nats-bench\")\n",
    "#     archs = arch.gen_all(search_space, xargs.max_nodes, False)\n",
    "    \n",
    "#     def get_results_from_api(api, arch, dataset='cifar10'):\n",
    "#         dataset_candidates = ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120']\n",
    "#         assert dataset in dataset_candidates\n",
    "#         index = api.query_index_by_arch(arch)\n",
    "#         api._prepare_info(index)\n",
    "#         archresult = api.arch2infos_dict[index]['200']\n",
    "#         if dataset == 'cifar10-valid':\n",
    "#             acc = archresult.get_metrics(dataset, 'x-valid', iepoch=199, is_random=False)['accuracy']\n",
    "#         else:\n",
    "#             acc = archresult.get_metrics(dataset, 'ori-test', iepoch=199, is_random=False)['accuracy']\n",
    "#         return acc\n",
    "\n",
    "#     accs = []\n",
    "#     for a in archs:\n",
    "#         accs.append(get_results_from_api(api, a, dataset))\n",
    "#     interval = len(archs) // n_samples\n",
    "#     sorted_indices = np.argsort(accs)\n",
    "#     new_archs = []\n",
    "#     for i, idx in enumerate(sorted_indices):\n",
    "#         if i % interval == 0:\n",
    "#             new_archs.append(archs[idx])\n",
    "#     archs = new_archs\n",
    "    \n",
    "#     return archs\n",
    "\n",
    "# if os.path.exists(\"./tss_uniform_arch.pickle\"):\n",
    "#     with open(\"./tss_uniform_arch.pickle\", \"rb\") as fp:\n",
    "#         uniform_archs = pickle.load(fp)\n",
    "# else:\n",
    "#     uniform_archs = uniform_sample_archs(network, xargs, api, 1000, 'cifar10')\n",
    "#     with open(\"./tss_uniform_arch.pickle\", \"wb\") as fp:\n",
    "#         pickle.dump(uniform_archs, fp)\n",
    "# archs, results = search_find_best(xargs, train_loader, network, archs=uniform_archs)\n",
    "\n",
    "\n",
    "######### search across all archs #########\n",
    "def generate_all_archs(network, xargs):\n",
    "    arch = network.random_genotype(True)\n",
    "    search_space = get_search_spaces(xargs.search_space, \"nats-bench\")\n",
    "    archs = arch.gen_all(search_space, xargs.max_nodes, False)\n",
    "    return archs\n",
    "\n",
    "all_archs = generate_all_archs(network, xargs)\n",
    "archs, results = search_find_best(xargs, train_loader, network, archs=all_archs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4147a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_from_api(api, arch, dataset='cifar10'):\n",
    "    dataset_candidates = ['cifar10-valid', 'cifar10', 'cifar100', 'ImageNet16-120']\n",
    "    assert dataset in dataset_candidates\n",
    "    index = api.query_index_by_arch(arch)\n",
    "    api._prepare_info(index)\n",
    "    archresult = api.arch2infos_dict[index]['200']\n",
    "    \n",
    "    if dataset == 'cifar10-valid':\n",
    "        acc = archresult.get_metrics(dataset, 'x-valid', iepoch=199, is_random=False)['accuracy']\n",
    "    else:\n",
    "        acc = archresult.get_metrics(dataset, 'ori-test', iepoch=199, is_random=False)['accuracy']\n",
    "    flops = archresult.get_compute_costs(dataset)['flops']\n",
    "    params = archresult.get_compute_costs(dataset)['params']\n",
    "    \n",
    "    return acc, flops, params\n",
    "\n",
    "api_valid_accs, api_flops, api_params = [], [], []\n",
    "for a in archs:\n",
    "    valid_acc, flops, params = get_results_from_api(api, a, 'cifar10')\n",
    "#     valid_acc, flops, params = get_results_from_api(api, a, 'cifar100')\n",
    "#     valid_acc, flops, params = get_results_from_api(api, a, 'ImageNet16-120')\n",
    "    api_valid_accs.append(valid_acc)\n",
    "    api_flops.append(flops)\n",
    "    api_params.append(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e525a88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig_scale = 1.1\n",
    "\n",
    "rank_agg = None\n",
    "l = len(api_flops)\n",
    "rank_agg = np.log(stats.rankdata(api_flops) / l)\n",
    "for k in results.keys():\n",
    "    if rank_agg is None:\n",
    "        rank_agg = np.log( stats.rankdata(results[k]) / l)\n",
    "    else:\n",
    "        rank_agg = rank_agg + np.log( stats.rankdata(results[k]) / l)\n",
    "\n",
    "        \n",
    "best_idx = np.argmax(rank_agg)\n",
    "\n",
    "best_arch, acc = archs[best_idx], api_valid_accs[best_idx]\n",
    "if api is not None:\n",
    "    print(\"{:}\".format(api.query_by_arch(best_arch, \"200\")))\n",
    "    \n",
    "\n",
    "x = stats.rankdata(rank_agg)\n",
    "y = stats.rankdata(api_valid_accs)\n",
    "kendalltau = stats.kendalltau(x, y)\n",
    "spearmanr = stats.spearmanr(x, y)\n",
    "pearsonr = stats.pearsonr(x, y)\n",
    "print(\"aggregated: {}\\t{}\\t{}\\t\".format(kendalltau[0], pearsonr[0], spearmanr[0]))\n",
    "plt.figure(figsize=(4*fig_scale,3*fig_scale))\n",
    "plt.scatter(x, y, linewidths=0.1)\n",
    "plt.scatter(x[best_idx], y[best_idx], c=\"r\", linewidths=0.1)\n",
    "plt.title(\"Rank_agg\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# for each\n",
    "metrics = {'FLOPs':api_flops, 'Params':api_params}\n",
    "for k, v in results.items():\n",
    "    metrics[k] = v\n",
    "for k in metrics.keys():\n",
    "    x = stats.rankdata(metrics[k])\n",
    "    y = stats.rankdata(api_valid_accs)\n",
    "    kendalltau = stats.kendalltau(x, y)\n",
    "    spearmanr = stats.spearmanr(x, y)\n",
    "    pearsonr = stats.pearsonr(x, y)\n",
    "    print(\"{}: {}\\t{}\\t{}\\t\".format(k, kendalltau[0], pearsonr[0], spearmanr[0]))\n",
    "    plt.figure(figsize=(4*fig_scale,3*fig_scale))\n",
    "    plt.scatter(x, y, linewidths=0.1)\n",
    "    plt.scatter(x[best_idx], y[best_idx], c=\"r\", linewidths=0.1)\n",
    "    plt.title(\"Rank_{}\".format(k))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f90e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics['gradsign'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63423a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b in zip(archs, metrics['gradsign']):\n",
    "    print(a,b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
